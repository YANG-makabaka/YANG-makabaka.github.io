<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>OpenCV——7</title>
      <link href="/posts/b5e212c6.html"/>
      <url>/posts/b5e212c6.html</url>
      
        <content type="html"><![CDATA[<h1 id="25像素重映射"><a href="#25像素重映射" class="headerlink" title="25像素重映射"></a>25<strong>像素重映射</strong></h1><h4 id="把像素点P-x-y-重新映射到一个新的位置P’-x’-y’"><a href="#把像素点P-x-y-重新映射到一个新的位置P’-x’-y’" class="headerlink" title="把像素点P(x,y)重新映射到一个新的位置P’(x’, y’)"></a>把像素点P(x,y)重新映射到一个新的位置P’(x’, y’)</h4><h4 id="像素重映射函数"><a href="#像素重映射函数" class="headerlink" title="像素重映射函数"></a>像素重映射函数</h4><h4 id="cv-remap-src-map1-map2-interpolation-dst-borderMode-borderValue-gt-dst"><a href="#cv-remap-src-map1-map2-interpolation-dst-borderMode-borderValue-gt-dst" class="headerlink" title="cv.remap(src, map1, map2, interpolation[, dst[, borderMode[, borderValue]]] ) -&gt;dst"></a>cv.remap(src, map1, map2, interpolation[, dst[, borderMode[, borderValue]]] ) -&gt;dst</h4><p>​    •src表示图像</p><p>​    •map1表示x,y方向映射规则，或者x方向映射</p><p>​    •Map2如果map1表示x,y映射时为空，否则表示y</p><p>​    •表示映射时候的像素插值方法 支持：INTER_NEAREST 、NTER_LINEAR 、NTER_CUBIC</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">#25像素重映射</span><br><span class="line">def remap_demo():</span><br><span class="line">    image = cv.imread(&quot;123.jpg&quot;)</span><br><span class="line">    cv.namedWindow(&quot;remap-demo&quot;, cv.WINDOW_AUTOSIZE)</span><br><span class="line">    cv.createTrackbar(&quot;remap-type&quot;, &quot;remap-demo&quot;, 0, 3, trackbar_callback)</span><br><span class="line">    h, w, c = image.shape</span><br><span class="line">    cv.imshow(&quot;input&quot;, image)</span><br><span class="line">    map_x = np.zeros((h, w), dtype=np.float32)</span><br><span class="line">    map_y = np.zeros((h, w), dtype=np.float32)</span><br><span class="line"></span><br><span class="line">    while True:</span><br><span class="line">        pos = cv.getTrackbarPos(&quot;remap-type&quot;, &quot;remap-demo&quot;)</span><br><span class="line">        if pos == 0:  # 倒立</span><br><span class="line">            for i in range(map_x.shape[0]):</span><br><span class="line">                map_x[i, :] = [x for x in range(map_x.shape[1])]</span><br><span class="line">            for j in range(map_y.shape[1]):</span><br><span class="line">                map_y[:, j] = [map_y.shape[0] - y for y in range(map_y.shape[0])]</span><br><span class="line">        elif pos == 1:  # 镜像</span><br><span class="line">            for i in range(map_x.shape[0]):</span><br><span class="line">                map_x[i, :] = [map_x.shape[1] - x for x in range(map_x.shape[1])]</span><br><span class="line">            for j in range(map_y.shape[1]):</span><br><span class="line">                map_y[:, j] = [y for y in range(map_y.shape[0])]</span><br><span class="line">        elif pos == 2:  # 对象线对称</span><br><span class="line">            for i in range(map_x.shape[0]):</span><br><span class="line">                map_x[i, :] = [map_x.shape[1] - x for x in range(map_x.shape[1])]</span><br><span class="line">            for j in range(map_y.shape[1]):</span><br><span class="line">                map_y[:, j] = [map_y.shape[0] - y for y in range(map_y.shape[0])]</span><br><span class="line">        elif pos == 3:  # 放大两倍</span><br><span class="line">            for i in range(map_x.shape[0]):</span><br><span class="line">                map_x[i, :] = [int(x/2) for x in range(map_x.shape[1])]</span><br><span class="line">            for j in range(map_y.shape[1]):</span><br><span class="line">                map_y[:, j] = [int(y/2) for y in range(map_y.shape[0])]</span><br><span class="line"></span><br><span class="line">        dst = cv.remap(image, map_x, map_y, cv.INTER_LINEAR)</span><br><span class="line">        cv.imshow(&quot;remap-demo&quot;, dst)</span><br><span class="line">        c = cv.waitKey(100)</span><br><span class="line">        if c == 27:</span><br><span class="line">            break</span><br><span class="line">    cv.destroyAllWindows()</span><br></pre></td></tr></table></figure><h1 id="26图像二值化"><a href="#26图像二值化" class="headerlink" title="26图像二值化"></a>26<strong>图像二值化</strong></h1><h4 id="图像二值化定义"><a href="#图像二值化定义" class="headerlink" title="图像二值化定义"></a>图像二值化定义</h4><p>•只有两个像素值0、1(0表示黑色，1-255表示白色)，黑色表示背景，白色表示对象（规则）</p><h4 id="图像二值化方法"><a href="#图像二值化方法" class="headerlink" title="图像二值化方法"></a>图像二值化方法</h4><p>•cv.mean,计算灰度图像均值<em>m</em></p><p>•inRange方法分割</p><h4 id="二值化函数"><a href="#二值化函数" class="headerlink" title="二值化函数"></a>二值化函数</h4><p><strong>cv.threshold</strong>(<strong>src</strong>, thresh,<strong>maxval</strong>, type[,<strong>dst</strong>]) -&gt;<strong>retval</strong>, <strong>dst</strong></p><p>​    <strong>src</strong>表示输入图像</p><p>​    <strong>thresh</strong>表示阈值</p><p>​    <strong>maxval</strong>表示最大值</p><p>​    <strong>type</strong>表示  <strong>二值化THRESH_BINARY</strong>   或者   <strong>二值化反THRESH_BINARY_INV</strong></p><p>​    <strong>retval</strong>表示返回阈值，<strong>dst</strong>返回的二值图像</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"># 26图像二值化</span><br><span class="line">def binary_demo():</span><br><span class="line">    image = cv.imread(&quot;123.jpg&quot;)</span><br><span class="line">    gray = cv.cvtColor(image, cv.COLOR_BGR2GRAY)</span><br><span class="line">    cv.imshow(&quot;gray&quot;, gray)</span><br><span class="line"></span><br><span class="line">    # 手动阈值，二值化</span><br><span class="line">    ret, binary = cv.threshold(gray, 127, 255, cv.THRESH_BINARY)</span><br><span class="line">    cv.imshow(&quot;binary&quot;,binary)</span><br><span class="line">    cv.waitKey(0)</span><br><span class="line"></span><br><span class="line">    # 求均值，二值化</span><br><span class="line">    m = cv.mean(gray)[0]</span><br><span class="line">    ret, binary = cv.threshold(gray, m, 255, cv.THRESH_BINARY)</span><br><span class="line">    cv.imshow(&quot;binary&quot;, binary)</span><br><span class="line">    cv.waitKey(0)</span><br><span class="line"></span><br><span class="line">    cv.destroyAllWindows()</span><br></pre></td></tr></table></figure><h1 id="27全局与自适应二值化"><a href="#27全局与自适应二值化" class="headerlink" title="27全局与自适应二值化"></a>27<strong>全局与自适应二值化</strong></h1><h3 id="全局二值化"><a href="#全局二值化" class="headerlink" title="全局二值化"></a>全局二值化</h3><h4 id="（1）大津法（针对两峰）：0-5六个灰度级别，根据直方图分布，以每个灰度等级分割直方图分布为两个部分，分别求取均值跟方差，如图示，最小方法差和对应的灰度值为，分割阈值"><a href="#（1）大津法（针对两峰）：0-5六个灰度级别，根据直方图分布，以每个灰度等级分割直方图分布为两个部分，分别求取均值跟方差，如图示，最小方法差和对应的灰度值为，分割阈值" class="headerlink" title="（1）大津法（针对两峰）：0~5六个灰度级别，根据直方图分布，以每个灰度等级分割直方图分布为两个部分，分别求取均值跟方差，如图示，最小方法差和对应的灰度值为，分割阈值."></a>（1）<strong>大津法</strong>（针对两峰）：<strong>0~5</strong>六个灰度级别，根据直方图分布，以每个灰度等级分割直方图分布为两个部分，分别求取均值跟方差，如图示，最小方法差和对应的灰度值为，分割阈值.</h4><p><img src="https://s3.bmp.ovh/imgs/2022/12/24/ac02c9856935afa6.png" alt=""></p><h4 id="（2）三角法（针对单峰）"><a href="#（2）三角法（针对单峰）" class="headerlink" title="（2）三角法（针对单峰）"></a>（2）三角法（针对单峰）</h4><p><img src="https://s3.bmp.ovh/imgs/2022/12/24/c0c9b34c866781c5.png" alt=""></p><p>α和β角都为45°，最长的d对应的点偏移0.2即为阈值点。</p><h4 id="两种方法都是基于直方图分布"><a href="#两种方法都是基于直方图分布" class="headerlink" title="两种方法都是基于直方图分布"></a>两种方法都是基于直方图分布</h4><h3 id="全局二值化函数"><a href="#全局二值化函数" class="headerlink" title="全局二值化函数"></a>全局二值化函数</h3><p><strong>cv.threshold(src, thresh,maxval, type[,dst]) -&gt;retval,dst</strong></p><p>​    •<strong>type</strong>表示二值化</p><p>​        •<strong>THRESH_BINARY | THRESH_OTSU  全局自动阈值＋二值化（大津）</strong></p><p>​        •<strong>THRESH_BINARY | THRESH_TRIANGLE  全局自动阈值＋二值化（三角）</strong></p><p>​        •<strong>THRESH_BINARY_INV | THRESH_OTSU</strong></p><p>​        <strong>表示不同的全局二值化方法</strong></p><h3 id="自适应二值化"><a href="#自适应二值化" class="headerlink" title="自适应二值化"></a>自适应二值化</h3><p>•<strong>模糊图像 D</strong>（可以为均值模糊/高斯模糊）</p><p>•<strong>原图</strong>S + 加上<strong>偏置常量</strong>C</p><p>•<strong>T = S –D &gt; -C ? 255 : 0</strong></p><h4 id="自适应二值化函数"><a href="#自适应二值化函数" class="headerlink" title="自适应二值化函数"></a>自适应二值化函数</h4><p><strong>cv.adaptiveThreshold</strong>(<strong>src</strong>, <strong>maxValue</strong>, <strong>adaptiveMethod</strong>, <strong>thresholdType</strong>, <strong>blockSize</strong>, C[,<strong>dst</strong>] ) -&gt;<strong>dst</strong></p><p>​    •cv.ADAPTIVE_THRESH_MEAN_C         均值</p><p>​    cv.ADAPTIVE_THRESH_GAUSSIAN_C   高斯</p><p>​    •<strong>blockSize</strong>必须为奇数</p><p>​    •<strong>C</strong>表示要减去的权重，可以是正数，负数，0</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"># 27全局与自适应二值化</span><br><span class="line">def binarier_demo():</span><br><span class="line">    image = cv.imread(&quot;123.jpg&quot;)</span><br><span class="line">    gray = cv.cvtColor(image, cv.COLOR_BGR2GRAY)</span><br><span class="line">    cv.imshow(&quot;gray&quot;, gray)</span><br><span class="line"></span><br><span class="line">    # 手动阈值，大津法</span><br><span class="line">    ret, binary = cv.threshold(gray, 0, 255, cv.THRESH_BINARY | cv.THRESH_OTSU)</span><br><span class="line">    cv.imshow(&quot;binary1&quot;, binary)</span><br><span class="line">    cv.waitKey(0)</span><br><span class="line"></span><br><span class="line">    # 手动阈值，三角法</span><br><span class="line">    ret, binary = cv.threshold(gray, 0, 255, cv.THRESH_BINARY | cv.THRESH_TRIANGLE)</span><br><span class="line">    cv.imshow(&quot;binary2&quot;, binary)</span><br><span class="line">    cv.waitKey(0)</span><br><span class="line"></span><br><span class="line">    # 自适应法</span><br><span class="line">    binary = cv.adaptiveThreshold(gray, 255, cv.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 25, 10)</span><br><span class="line">    cv.imshow(&quot;binary3&quot;, binary)</span><br><span class="line">    cv.waitKey(0)</span><br><span class="line"></span><br><span class="line">    cv.destroyAllWindows()</span><br></pre></td></tr></table></figure><h1 id="28实时人脸检测"><a href="#28实时人脸检测" class="headerlink" title="28实时人脸检测"></a><strong>28实时人脸检测</strong></h1><h3 id="OpenCV4-DNN模块"><a href="#OpenCV4-DNN模块" class="headerlink" title="OpenCV4 DNN模块"></a><strong>OpenCV4</strong> <strong>DNN</strong>模块</h3><p>•来自另外一个开源项目tiny dnn</p><p>•OpenCV3.3正式发布</p><p>•最新版本OpenCV4.5.5</p><p>•支持后台硬件加速机制 CPU/GPU等</p><p>•支持多种任务(分类、检测、分割、风格迁移、场景文字检测等)</p><p>•只支持推理（模型部署），不支持模型训练</p><p>•支持主流的深度学习框架生成模型，OpenCV加载</p><p>•推荐使用pytorch/tensorflow</p><h3 id="OpenCV人脸检测支持演化"><a href="#OpenCV人脸检测支持演化" class="headerlink" title="OpenCV人脸检测支持演化"></a>OpenCV人脸检测支持演化</h3><p>•OpenCV3.3之前基于HAAR/LBP级联检测</p><p>•OpenCV3.3开始支持深度学习人脸检测</p><p>•支持人脸检测模型caffe/tensorflow</p><p>•OpenCV4.5.4 支持人脸检测+landmark</p><p>•模型下载地址：</p><p>•<a href="https://gitee.com/opencv_ai/opencv_tutorial_data">https://gitee.com/opencv_ai/opencv_tutorial_data</a></p><h3 id="DNN相关函数"><a href="#DNN相关函数" class="headerlink" title="DNN相关函数"></a>DNN相关函数</h3><p>•读取模型：readNetFromTensorflow</p><p>•转换为blob对象：blobFromImage</p><p>•设置输入：setInput</p><p>•推理预测：forward</p><h3 id="人脸检测显示"><a href="#人脸检测显示" class="headerlink" title="人脸检测显示"></a>人脸检测显示</h3><p>•模型输入:1x3x300x300</p><p>•模型输出:1xN（张人脸）x7（个数据）</p><p>​    人脸检测框坐标（左上右下） – 后面四个值</p><p>​    预测置信度（score） – 第三个值</p><p>​    class_id（类别） – 第一个值</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br></pre></td><td class="code"><pre><span class="line"># 人脸检测</span><br><span class="line">#下载下面两个模型到项目地址（https://gitee.com/opencv_ai/opencv_tutorial_data）</span><br><span class="line">model_bin = &quot;opencv_face_detector_uint8.pb&quot;</span><br><span class="line">config_text = &quot;opencv_face_detector.pbtxt&quot;;</span><br><span class="line"></span><br><span class="line"># 视频人脸检测</span><br><span class="line">def video_detection():</span><br><span class="line">    font = cv.FONT_HERSHEY_SIMPLEX</span><br><span class="line">    font_scale = 0.5</span><br><span class="line">    thickness = 1</span><br><span class="line"></span><br><span class="line">    #load tensorflow model</span><br><span class="line">    net = cv.dnn.readNetFromTensorflow(model_bin, config=config_text)</span><br><span class="line">    capture = cv.VideoCapture(0) #获取摄像头图像</span><br><span class="line">    # 人脸检测</span><br><span class="line">    while True:</span><br><span class="line">        e1 = cv.getTickCount()</span><br><span class="line">        ret, frame = capture.read()</span><br><span class="line">        frame = cv.flip(frame, 1)</span><br><span class="line">        if ret is not True:</span><br><span class="line">            break</span><br><span class="line">        h, w, c = frame.shape</span><br><span class="line">        blobImage = cv.dnn.blobFromImage(frame, 1.0, (300, 300), (104.0, 177.0, 123.0), False, False);</span><br><span class="line">        net.setInput(blobImage)</span><br><span class="line">        cvOut = net.forward()</span><br><span class="line">        print(cvOut.shape)</span><br><span class="line"></span><br><span class="line">        # Put efficiency information</span><br><span class="line">        t, _ = net.getPerfProfile()</span><br><span class="line">        label = &#x27;Inference time: %.2f ms&#x27; % (t * 1000.0 / cv.getTickFrequency())</span><br><span class="line"></span><br><span class="line">        # 绘制检测矩阵</span><br><span class="line">        for detection in cvOut[0, 0, :, :]:</span><br><span class="line">            score = float(detection[2])</span><br><span class="line">            objIndex = int(detection[1])</span><br><span class="line">            if score &gt; 0.5:</span><br><span class="line">                left = detection[3] * w</span><br><span class="line">                top = detection[4] * h</span><br><span class="line">                right = detection[5] * w</span><br><span class="line">                bottom = detection[6] * h</span><br><span class="line"></span><br><span class="line">                # 绘制矩形框</span><br><span class="line">                cv.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (255, 0, 0),thickness=2)</span><br><span class="line"></span><br><span class="line">                # 绘制类别跟得分</span><br><span class="line">                label_txt = &quot;score: %.2f&quot;%score</span><br><span class="line">                (fw, uph), dh = cv.getTextSize(label_txt, font, font_scale, thickness)</span><br><span class="line">                cv.rectangle(frame, (int(left), int(top) - uph - dh), (int(left) + fw, int(top)), (255, 255, 255), -1, 8)</span><br><span class="line">                cv.putText(frame, label_txt, (int(left), int(top) - dh), font, font_scale, (255, 0, 255), thickness)</span><br><span class="line"></span><br><span class="line">        e2 = cv.getTickCount()</span><br><span class="line">        fps = cv.getTickFrequency() / (e2 - e1)</span><br><span class="line">        cv.putText(frame, label + (&quot; FPS: %.2f&quot;%fps), (10, 50), cv.FONT_HERSHEY_SIMPLEX, 1.0, (0, 0, 255), 2)</span><br><span class="line">        cv.imshow(&#x27;face-detection-demo&#x27;, frame)</span><br><span class="line">        c = cv.waitKey(1)</span><br><span class="line">        if c == 27:</span><br><span class="line">            break</span><br><span class="line">    cv.destroyAllWindows()</span><br><span class="line"></span><br><span class="line"># 图片人脸检测</span><br><span class="line">def image_detection():</span><br><span class="line">    font = cv.FONT_HERSHEY_SIMPLEX</span><br><span class="line">    font_scale = 0.5</span><br><span class="line">    thickness = 1</span><br><span class="line"></span><br><span class="line">    #load tensorflow model</span><br><span class="line">    net = cv.dnn.readNetFromTensorflow(model_bin, config=config_text)</span><br><span class="line">    capture = cv.VideoCapture(0)</span><br><span class="line">    # 人脸检测</span><br><span class="line">    e1 = cv.getTickCount()</span><br><span class="line">    frame = cv.imread(&quot;face.png&quot;)</span><br><span class="line">    h, w, c = frame.shape</span><br><span class="line">    blobImage = cv.dnn.blobFromImage(frame, 1.0, (300, 300), (104.0, 177.0, 123.0), False, False);</span><br><span class="line">    net.setInput(blobImage)</span><br><span class="line">    cvOut = net.forward()</span><br><span class="line">    print(cvOut.shape)</span><br><span class="line"></span><br><span class="line">    # Put efficiency information</span><br><span class="line">    t, _ = net.getPerfProfile()</span><br><span class="line">    label = &#x27;Inference time: %.2f ms&#x27; % (t * 1000.0 / cv.getTickFrequency())</span><br><span class="line"></span><br><span class="line">    # 绘制检测矩阵</span><br><span class="line">    for detection in cvOut[0, 0, :, :]:</span><br><span class="line">        score = float(detection[2])</span><br><span class="line">        objIndex = int(detection[1])</span><br><span class="line">        if score &gt; 0.5:</span><br><span class="line">            left = detection[3] * w</span><br><span class="line">            top = detection[4] * h</span><br><span class="line">            right = detection[5] * w</span><br><span class="line">            bottom = detection[6] * h</span><br><span class="line"></span><br><span class="line">            # 绘制矩形框</span><br><span class="line">            cv.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (255, 0, 0), thickness=2)</span><br><span class="line"></span><br><span class="line">            # 绘制类别跟得分</span><br><span class="line">            label_txt = &quot;score: %.2f&quot; % score</span><br><span class="line">            (fw, uph), dh = cv.getTextSize(label_txt, font, font_scale, thickness)</span><br><span class="line">            cv.rectangle(frame, (int(left), int(top) - uph - dh), (int(left) + fw, int(top)), (255, 255, 255), -1, 8)</span><br><span class="line">            cv.putText(frame, label_txt, (int(left), int(top) - dh), font, font_scale, (255, 0, 255), thickness)</span><br><span class="line"></span><br><span class="line">    e2 = cv.getTickCount()</span><br><span class="line">    fps = cv.getTickFrequency() / (e2 - e1)</span><br><span class="line">    cv.putText(frame, label + (&quot; FPS: %.2f&quot; % fps), (10, 50), cv.FONT_HERSHEY_SIMPLEX, 1.0, (0, 0, 255), 2)</span><br><span class="line">    cv.imshow(&#x27;face-detection-demo&#x27;, frame)</span><br><span class="line">    c = cv.waitKey(0)</span><br><span class="line">    cv.destroyAllWindows()</span><br></pre></td></tr></table></figure><p>​    </p>]]></content>
      
      
      
        <tags>
            
            <tag> CV </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>OpenCV——6</title>
      <link href="/posts/c2e52250.html"/>
      <url>/posts/c2e52250.html</url>
      
        <content type="html"><![CDATA[<h1 id="21图像直方图"><a href="#21图像直方图" class="headerlink" title="21图像直方图"></a>21<strong>图像直方图</strong></h1><h3 id="图像直方图函数"><a href="#图像直方图函数" class="headerlink" title="图像直方图函数"></a>图像直方图函数</h3><p>•<strong>calcHist(images, channels, mask,histSize, ranges[,hist[, accumulate]]) -&gt;hist</strong></p><p>•<strong>images</strong>表示图像</p><p>•<strong>channels</strong>表示通道</p><p>•<strong>mask<em> </em>默认</strong>None</p><p>•<strong>histSzie</strong>表示<strong>bin</strong>的个数，灰度等级</p><p>•<strong>ranges</strong>表示通道的取值范围</p><p>函数返回的直方图数据类型为np.float32</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">#21图像直方图</span><br><span class="line">def image_hist():</span><br><span class="line">    image = cv.imread(&quot;123.jpg&quot;)</span><br><span class="line">    cv.imshow(&quot;input&quot;, image)</span><br><span class="line">    color = (&#x27;blue&#x27;, &#x27;green&#x27;, &#x27;red&#x27;)</span><br><span class="line">    for i,color in enumerate(color):</span><br><span class="line">        hist = cv.calcHist([image], [i], None, [32], [0,256])</span><br><span class="line">        print(hist.dtype)</span><br><span class="line">        plt.plot(hist,color = color)</span><br><span class="line">        plt.xlim([0,32])</span><br><span class="line">    plt.show()</span><br><span class="line">    cv.waitKey(0)</span><br><span class="line">    cv.destroyAllWindows()</span><br></pre></td></tr></table></figure><h1 id="22直方图均衡化"><a href="#22直方图均衡化" class="headerlink" title="22直方图均衡化"></a>22<strong>直方图均衡化</strong></h1><h3 id="均衡化作用"><a href="#均衡化作用" class="headerlink" title="均衡化作用"></a><strong>均衡化作用</strong></h3><p>​    •提升对比度</p><p>​    •灰度图象支持</p><p>所谓均衡化就是减少Bin数量进而扩大各范围的差距</p><h3 id="直方图均衡化函数"><a href="#直方图均衡化函数" class="headerlink" title="直方图均衡化函数"></a>直方图均衡化函数</h3><p>•<strong>cv.equalizeHist(</strong>src<strong>[,<em> </em>dst</strong>]) -&gt;<strong>dst</strong></p><p>•<strong>src</strong>必须是八位单通道图像*</p><p>•<strong>dst</strong>返回结果图像，类型与<strong>src</strong>保持一致</p><h4 id="彩色直方图均衡化可以先转换到HSV空间然后对V通道均衡化（只对亮度通道增强）"><a href="#彩色直方图均衡化可以先转换到HSV空间然后对V通道均衡化（只对亮度通道增强）" class="headerlink" title="彩色直方图均衡化可以先转换到HSV空间然后对V通道均衡化（只对亮度通道增强）"></a>彩色直方图均衡化可以先转换到HSV空间然后对V通道均衡化（只对亮度通道增强）</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">#22直方图均衡化</span><br><span class="line">def hist_equ():</span><br><span class="line">    image = cv.imread(&quot;123.jpg&quot;,cv.IMREAD_GRAYSCALE)</span><br><span class="line">    cv.imshow(&quot;input&quot;,image)</span><br><span class="line">    hist = cv.calcHist([image], [0], None, [32], [0,256])</span><br><span class="line">    print(hist.dtype)</span><br><span class="line">    plt.plot(hist,color = &quot;gray&quot;)</span><br><span class="line">    plt.xlim([0,32])</span><br><span class="line">    plt.show()</span><br><span class="line">    cv.waitKey(0)</span><br><span class="line"></span><br><span class="line">    eqimg = cv.equalizeHist(image)</span><br><span class="line">    hist = cv.calcHist([eqimg], [0], None, [32], [0,256])</span><br><span class="line">    print(hist.dtype)</span><br><span class="line">    plt.plot(hist, color = &quot;gray&quot;)</span><br><span class="line">    plt.xlim([0,32])</span><br><span class="line">    plt.show()</span><br><span class="line">    cv.waitKey(0)</span><br><span class="line">    cv.destroyAllWindows()</span><br></pre></td></tr></table></figure><h1 id="23图像卷积操作"><a href="#23图像卷积操作" class="headerlink" title="23图像卷积操作"></a>23<strong>图像卷积操作</strong></h1><p>卷积的本质是线性组合。</p><h3 id="图像卷积定义"><a href="#图像卷积定义" class="headerlink" title="图像卷积定义"></a>图像卷积定义</h3><p><img src="https://s3.bmp.ovh/imgs/2022/12/22/ed0f74cd8d8871cd.png" alt=""></p><h3 id="卷积的边缘填充"><a href="#卷积的边缘填充" class="headerlink" title="卷积的边缘填充"></a>卷积的边缘填充</h3><p>•边缘处理，边缘填充的方式</p><p>•（1）cv.BORDER_DEFAULT        gfedcb|abcdefgh|gfedcba</p><p>•（2）cv.BORDER_WRAP              cdefgh|abcdefgh|abcdefg</p><p>•（3）cv.BORDER_CONSTANT           iiiiii|abcdefgh|iiiiiii</p><h3 id="卷积模糊函数"><a href="#卷积模糊函数" class="headerlink" title="卷积模糊函数"></a>卷积模糊函数</h3><p>•<strong>cv.blur</strong>( <strong>src</strong>,<strong>ksize</strong>[,<em> <strong>dst</strong>[, anchor[,<strong>borderType</strong>]]]) -&gt;<em>*dst</em></em></p><p>​    •<strong>src</strong>表示输入图像 CV_8U, CV_32F or CV_64F*</p><p>​    •<strong>Ksize</strong>卷积核大小</p><p>​    •<strong>Anchor</strong>锚定位置（被平滑的点），默认值(-1,-1)，如果这个点坐标是负值的话，就表示取核的中心为锚点</p><p>​    •<strong>borderType</strong>边缘处理方式</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">#23图像卷积操作</span><br><span class="line">def conv_demo():</span><br><span class="line">    image = cv.imread(&quot;123.jpg&quot;)</span><br><span class="line">    dst = np.copy(image)</span><br><span class="line">    cv.imshow(&quot;input&quot;,image)</span><br><span class="line">    h, w, c = image.shape</span><br><span class="line">    for row in range(1, h-1, 1):</span><br><span class="line">        for col in range(1, w-1, 1):</span><br><span class="line">            m = cv.mean(image[row-2:row+2, col-2:col+2])</span><br><span class="line">            dst[row, col] = (int(m[0]), int(m[1]), int(m[2]))</span><br><span class="line">    cv.imshow(&quot;convolution-demo&quot;, dst)</span><br><span class="line"></span><br><span class="line">    # blured = cv.blur(image, (5,5), anchor=(-1, -1)) #修改Ksize数值调整模糊程度</span><br><span class="line">    # cv.imshow(&quot;blur-demo&quot;, blured)</span><br><span class="line"></span><br><span class="line">    cv.waitKey(0)</span><br><span class="line">    cv.destroyAllWindows()</span><br></pre></td></tr></table></figure><h1 id="24高斯模糊"><a href="#24高斯模糊" class="headerlink" title="24高斯模糊"></a>24<strong>高斯模糊</strong></h1><h4 id="用高斯公式产生高斯卷积核"><a href="#用高斯公式产生高斯卷积核" class="headerlink" title="用高斯公式产生高斯卷积核"></a>用高斯公式产生高斯卷积核</h4><h4 id="卷积核根据高斯函数生成，权重系数不同"><a href="#卷积核根据高斯函数生成，权重系数不同" class="headerlink" title="卷积核根据高斯函数生成，权重系数不同"></a>卷积核根据高斯函数生成，权重系数不同</h4><p><img src="https://s3.bmp.ovh/imgs/2022/12/22/65c2e59e9e444c64.png" alt=""></p><h3 id="高斯函数"><a href="#高斯函数" class="headerlink" title="高斯函数"></a>高斯函数</h3><p><strong>cv.GaussianBlur(src, ksize, sigmaX[, dst[, sigmaY[, borderType]]]) -&gt;dst</strong></p><p>​    •ksize必须是正数而且是奇数（中心对称）</p><p>​    •sigmaX高斯核函数X方向标准方差</p><p>​    •sigmaY高斯核函数Y方向标准方差,默认0，表示跟sigmaX相同</p><p>​    •ksize==0表示从sigmaX计算生成ksize</p><p>​    •ksize &gt;0 表示从ksize计算生成sigmaX，此时无视signaX所给的值</p><p><img src="https://s3.bmp.ovh/imgs/2022/12/22/ceeff837bc6b4300.png" alt=""></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">#24高斯模糊</span><br><span class="line">def gaussian_blur_demo():</span><br><span class="line">    image = cv.imread(&quot;123.jpg&quot;)</span><br><span class="line">    cv.imshow(&quot;input&quot;,image)</span><br><span class="line">    g1 = cv.GaussianBlur(image, (0, 0), 15)</span><br><span class="line">    g2 = cv.GaussianBlur(image, (15, 15), 15)</span><br><span class="line">    cv.imshow(&quot;GaussianBlur-demo1&quot;, g1)</span><br><span class="line">    cv.imshow(&quot;GaussianBlur-demo2&quot;, g2)</span><br><span class="line"></span><br><span class="line">    cv.waitKey(0)</span><br><span class="line">    cv.destroyAllWindows()</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> CV </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>OpenCV——5</title>
      <link href="/posts/5bec73ea.html"/>
      <url>/posts/5bec73ea.html</url>
      
        <content type="html"><![CDATA[<h1 id="17鼠标响应与操作"><a href="#17鼠标响应与操作" class="headerlink" title="17鼠标响应与操作"></a>17鼠标响应与操作</h1><p><img src="https://s3.bmp.ovh/imgs/2022/12/20/0aa42ce71379b7b7.png" alt=""></p><p><img src="https://s3.bmp.ovh/imgs/2022/12/20/993a2335eb6f2248.png" alt=""></p><h4 id="•回调函数参数-int-event-int-x-int-y-int-flags-void-userdata"><a href="#•回调函数参数-int-event-int-x-int-y-int-flags-void-userdata" class="headerlink" title="•回调函数参数: int event, int x, int y, int flags, void **userdata*"></a>•回调函数参数: <em>int</em> <em>event,</em> <em>int</em> <em>x,</em> <em>int</em> <em>y,</em> <em>int</em> <em>flags, void **</em>userdata*</h4><p>•<strong>Event</strong>表示鼠标事件</p><p>•<strong>(x, y)</strong>表示当前鼠标位置</p><p>•<strong>Flags</strong>表示鼠标状态</p><p>•<strong>Userdata</strong>表示回调用户数据，可以为空</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">b1 = cv.imread(&quot;123.jpg&quot;)</span><br><span class="line">img = np.copy(b1)</span><br><span class="line">x1 = -1</span><br><span class="line">y1 = -1</span><br><span class="line">x2 = -1</span><br><span class="line">y2 = -1</span><br><span class="line">def mouse_drawing(event, x, y, flags, param):</span><br><span class="line">    global x1, y1, x2, y2</span><br><span class="line">    if event == cv.EVENT_LBUTTONDOWN:</span><br><span class="line">        x1 = x</span><br><span class="line">        y1 = y</span><br><span class="line">    if event == cv.EVENT_MOUSEMOVE:</span><br><span class="line">        if x1 &lt;0 or y1 &lt;0:</span><br><span class="line">            return</span><br><span class="line">        x2 = x</span><br><span class="line">        y2 = y</span><br><span class="line">        dx = x2 - x1</span><br><span class="line">        dy = y2 - y1</span><br><span class="line">        if dx &gt; 0 and dy &gt; 0:</span><br><span class="line">            b1[:,:,:] = img[:,:,:]</span><br><span class="line">            cv.rectangle(b1, (x1, y1), (x2, y2), (0, 0, 255), 2, 8, 0)</span><br><span class="line">    if event == cv.EVENT_LBUTTONUP:</span><br><span class="line">        x2 = x</span><br><span class="line">        y2 = y</span><br><span class="line">        dx = x2 - x1</span><br><span class="line">        dy = y2 - y1</span><br><span class="line">        if dx &gt; 0 and dy &gt; 0:</span><br><span class="line">            b1[:,:,:] = img[:,:,:]</span><br><span class="line">            cv.rectangle(b1, (x1, y1), (x2, y2), (0, 0, 255), 2, 8, 0)</span><br><span class="line">        x1 = -1</span><br><span class="line">        x2 = -1</span><br><span class="line">        y1 = -1</span><br><span class="line">        y2 = -1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def mouse_demo():</span><br><span class="line">    cv.namedWindow(&quot;mouse_demo&quot;,cv.WINDOW_AUTOSIZE)</span><br><span class="line">    cv.setMouseCallback(&quot;mouse_demo&quot;,mouse_drawing)</span><br><span class="line">    while True:</span><br><span class="line">        cv.imshow(&quot;mouse_demo&quot;,b1)</span><br><span class="line">        c = cv.waitKey(10)</span><br><span class="line">        if c == 27:</span><br><span class="line">            break</span><br><span class="line">    cv.destroyAllWindows()</span><br></pre></td></tr></table></figure><h1 id="18图像像素类型转换与归一化"><a href="#18图像像素类型转换与归一化" class="headerlink" title="18图像像素类型转换与归一化"></a>18<strong>图像像素类型转换与归一化</strong></h1><p><img src="https://s3.bmp.ovh/imgs/2022/12/20/e02e0be369644a53.png" alt=""></p><h4 id="归一化函数"><a href="#归一化函数" class="headerlink" title="归一化函数"></a>归一化函数</h4><p>•cv.normalize( src, dst[, alpha[, beta[, norm_type[, dtype[, mask]]]]] ) -&gt; dst</p><p>•src表示输入图像, dst表示输出图像</p><p>•alpha, beta 默认是1， 0，是归一化的区间值</p><p>•norm_type默认是NORM_L2,</p><p>•norm_type常用是NORM_MINMAX</p><p>Imread读入默认是uint8, 转换为float32,通过imshow显示之前，必须归一化到[0~1]之间。</p><p>把float32的归一化图像转换为uint8类型：np.uint8(image*255)</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">#归一化</span><br><span class="line">def trackbar_callback(pos):</span><br><span class="line">    print(pos)</span><br><span class="line"></span><br><span class="line">def norm_demo():</span><br><span class="line">    image_unit8 = cv.imread(&quot;123.jpg&quot;)</span><br><span class="line">    cv.imshow(&quot;image_uint8&quot;,image_unit8)</span><br><span class="line">    img_f32 = np.float32(image_unit8)</span><br><span class="line">    # cv.imshow(&quot;img_f32&quot;,img_f32)</span><br><span class="line">    # cv.normalize(img_f32, img_f32, 1, 0, cv.NORM_MINMAX)</span><br><span class="line">    # cv.imshow(&quot;norm-img_f32&quot;,img_f32)</span><br><span class="line">    # cv.waitKey(0)</span><br><span class="line">    # cv.destroyAllWindows()</span><br><span class="line"></span><br><span class="line">    cv.namedWindow(&quot;norm_demo&quot;,cv.WINDOW_AUTOSIZE)</span><br><span class="line">    cv.createTrackbar(&quot;mormtype&quot;, &quot;norm_demo&quot;, 10, 3, trackbar_callback)</span><br><span class="line">    while True:</span><br><span class="line">        dst = np.float32(image_unit8)</span><br><span class="line">        pos = cv.getTrackbarPos(&quot;normtype&quot;,&quot;norm-demo&quot;)</span><br><span class="line">        if pos == 0:</span><br><span class="line">            cv.normalize(dst, dst, 1, 0, cv.NORM_MINMAX)</span><br><span class="line">        if pos == 1:</span><br><span class="line">            cv.normalize(dst, dst, 1, 0, cv.NORM_L1)</span><br><span class="line">        if pos == 2:</span><br><span class="line">            cv.normalize(dst, dst, 1, 0, cv.NORM_L2)</span><br><span class="line">        if pos == 3:</span><br><span class="line">            cv.normalize(dst, dst, 1, 0, cv.NORM_INF)</span><br><span class="line">        cv.imshow(&quot;norm-demo&quot;,img_f32)</span><br><span class="line">        c = cv.waitKey(50)</span><br><span class="line">        if c == 27:</span><br><span class="line">            break</span><br><span class="line">    cv.destroyAllWindows()</span><br></pre></td></tr></table></figure><h1 id="19图像几何变换"><a href="#19图像几何变换" class="headerlink" title="19图像几何变换"></a>19<strong>图像几何变换</strong></h1><p>•cv.warpAffine(src, M, dsize[, dst[, flags[, borderMode[, borderValue]]]] ) -&gt; dst</p><p><img src="https://s3.bmp.ovh/imgs/2022/12/20/d506a88ea5e4090d.png" alt=""></p><p>•src表示输入图像</p><p>•M 表示2x3变换矩阵</p><p>•dsize表示目标图像dst的大小</p><p>•支持平移变换、放缩变换、旋转变换</p><p><img src="https://s3.bmp.ovh/imgs/2022/12/20/640c93c7d43f08e6.png" alt=""></p><p><img src="https://s3.bmp.ovh/imgs/2022/12/20/cb7ad0f5308870fa.png" alt=""></p><p><img src="https://s3.bmp.ovh/imgs/2022/12/20/a9105cbaf0538d38.png" alt=""></p><h3 id="获取旋转矩阵"><a href="#获取旋转矩阵" class="headerlink" title="获取旋转矩阵"></a>获取旋转矩阵</h3><p>•旋转矩阵获取cv.getRotationMatrix2D</p><p>•Center表示旋转中心, angle表示度数，大于零表示逆时针旋转, scale表示放缩尺度大小。</p><h3 id="翻转与特殊角度旋转"><a href="#翻转与特殊角度旋转" class="headerlink" title="翻转与特殊角度旋转"></a>翻转与特殊角度旋转</h3><p>•cv.flip(src, flipCode[, dst] ) -&gt;dst</p><p>•cv.rotate(src, rotateCode[, dst] ) -&gt; dst</p><p>•src表示输入图像</p><p>•flipCode支持0水平、1垂直，-1对角线翻转，</p><p>•rotateCode支持旋转90°，180°，270°</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">#图像几何变换</span><br><span class="line">def affine_demo():</span><br><span class="line">    image = cv.imread(&quot;123.jpg&quot;)</span><br><span class="line">    h, w, c = image.shape</span><br><span class="line">    cx = int(w / 2)</span><br><span class="line">    cy = int(h / 2)</span><br><span class="line">    cv.imshow(&quot;image&quot;,image)</span><br><span class="line"></span><br><span class="line">    M = np.zeros((2,3), dtype=np.float32)</span><br><span class="line">    M[0, 0] = .7</span><br><span class="line">    M[1, 1] = .7</span><br><span class="line">    M[0, 2] = 0</span><br><span class="line">    M[1, 2] = 0</span><br><span class="line">    print(&quot;M(2x3) = \n&quot;, M)</span><br><span class="line">    dst = cv.warpAffine(image, M, (int(w*.7), int(h*.7)))</span><br><span class="line">    cv.imshow(&quot;rescale-demo&quot;,dst)</span><br><span class="line">    cv.imwrite(&quot;result.png&quot;,dst)</span><br><span class="line"></span><br><span class="line">    #获取旋转矩阵，degree &gt; 0 表示逆时针旋转，原点在左上角</span><br><span class="line">    M = cv.getRotationMatrix2D((w/2, h/2), 45.0, 1.0)</span><br><span class="line">    dst = cv.warpAffine(image, M, (w,h))</span><br><span class="line">    cv.imshow(&quot;rotate-demo&quot;,dst)</span><br><span class="line"></span><br><span class="line">    dst = cv.flip(image, 0)</span><br><span class="line">    cv.imshow(&quot;flip-demo&quot;,dst)</span><br><span class="line"></span><br><span class="line">    cv.waitKey(0)</span><br><span class="line">    cv.destroyAllWindows()</span><br></pre></td></tr></table></figure><h1 id="20视频读写处理"><a href="#20视频读写处理" class="headerlink" title="20视频读写处理"></a>20<strong>视频读写处理</strong></h1><h3 id="视频标准与格式"><a href="#视频标准与格式" class="headerlink" title="视频标准与格式"></a>视频标准与格式</h3><p>•SD(Standard Definition)标清480P</p><p>•HD(High Definition)高清720P/1080P</p><p>•UHD(Ultra High Definition)超高清4K/2160P</p><p>•分辨率表示</p><p>•SD-640x480, 704x480, 720x480, 848x480等</p><p>•HD-960x720,1280x720,1440x1080,1920x1080</p><p>•UHD-4K,2160P</p><h3 id="视频读取函数"><a href="#视频读取函数" class="headerlink" title="视频读取函数"></a>视频读取函数</h3><p><code>cv.VideoCapture ( filename, index, apiPreference)</code></p><p>•filename表示视频文件</p><p>•Index表示USB摄像头或者web camera的索引</p><p>•apiPreference = CAP_ANY意思自动决定第三方视频库如： cv.CAP_FFMPEG， cv.CAP_DSHOW</p><h3 id="查询视频属性"><a href="#查询视频属性" class="headerlink" title="查询视频属性"></a>查询视频属性</h3><p>•VideoCaput的get方法</p><p>•cv.CAP_PROP_FRAME_WIDT</p><p>•cv.CAP_PROP_FRAME_HEIGHT</p><p>•cv.CAP_PROP_FPS（对视频流来说是0）</p><p>•cv.CAP_PROP_FOURCC</p><p>•cv.CAP_PROP_FRAME_COUNT</p><h3 id="视频文件保存"><a href="#视频文件保存" class="headerlink" title="视频文件保存"></a>视频文件保存</h3><p>cv.VideoWriter( </p><p>​    filename, 保存文件名称</p><p>​    fourcc, 编码方式</p><p>​    fps, 帧率</p><p>​    frameSize 视频帧大小，<strong>与实现大小相符</strong>，否则无法保存</p><p>​    [, isColor] )</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">#20视频读写处理</span><br><span class="line">def video_demo():</span><br><span class="line">    cap = cv.VideoCapture(&quot;456.avi&quot;)</span><br><span class="line">    # query video file metadata</span><br><span class="line">    fps = cap.get(cv.CAP_PROP_FPS)</span><br><span class="line">    frame_w = cap.get(cv.CAP_PROP_FRAME_WIDTH)</span><br><span class="line">    frame_h = cap.get(cv.CAP_PROP_FRAME_HEIGHT)</span><br><span class="line">    print(fps, frame_w, frame_h)</span><br><span class="line">    # encode mode</span><br><span class="line">    # fourcc =cv.VideoWriter_fourcc(*&#x27;vp09&#x27;)</span><br><span class="line">    fourcc = cap.get(cv.CAP_PROP_FOURCC)</span><br><span class="line">    # create Video writer</span><br><span class="line">    writer = cv.VideoWriter(&#x27;output.mp4&#x27;, int(fourcc), fps, (int(frame_w), int(frame_h)))</span><br><span class="line">    # loop read frame until last frame</span><br><span class="line">    while True:</span><br><span class="line">        ret, frame = cap.read()</span><br><span class="line">        if ret is not True:</span><br><span class="line">            break</span><br><span class="line">        hsv = cv.cvtColor(frame, cv.COLOR_BGR2HSV)</span><br><span class="line">        cv.imshow(&quot;hsv&quot;,hsv)</span><br><span class="line">        cv.imshow(&quot;frame&quot;,frame)</span><br><span class="line">        c = cv.waitKey(1)</span><br><span class="line">        if c == 27:</span><br><span class="line">            break</span><br><span class="line">        writer.write(frame)</span><br><span class="line"></span><br><span class="line">    # release camera resource</span><br><span class="line">    cap.release()</span><br><span class="line">    writer.release()</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> CV </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>OpenCV——4</title>
      <link href="/posts/2ceb437c.html"/>
      <url>/posts/2ceb437c.html</url>
      
        <content type="html"><![CDATA[<h1 id="13图像统计信息"><a href="#13图像统计信息" class="headerlink" title="13图像统计信息"></a>13图像统计信息</h1><h4 id="像素值统计-均值"><a href="#像素值统计-均值" class="headerlink" title="像素值统计-均值"></a>像素值统计-均值</h4><p><code>•cv.mean(src[, mask] ) -&gt;retval</code></p><h4 id="像素值统计-方差"><a href="#像素值统计-方差" class="headerlink" title="像素值统计-方差"></a>像素值统计-方差</h4><p><code>•cv.meanStdDev(src[, mean[, stddev[, mask]]]) -&gt;mean, stddev</code></p><h4 id="像素值统计-极值"><a href="#像素值统计-极值" class="headerlink" title="像素值统计-极值"></a>像素值统计-极值</h4><p><code>•cv.minMaxLoc(src[, mask]) -&gt;minVal, maxVal, minLoc, maxLoc</code></p><h5 id="•src表示输入图像-mask表示计算区域"><a href="#•src表示输入图像-mask表示计算区域" class="headerlink" title="•src表示输入图像,mask表示计算区域"></a>•src表示输入图像,mask表示计算区域</h5><h5 id="•mean-stddev-minVal-maxVal分别表示均值，标准方差，最小与最大"><a href="#•mean-stddev-minVal-maxVal分别表示均值，标准方差，最小与最大" class="headerlink" title="•mean, stddev, minVal, maxVal分别表示均值，标准方差，最小与最大"></a>•mean, stddev, minVal, maxVal分别表示均值，标准方差，最小与最大</h5><h5 id="•cv2-convertScaleAbs-函数通过线性变换将数据转为均值，然后转换成8位-uint8"><a href="#•cv2-convertScaleAbs-函数通过线性变换将数据转为均值，然后转换成8位-uint8" class="headerlink" title="•cv2.convertScaleAbs()函数通过线性变换将数据转为均值，然后转换成8位[uint8]"></a>•cv2.convertScaleAbs()函数通过线性变换将数据转为均值，然后转换成8位[uint8]</h5><h4 id="每个通道分别计算均值和方差"><a href="#每个通道分别计算均值和方差" class="headerlink" title="每个通道分别计算均值和方差"></a>每个通道分别计算均值和方差</h4><h4 id="通过图像方差判断是否含有有效信息"><a href="#通过图像方差判断是否含有有效信息" class="headerlink" title="通过图像方差判断是否含有有效信息"></a>通过图像方差判断是否含有有效信息</h4><h4 id="调整图像对比度的本质是调整图像之间的差值"><a href="#调整图像对比度的本质是调整图像之间的差值" class="headerlink" title="调整图像对比度的本质是调整图像之间的差值"></a>调整图像对比度的本质是调整图像之间的差值</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">import cv2</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">image = cv2.imread(&quot;123.jpg&quot;)</span><br><span class="line">cv2.imshow(&quot;demo1&quot;,image)</span><br><span class="line">bgr_m = cv2.mean(image)</span><br><span class="line">sub_m = np.float32(image)[:, :] - (bgr_m[0],bgr_m[1],bgr_m[2])</span><br><span class="line">result = sub_m * 0.5</span><br><span class="line">result = result[:, :] + (bgr_m[0],bgr_m[1],bgr_m[2])</span><br><span class="line">cv2.imshow(&quot;低对比度&quot;,cv2.convertScaleAbs(result))</span><br><span class="line"></span><br><span class="line"># result = sub_m *  2.0</span><br><span class="line"># result = result[:, :] + (bgr_m[0],bgr_m[1],bgr_m[2])</span><br><span class="line"># cv2.imshow(&quot;高对比度&quot;,cv2.convertScaleAbs(result))</span><br><span class="line"></span><br><span class="line">cv2.waitKey(0)</span><br><span class="line">cv2.destroyAllWindows()</span><br></pre></td></tr></table></figure><h1 id="14图像几何形状绘制"><a href="#14图像几何形状绘制" class="headerlink" title="14图像几何形状绘制"></a>14<strong>图像几何形状绘制</strong></h1><p>•支持绘制线、矩形、圆形</p><p>•支持填充矩形、圆形、椭圆</p><p>•支持绘制文本（不支持中文）</p><p>•相关函数<em>cv.line()<strong>、</strong>cv.circle<strong>()</strong>、<strong>cv.rectangle</strong>()<strong>、</strong>cv.ellipse<strong>()</strong>、<strong>cv.putText</strong>()</em></p><p>•相关参数解释：</p><p><strong>•img</strong>表示输入图像</p><p><strong>•color</strong>表示颜色，如(255, 0,0)表示蓝色（必须与img的通道匹配）</p><p><strong>•thickness</strong>表示线宽, 大于0表示绘制，小于0表示填充</p><p><strong>•lineType</strong>表示渲染模式, 默认LINE_8（渲染周围8个点即8连通像素，性能有限使用）, LINE_AA表示反锯齿（质量更高）</p><h4 id="文本绘制"><a href="#文本绘制" class="headerlink" title="文本绘制"></a>文本绘制</h4><p>•<strong>putText</strong> 默认只支持英文</p><p><strong>•org</strong>表示文字起始坐标点</p><p><strong>•fontFace</strong>表示字体类型</p><p><strong>•fontScale</strong>表示字体大小</p><h4 id="计算文本区域大小"><a href="#计算文本区域大小" class="headerlink" title="计算文本区域大小"></a>计算文本区域大小</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">#函数计算文本区域大小函数</span><br><span class="line">getTextSize(</span><br><span class="line"></span><br><span class="line">text, # 表示文本信息</span><br><span class="line"></span><br><span class="line">fontFace, # 表示字体类型</span><br><span class="line"></span><br><span class="line">fontScale, # 表示字体大小</span><br><span class="line"></span><br><span class="line">thickness # 表示线宽</span><br><span class="line">) </span><br><span class="line">#返回文本信息区域大小，与字体的基线baseline位置</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">def paint():</span><br><span class="line">    canvas = np.zeros((512,512,3),dtype=np.uint8)</span><br><span class="line"></span><br><span class="line">    #动态合理显示文本区域</span><br><span class="line">    font_color = (140,199,0) #框颜色</span><br><span class="line">    cv.rectangle(canvas,(100,100),(300,300),font_color,2,8) #框</span><br><span class="line"></span><br><span class="line">    label_txt = &quot;OpenCV&quot;</span><br><span class="line">    font = cv.FONT_HERSHEY_SIMPLEX  #字体</span><br><span class="line">    font_scale = 0.5  #字体大小</span><br><span class="line">    thickness = 1     #线宽</span><br><span class="line">    (fw, uph),dh = cv.getTextSize(label_txt,font,font_scale,thickness)</span><br><span class="line">    cv.rectangle(canvas,(100,100-uph-dh),(100+fw,100),(255,255,255),-1,8)</span><br><span class="line">    cv.putText(canvas,label_txt,(100,100-dh),font,font_scale,(255,0,255),thickness)</span><br><span class="line">    cv.imshow(&quot;canvas&quot;,canvas)</span><br><span class="line">    cv.waitKey(0)</span><br><span class="line"></span><br><span class="line">    cv.waitKey(0)</span><br><span class="line">    cv.destroyAllWindows()</span><br></pre></td></tr></table></figure><h4 id="rectangle函数"><a href="#rectangle函数" class="headerlink" title="rectangle函数"></a>rectangle函数</h4><p>cv2.rectangle(img, pt1, pt2, color, thickness, lineType, shift )</p><p>参数表示依次为：(图片，长方形框左上角坐标, 长方形框右下角坐标， 字体颜色，字体粗细）</p><p>在图片img上画长方形，坐标原点是图片左上角，向右为x轴正方向，向下为y轴正方向。左上角（x，y），右下角（x，y） ，颜色(B,G,R), 线的粗细</p><h1 id="15随机数与随机颜色"><a href="#15随机数与随机颜色" class="headerlink" title="15随机数与随机颜色"></a>15<strong>随机数与随机颜色</strong></h1><h4 id="Numpy随机数"><a href="#Numpy随机数" class="headerlink" title="Numpy随机数"></a>Numpy随机数</h4><p>•random.randint(low, high=None, size=None, dtype=int)</p><p>•Low表低值，high表示高值，size表示维度，dtype表示类型</p><p>•np.random.randint(256)</p><p>•np.random.randint(0, 256)</p><p>•表示产生0~255随机数，类型是int</p><p>•np.random.randint(0, 256, size=3) #size表示生成随机数的数量，用数组表示</p><h4 id="随机噪声图"><a href="#随机噪声图" class="headerlink" title="随机噪声图"></a>随机噪声图</h4><p>•cv.randn(dst, mean, stddev)</p><p>•生成目标图像dst</p><p>•噪声均值mean</p><p>•噪声方差stddev</p><p>•cv.randn(canvas, (40, 200, 140), (10, 50, 10)) #参数：图像，均值，方差</p><h4 id="代码演示"><a href="#代码演示" class="headerlink" title="代码演示"></a>代码演示</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">def rando():</span><br><span class="line">    canvas = np.zeros((512, 512, 3), dtype=np.uint8)</span><br><span class="line">    # random draw</span><br><span class="line">    while True:</span><br><span class="line">        b,g,r = np.random.randint(0, 256, size=3)</span><br><span class="line">        x1 = np.random.randint(0, 512)</span><br><span class="line">        x2 = np.random.randint(0, 512)</span><br><span class="line">        y1 = np.random.randint(0, 512)</span><br><span class="line">        y2 = np.random.randint(0, 512)</span><br><span class="line">        cv.rectangle(canvas,(x1,y1), (x2, y2), (int (b), int(g), int (r)), -1, 8)</span><br><span class="line">        cv.imshow( &quot;canvas&quot;,canvas)</span><br><span class="line">        c = cv.waitKey(50)</span><br><span class="line">        if c == 27:</span><br><span class="line">            break</span><br><span class="line">        cv.rectangle(canvas, (0,0), (512, 512), (0, 0, 0),-1, 8)</span><br></pre></td></tr></table></figure><h1 id="16多边形填充与绘制"><a href="#16多边形填充与绘制" class="headerlink" title="16多边形填充与绘制"></a>16多边形填充与绘制</h1><h4 id="绘制函数"><a href="#绘制函数" class="headerlink" title="绘制函数"></a>绘制函数</h4><p>•cv.fillPoly(img, pts, color[, lineType[, shift[, offset]]]) -&gt;img</p><p>•填充多边形</p><p>•cv.polylines(img, pts, isClosed, color[, <em>thickness</em>[, lineType[, shift]]] ) -&gt;img</p><p>•绘制多边形</p><p>•pts表示一个或者多个点集，polylines支持一次绘制多个多边形</p><p>•color表示颜色</p><p>•thickness表示线宽，<strong>注意：</strong>必须大于0</p><p>•lineType 表示渲染方式</p><h4 id="点集支持"><a href="#点集支持" class="headerlink" title="点集支持"></a>点集支持</h4><p>•pts表示一个或者多个点集</p><p>•pts = []</p><p>•pts.append((100, 100))</p><p>•pts.append((200, 50))</p><p>•pts.append((280, 100))</p><p>•pts.append((290, 300))</p><p>•pts.append((50, 300))</p><p>•pts = np.asarray(pts, dtype=np.int32)</p><p>•print(pts.shape)</p><p><strong>•要求：必须是CV_32S, 对应np.int32</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">def paintmore():</span><br><span class="line">    canvas = np.zeros((512, 512, 3), dtype=np.uint8)</span><br><span class="line">    pts =[]</span><br><span class="line">    pts.append((100, 100))</span><br><span class="line">    pts.append((200, 50))</span><br><span class="line">    pts.append((280, 100))</span><br><span class="line">    pts.append((290, 300))</span><br><span class="line">    pts.append((50, 300))</span><br><span class="line">    pts = np.asarray(pts, dtype=np.int32) #必须是int32</span><br><span class="line">    print(pts.shape)</span><br><span class="line"></span><br><span class="line">    pts2 = []</span><br><span class="line">    pts2.append((300, 300))</span><br><span class="line">    pts2.append((400, 250))</span><br><span class="line">    pts2.append((500, 300))</span><br><span class="line">    pts2.append((500, 500))</span><br><span class="line">    pts2.append((250, 500))</span><br><span class="line">    pts2 = np.asarray(pts2, dtype=np.int32)</span><br><span class="line">    print(pts2.shape)</span><br><span class="line"></span><br><span class="line">    cv.polylines(canvas, [pts, pts2], True, (0, 0, 255), 2, 8)</span><br><span class="line">    cv.fillPoly(canvas, [pts, pts2], (255, 0, 0), 8, 0)</span><br><span class="line">    cv.imshow(&quot;poly-demo&quot;, canvas)</span><br><span class="line">    cv.waitKey(0)</span><br><span class="line">    cv.destroyAllWindows()</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> CV </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>OpenCV——3</title>
      <link href="/posts/b28fd6df.html"/>
      <url>/posts/b28fd6df.html</url>
      
        <content type="html"><![CDATA[<h2 id="09滚动条操作"><a href="#09滚动条操作" class="headerlink" title="09滚动条操作"></a>09滚动条操作</h2><h4 id="Callback回调基本流程"><a href="#Callback回调基本流程" class="headerlink" title="Callback回调基本流程"></a>Callback回调基本流程</h4><p><img src="https://s3.bmp.ovh/imgs/2022/12/16/f5830e4e8f2ee9c9.png" alt=""></p><p>引用举例：你到一个商店买东西，刚好你要的东西没有货，于是你在店员那里留下了你的电话，过了几天店里有货了，店员就打了你的电话，然后你接到电话后就到店里去取了货。在这个例子里，你的电话号码就叫回调函数，你把电话留给店员就叫登记回调函数，店里后来有货了叫做触发了回调关联的事件，店员给你打电话叫做调用回调函数，你到店里去取货叫做响应回调事件。回答完毕。 (链接：<a href="https://www.zhihu.com/question/19801131">https://www.zhihu.com/question/19801131</a>)</p><p>先注册后使用</p><h4 id="事件响应函数"><a href="#事件响应函数" class="headerlink" title="事件响应函数"></a>事件响应函数</h4><p>•<code>typedef void(* cv::TrackbarCallback) (int pos//滑块位置, void *userdata//用户数据，可不写)</code></p><p>•完成事件响应函数的声明与实现</p><p><code>•def trackbar_callback (pos):</code></p><p>​    <code>print(pos)</code></p><h4 id="创建窗口函数"><a href="#创建窗口函数" class="headerlink" title="创建窗口函数"></a>创建窗口函数</h4><p>•<code>cv.namedWindow(winname [, flags]) -&gt; None</code></p><p>•参数: winname表示窗口标题</p><p>•参数flags支持的flag有：</p><p>​    WINDOW_NORMAL – 可以调整窗口大小，图片很大时使用</p><p>​    WINDOW_AUTOSIZE – 根据图像大小自动适应，不可调</p><p>​    WINDOW_KEEPRATIO – 可以保持比例窗口，调整大小</p><h4 id="调整图像亮度"><a href="#调整图像亮度" class="headerlink" title="调整图像亮度"></a>调整图像亮度</h4><p>•RGB值表示亮度</p><p>•RGB(0, 0,0) 黑色 -&gt; RGB(255,255,255)白色，通过调整像素值来调整亮度</p><p>•add函数支持图像+图像与图像+常量方式</p><p>•subtract函数支持图像+图像与图像+常量方式</p><p>•动态调整，基于滚动条修改常量值，实现动态修改图像亮度并刷新显示</p><p>•创建图像窗口</p><p>•创建滚动条组件</p><p>•在窗口显示图像</p><p>•拖拉滚动条修改图像亮度</p><p><img src="https://z4a.net/images/2022/12/16/222.png" alt=""></p><h2 id="10键盘响应操作"><a href="#10键盘响应操作" class="headerlink" title="10键盘响应操作"></a>10键盘响应操作</h2><h4 id="键盘响应事件"><a href="#键盘响应事件" class="headerlink" title="键盘响应事件"></a>键盘响应事件</h4><p>•cv.waitKey( [, delay] ) -&gt;retval</p><p>​    delay如果没有声明或者delay=0,表示一直阻塞</p><p>​    delay大于0，表示阻塞指定毫秒数</p><p>​    Retval返回的对应键盘键值，注意:在不同的操作系统中可能会有差异</p><p>​    典型的retval = 27是ESC按键</p><h4 id="响应不同的键盘操作"><a href="#响应不同的键盘操作" class="headerlink" title="响应不同的键盘操作"></a>响应不同的键盘操作</h4><p>•检查返回键值，根据不同键值完成不同操作</p><p>•推荐使用if-elif-else, switch-case方式python3.10支持</p><p>if <expr>:</p>  <statement(s)><p>elif <expr>:</p>  <statement(s)><p>elif <expr>:</p>  <statement(s)><p>  …</p><p>else:</p>  <statement(s)><p><img src="https://z4a.net/images/2022/12/16/333.png" alt=""></p><p>•按ESC推出</p><p>•按1显示HSV图像</p><p>•按2显示YCrCb</p><p>•按3显示RGB图像</p><p>•按0恢复原图BGR显示</p><h2 id="11自带颜色表操作"><a href="#11自带颜色表操作" class="headerlink" title="11自带颜色表操作"></a>11自带颜色表操作</h2><p>查找表（LUT，look up table）</p><p>优势：预计算，空间换时间，避免重复计算，节约计算时间</p><h4 id="Gamma校正"><a href="#Gamma校正" class="headerlink" title="Gamma校正"></a>Gamma校正</h4><p>•公式p(x, y)表示输入图像像素值</p><p><img src="https://z4a.net/images/2022/12/16/444.png" alt=""></p><p>•像素值取值范围在0~255之间，每一个值对应一个输出值，这样映射关系，可以先建立查找表LUT</p><p>•根据输入得像素值作为index，在LUT中直接映射读取得到gamma校正之后得值</p><p>•对256x256大小的图像，计算量对比：</p><p>•不应用找表计算gamma - 65536次，</p><p>•应用查找表计算gamma – 256次</p><h4 id="OpenCV中LUT支持"><a href="#OpenCV中LUT支持" class="headerlink" title="OpenCV中LUT支持"></a>OpenCV中LUT支持</h4><p>•cv.applyColorMap(src, colormap[, dst]) -&gt;dst</p><p>•第一个参数输入图像</p><p>•第二个参数是颜色表</p><p>•dst返回图像</p><p><img src="https://z4a.net/images/2022/12/16/555.png" alt=""></p><p><img src="https://z4a.net/images/2022/12/16/666.png" alt=""></p><p>系统查找表使用cv.applyColorMap，自定义查找表使用cv.LUT</p><p>自定义colormap大小必须为256x1</p><h2 id="12通道分离与合并"><a href="#12通道分离与合并" class="headerlink" title="12通道分离与合并"></a>12通道分离与合并</h2><h4 id="通道分类与合并"><a href="#通道分类与合并" class="headerlink" title="通道分类与合并"></a>通道分类与合并</h4><p>RGB/HSV彩色通道分离为单独通道</p><p>针对不同通道使用不同阈值提取mask</p><h4 id="分离函数"><a href="#分离函数" class="headerlink" title="分离函数"></a>分离函数</h4><p>•通道分离函数cv.split(m[, mv]) -&gt;mv</p><p>•m表示输入图像,必须是多通道图像</p><p>•mv表示输出分离的单通道数组</p><h4 id="合并与混合"><a href="#合并与混合" class="headerlink" title="合并与混合"></a>合并与混合</h4><p>•cv.merge(mv[, dst])-&gt;dst</p><p>​    mv表示各个通道</p><p>•cv.mixChannels(src, dst, fromTo)-&gt;dst</p><p>​    src表示输入多通道图像</p><p>​    fromTo表示通道索引</p><p>​    dst表示返回结果</p><h4 id="通道阈值"><a href="#通道阈值" class="headerlink" title="通道阈值"></a>通道阈值</h4><p>•cv.inRange( src, lowerb, upperb[, dst]) -&gt; dst</p><p>转为二值图</p><p>•其中src是输入图像</p><p>•Lowerb是低值</p><p>•Upperb是高值</p><p>•dst = (lowerb &lt; src &lt; upperb)</p><p>范围内的为1（白色），范围外的为0（黑色）</p><p><img src="https://z4a.net/images/2022/12/16/777.png" alt=""></p>]]></content>
      
      
      
        <tags>
            
            <tag> CV </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>OpenCV——2</title>
      <link href="/posts/c588e649.html"/>
      <url>/posts/c588e649.html</url>
      
        <content type="html"><![CDATA[<h2 id="05-图像色彩空间转换"><a href="#05-图像色彩空间转换" class="headerlink" title="05 图像色彩空间转换"></a>05 图像色彩空间转换</h2><p><strong>常见的色彩空间：HSV、RGB、YCrCb</strong></p><p>​    RGB色彩空间，设备独立</p><p>​    HSV色彩空间，对计算机友好，区分各种色彩</p><p>​    YCrCb, Y分量表示信息，CrCb可以被压缩</p><p>​    RGB是计算机显示器的标准支持色彩系统</p><p>​    RGB的取值范围0~255</p><p>​    HSV取值范围H（色调）:0°~360°，S（饱和度）:0~255，V（明度）:0~255</p><p><strong>从一个色彩空间转换到另外一个色彩空间</strong>要考虑：</p><p>​    信息传递与损失过程、可逆与不可逆</p><p><strong>函数与参数</strong></p><p>​    cv.cvtColor(<strong>src</strong>,<strong>code</strong>[,dst[,dstCn]])-&gt;dst</p><p>​    <strong>·</strong> src表示输入图像, 类型CV_8U、CV_32F</p><p>​    <strong>·</strong> code表示， </p><p>​        cv::COLOR_BGR2RGB = 4</p><p>​        cv::COLOR_BGR2GRAY = 6</p><p>​        cv::COLOR_GRAY2BGR = 8</p><p>​        cv::COLOR_BGR2HSV = 40</p><p>​        例：img2 = cv.cvtColor(img1, cv.COLOR_BGR2GRAY)</p><p>​        注：当彩色图像转为灰度图像，由三通道转为单通道，其部分信息永久消失，再次转回BGR后图像变为三通道，但仍为灰色。</p><h2 id="06-图像对象的创建与赋值"><a href="#06-图像对象的创建与赋值" class="headerlink" title="06 图像对象的创建与赋值"></a>06 图像对象的创建与赋值</h2><p><strong>① OpenCV-Python支持的数据类型</strong>：np.uint8（默认）、np.float32（方便计算）、np.int32、np.int64</p><p><strong>② Numpy常用函数：</strong></p><p>​    numpy.array、numpy.zeros、numpy.zeros_like（快速产生与读入图像尺寸相同的纯黑图像）、numpy.asarray（将普通数组转为NumpyArray）、numpy.copy（复制图像）、numpy.reshape（各种转换）</p><p>​    </p><p>函数解释：</p><p>​        (1) numpy.array(object, dtype=None, *, copy=True, order=’K’, subok=False, ndmin=0, like=None)</p><p>​        object 数组</p><p>​        dtype 数据类型 </p><p>​        (2) numpy.zeros(shape, dtype=float, order=‘C’, *, like=None) </p><p>​        数组维度</p><p>​        dtype 数据类型</p><p>​        (3) numpy.asarray(a, dtype=None, order=None, *, like=None) </p><p>​        数组对象</p><p>​        dtype 数据类型</p><p>​        (4) numpy.reshape(a, newshape, order=’C’)</p><p>​        数组维度</p><p>​        dtype 数据类型</p><p><strong>③ 概念</strong></p><p>​    opencv-python中一切图像数据皆numpy array</p><p>​    创建图像就是创建numpy array</p><p><strong>④ 创建图像</strong></p><p>​    1）导入import numpy as np</p><p>​    2）创建np.array([[1, 2],[3, 4]], dtype=np.uint8)</p><p>​    </p><p>​    3）创建图像最常用函数：</p><p>​        np.zeros -&gt;创建一个黑色背景图像</p><p>​        np.zeros_like-&gt;创建一个与输入图像大小一致的黑色背景图像</p><p>​        np.ones创建一个全部像素值是1的图像</p><p><strong>⑤ 图像赋值</strong></p><p>​    图像赋值就是给numpy array数组赋值</p><p>​    m = np.zeros((3, 3, 3), dtype=uint8)</p><p>​    m[:] = 255，创建数组m，然后赋值为255(白色)</p><p>​    m[:] = (255,0,0)，创建数组m，然后赋值为(255,0,0)蓝色</p><p>​    h,w,c = img.shape，h,w,c分别为高，宽，通道</p><h2 id="07-图像像素的读写操作"><a href="#07-图像像素的读写操作" class="headerlink" title="07 图像像素的读写操作"></a>07 图像像素的读写操作</h2><p> <strong>理解像素：</strong></p><p>​    像素实际大小：dpi x inches = 像素总数</p><p>​    术语dpi：每英寸的点数目，96dpi – 针对打印</p><p>​    术语ppi: 每英寸的像素数目 – 针对图像分辨率</p><p><strong>OpenCV中像素</strong></p><p>​    矩阵表示每个像素信息</p><p>​    像素遍历本质就是numpy数组访问</p><p>​    <strong>假设变量image</strong></p><p>​        获取图像维度信息: image.shape</p><p>​        图像访问像素: image[row, col]</p><p>​        图像赋值像素: image[row, col] = (b,g,r)</p><p>​        读写像素，彩色图像：</p><p>​            b, g, r = image[row, col]</p><p>​            image[row, col] = (255-b, 255-g, 255-r)</p><p>​        读写像素，灰度图像：</p><p>​            pv = image[row, col]</p><p>​            image[row, col] = 255-pv</p><h2 id="08-图像算数操作"><a href="#08-图像算数操作" class="headerlink" title="08 图像算数操作"></a>08 图像算数操作</h2><p><strong>图像读取后是一个数组，它可以进行基本的算术操作</strong></p><p><strong>加</strong> cv.add(src1, src2[, dst[, mask[, dtype]]]) -&gt;dst</p><p><strong>减</strong> cv.subtract(src1,src2[,dst[,mask[,dtype]]])-&gt;dst</p><p><strong>mask</strong>参数控制操作范围，操作范围内正常加减，范围外全为0</p><p><strong>乘</strong> cv.multiply(src1,src2[,dst[,scale[,dtype]]])-&gt;dst</p><p><strong>除</strong> cv.divide(src1, src2[, dst[, scale[, dtype]]])-&gt;dst</p><p><strong>参数说明</strong> src1 &amp; src2表示图像</p><p><strong>加法运算保证不越界的方法</strong>：saturate(src1 + src2)-》0~255。saturate_cast函数的作用即是：当运算完之后，结果为负，则转为0，结果超出255，则为255。</p><p><strong>图像算术运算要求</strong>：图像大小通道数目一致</p><p><strong>加权加法</strong>：added_wt_img = cv2.addWeighted(img1, 0.6, img2, 0.4, 0)</p><p><strong>mask</strong>表示模板（蒙版），为矩阵形式，矩阵中0，表示不取该位置的值，1表示保留该位置的值。</p>]]></content>
      
      
      
        <tags>
            
            <tag> CV </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HappyBirthday!</title>
      <link href="/posts/9209df64.html"/>
      <url>/posts/9209df64.html</url>
      
        <content type="html"><![CDATA[<p><img src="https://s3.bmp.ovh/imgs/2022/12/12/5ad4160f9fe5d284.jpg" alt=""></p>]]></content>
      
      
      
        <tags>
            
            <tag> 杂文 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>OpenCV——1</title>
      <link href="/posts/5c81b7f3.html"/>
      <url>/posts/5c81b7f3.html</url>
      
        <content type="html"><![CDATA[<h1 id="一、认识计算机视觉"><a href="#一、认识计算机视觉" class="headerlink" title="一、认识计算机视觉"></a>一、认识计算机视觉</h1><h2 id="1-发展历史"><a href="#1-发展历史" class="headerlink" title="1.发展历史"></a>1.发展历史</h2><p>•最早追溯到老子小孔成像</p><p>•现代1966年MIT的马文·明斯基的学生实现PC链接摄像机，标志计算机视觉作为一门学科开始发展</p><p>•1982.马尔文发布《视觉》标志着CV正式成为一门学科</p><p>•1999.David Lowe 发表SIFT特征相关论文，OpenCV收录使用</p><p>•2001.V&amp;J发表基于HAAR特征的实时人脸检测算法</p><p>•2005.HOG提出特征提取的行人检测算法</p><p>•2006.Pascal VOC数据集发布</p><p>•2012.AlexNet模型赢得ImageNet图像分类比赛冠军，展现出深度学习在CV领域的应用前景</p><p>•未来世界离不开CV</p><h2 id="2-主要任务"><a href="#2-主要任务" class="headerlink" title="2.主要任务"></a>2.主要任务</h2><p>早期主要研究领域为<u>重建</u></p><p>2012后，受深度学习影响<u>重建</u>与<u>感知</u>快速发展</p><p>目标：通过图灵测试</p><h2 id="3-应用场景"><a href="#3-应用场景" class="headerlink" title="3.应用场景"></a>3.应用场景</h2><p>•自动驾驶/辅助驾驶</p><p>•计算机视觉-AI + 机构/工业质检检测</p><p>…</p><p>•形成全场景的行业应用</p><h1 id="二、计算机视觉框架"><a href="#二、计算机视觉框架" class="headerlink" title="二、计算机视觉框架"></a>二、计算机视觉框架</h1><h2 id="1-计算机视觉框架"><a href="#1-计算机视觉框架" class="headerlink" title="1.计算机视觉框架"></a>1.计算机视觉框架</h2><p>•Matlab . 追溯到1970年 . 支持图像处理</p><p>•Matrox mil . 1993年发布第一个版本</p><p>•Halcon . 追溯到1996 . CV领域应用最多，主流框架</p><p>•OpenCV . 1999启动，2006发布1.0版本 . 开源</p><p>•VisionPro . 2009年发布</p><p>传统计算机视觉框架</p><p>•SimpleCV</p><p>•BoofCV</p><p>•Dlib</p><p>•JavaCV</p><p>深度学习计算机视觉（训练）框架</p><p>•Caffe</p><p>•Tensorflow</p><p>•Pytorch</p><p>•Paddlepaddle</p><p>•Keras</p><p>深度学习计算机视觉（部署）框架</p><p>•OpenVINO</p><p>•TensorRT</p><p>•onnxruntime</p><p>•Deepface</p><p>•YOLO/DarkNet</p><p>•mmdetection</p><p>•Paddle-detection/seg/ocr</p><h2 id="2-当前主流框架"><a href="#2-当前主流框架" class="headerlink" title="2.当前主流框架"></a>2.当前主流框架</h2><p>•机器视觉方向-Halcon/VisionPro/Mil/OpenCV</p><p>•深度学习方向-tensorflow/pytorch/paddlepaddle + openvino/tensorRT/onnxruntime</p><p>•主流语言Python/C++</p><h2 id="3-计算机视觉框架的未来趋势"><a href="#3-计算机视觉框架的未来趋势" class="headerlink" title="3.计算机视觉框架的未来趋势"></a>3.计算机视觉框架的未来趋势</h2><p>•低代码平台流行趋势明显</p><p>•传统视觉跟深度学习整合趋势明显</p><p>•算法设计流程化/可视化</p><p>•算法模块易用性跟通用性</p><p>•计算资源异构化支持趋势</p><p>•深度学习模型训练简捷化</p><h1 id="三、OpenCV"><a href="#三、OpenCV" class="headerlink" title="三、OpenCV"></a>三、OpenCV</h1><p>•github: <a href="https://github.com/opencv">https://github.com/opencv</a></p><p>•Tutorial: <a href="https://docs.opencv.org/4.5.5/index.html">https://docs.opencv.org/4.5.5/index.html</a></p><h2 id="1-发展历史-1"><a href="#1-发展历史-1" class="headerlink" title="1.发展历史"></a>1.发展历史</h2><p>•OpenCV在1999年的开始开发….</p><p>•2006年 OpenCV1.0正式发布（C）</p><p>•2009年 OpenCV2.0正式发布（C++）</p><p>•2012年 社区托管模式（开源）</p><p>•2015年 OpenCV3.0正式发布（完善接口）</p><p>•2018年 OpenCV4.0正式发布</p><p>•2022年4月份，4.5.5版本</p><h2 id="2-OpenCV模块架构"><a href="#2-OpenCV模块架构" class="headerlink" title="2.OpenCV模块架构"></a>2.OpenCV模块架构</h2><p><img src="/Users/yang/Library/Application Support/typora-user-images/image-20221212200329994.png" alt="image-20221212200329994"></p><h2 id="3-OpenCV安装与支持"><a href="#3-OpenCV安装与支持" class="headerlink" title="3.OpenCV安装与支持"></a>3.OpenCV安装与支持</h2><p>•Python SDK安装，推荐3.6.5</p><p>•OpenCV-Python安装 <code>pip install opencv-python==4.5.4.60</code></p><p>​    (支持镜像安装<code>-i https://pypi.tuna.tsinghua.edu.cn/simple</code>)</p><p>•检查 <code>pip list</code></p><p><code>python</code></p><p><code>Import cv2 as cv</code></p><p><code>cv.__version__</code></p><h2 id="4-Intel-Devcloud-codelab使用"><a href="#4-Intel-Devcloud-codelab使用" class="headerlink" title="4.Intel Devcloud codelab使用"></a>4.Intel Devcloud codelab使用</h2><p>网址：devcloud.intel.com/edge（注册登录）</p><p>学习——教程——OpenCV Tutorial</p><h1 id="四、图像读取与显示"><a href="#四、图像读取与显示" class="headerlink" title="四、图像读取与显示"></a>四、图像读取与显示</h1><p>计算机通过数值识别灰度或彩色图像</p><h2 id="图像读取与显示"><a href="#图像读取与显示" class="headerlink" title="图像读取与显示"></a><strong>图像读取与显示</strong></h2><p>•<code>import cv2 as cv</code> – 导入OpenCV支持</p><p>•<code>import numpy as np</code> – 导入Numpy支持</p><p>•imread函数，读取图像</p><p>•imshow函数, 显示图像</p><p>•加载图像的通道顺序</p><p>•<code>cv.imread(filename[,flags])</code></p><p>​    -filename 表示文件路径</p><p>​    -[]内的参数表示可选，可以不填</p><p>•<code>cv.imshow( winname, mat)</code> #BGR</p><p>​    -winname表示窗口标题</p><p>​    -mat 表示图像对象</p><p>•<code>cv.waitKey(0)</code>  #表示一直等待，直到任意一个键盘操作</p><p>•<code>cv.waitKey(1000)</code>  #表示等待1000毫秒即1秒</p><p>•<code>cv.destroyAllWindows()</code>  #关闭窗口并取消分配任何相关的内存使用。对于一个简单的程序，实际上不必调用这些函数，因为退出时操作系统会自动关闭应用程序的所有资源和窗口</p>]]></content>
      
      
      
        <tags>
            
            <tag> CV </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Ubuntu22.04前期优化</title>
      <link href="/posts/dac10dd2.html"/>
      <url>/posts/dac10dd2.html</url>
      
        <content type="html"><![CDATA[<h1 id="Ubuntu前期优化"><a href="#Ubuntu前期优化" class="headerlink" title="Ubuntu前期优化"></a>Ubuntu前期优化</h1><h2 id="一、系统设置"><a href="#一、系统设置" class="headerlink" title="一、系统设置"></a>一、系统设置</h2><h4 id="1-换源"><a href="#1-换源" class="headerlink" title="1.换源"></a>1.换源</h4><p>“软件和更新”——“Ubuntu软件”——“下载自”——改为“位于 中国的服务器”</p><p><img src="https://s3.bmp.ovh/imgs/2022/12/23/ec0c52678b306689.png" alt=""></p><h4 id="2-显卡驱动"><a href="#2-显卡驱动" class="headerlink" title="2.显卡驱动"></a>2.显卡驱动</h4><p>“软件和更新”——附加驱动——选择合适驱动（本人选择NVIDIA driver metapackage来自nvidia-dirver-525(专有)）——应用更改——重启查看设置“关于”是否出现显卡信息</p><p><img src="https://s3.bmp.ovh/imgs/2022/12/23/74d55ad4466968da.png" alt=""></p><h4 id="3-界面优化"><a href="#3-界面优化" class="headerlink" title="3.界面优化"></a>3.界面优化</h4><p>“设置”——“外观”——“桌面图标“：关闭“显示个人文件夹</p><p>“设置”——“外观”——“Dock”:打开自动隐藏Dock,关闭面板模式，屏幕上的位置（底部）</p><p>“设置”——“鼠标和触摸板”——“触摸板”：打开自然滚动，调整触摸板速度，打开双指滚动</p><p><img src="https://s3.bmp.ovh/imgs/2022/12/23/e0e1f98b2962a84d.png" alt=""></p><h4 id="4-关闭更新"><a href="#4-关闭更新" class="headerlink" title="4.关闭更新"></a>4.关闭更新</h4><p>“软件和更新”——“更新”：根据自己的想法调整</p><p><img src="https://s3.bmp.ovh/imgs/2022/12/23/b92ce79306df42e0.png" alt=""></p><h4 id="5-关闭Dock栏显示其他分区磁盘（若出现此情况）"><a href="#5-关闭Dock栏显示其他分区磁盘（若出现此情况）" class="headerlink" title="5.关闭Dock栏显示其他分区磁盘（若出现此情况）"></a>5.关闭Dock栏显示其他分区磁盘（若出现此情况）</h4><p>“磁盘”——选择对应磁盘及分区——“其他分区选项”——“编辑挂载选项”——关闭“用户会话默认值”——取消勾选“系统启动时挂载”、“显示用户界面”——确定即可</p><p><img src="https://s3.bmp.ovh/imgs/2022/12/23/b0574b884c8ea719.png" alt=""></p><h4 id="6-设置区域与语言"><a href="#6-设置区域与语言" class="headerlink" title="6.设置区域与语言"></a>6.设置区域与语言</h4><p>“设置”——“区域与语言”——“管理已安装的语言”——会提示语言支持没有安装完整，点击安装，完成后重启即可。</p><h4 id="7-终端设置"><a href="#7-终端设置" class="headerlink" title="7.终端设置"></a>7.终端设置</h4><p>打开终端——“设置——”“配置文件首选项”——点击配置文件首选项，”内置方案“选择“Linux控制台“，“以亮色显示粗体字”，即时生效。</p><p><img src="https://s3.bmp.ovh/imgs/2022/12/23/f329d103efa8610f.png" alt=""></p><p><img src="https://s3.bmp.ovh/imgs/2022/12/23/3cfe28f1eb0ab101.png" alt=""></p><h4 id="8-修改用户目录为中文"><a href="#8-修改用户目录为中文" class="headerlink" title="8.修改用户目录为中文"></a>8.修改用户目录为中文</h4><p>打开终端输入以下命令：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export LANG=en_US</span><br><span class="line">xdg-user-dirs-gtk-update</span><br></pre></td></tr></table></figure><p>弹出对话框 ，<strong><em>不要勾选</em></strong>“下次别问我”之类的选项，选择更新名称。</p><p>终端输入：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export LANG=zh_CN</span><br></pre></td></tr></table></figure><p>关闭终端，重启系统。</p><p>进入系统，系统会提示是否把目录改回中文，勾选“不要再次询问我”，选择<strong><em>保留旧的名称</em></strong></p><h4 id="9-命令优化"><a href="#9-命令优化" class="headerlink" title="9.命令优化"></a>9.命令优化</h4><h5 id="1、添加open命令"><a href="#1、添加open命令" class="headerlink" title="1、添加open命令"></a>1、添加open命令</h5><h5 id="（1）打开当前目录"><a href="#（1）打开当前目录" class="headerlink" title="（1）打开当前目录"></a>（1）打开当前目录</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#打开var目录</span></span><br><span class="line">nautilus /var</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1. 编辑</span></span><br><span class="line">vim /etc/profile</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.取别名</span></span><br><span class="line">alias <span class="built_in">open</span>=<span class="string">&quot;nautilus $1&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.</span></span><br><span class="line">source /etc/profile</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4.打开文件夹</span></span><br><span class="line"><span class="built_in">open</span> .</span><br><span class="line"><span class="number">123456789101</span></span><br></pre></td></tr></table></figure><h5 id="（2）添加命令"><a href="#（2）添加命令" class="headerlink" title="（2）添加命令"></a>（2）添加命令</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">vi ~/.bashrc  # 或者 gedit ~/.bashrc 个人习惯了vi 命令</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">添加如下内容</span></span><br><span class="line">alias open=&quot;nautilus .&quot;</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">使资源生效</span></span><br><span class="line">source ~/.bashrc</span><br><span class="line">12345</span><br></pre></td></tr></table></figure><h5 id="（3）提示：vim操作"><a href="#（3）提示：vim操作" class="headerlink" title="（3）提示：vim操作"></a>（3）提示：vim操作</h5><p>“i”：编辑插入</p><p>“Esc”键：退出编辑</p><p>输入 “:wq”：保存退出</p><h4 id="10-科学上网"><a href="#10-科学上网" class="headerlink" title="10.科学上网"></a>10.科学上网</h4><p>链接: <a href="https://pan.baidu.com/s/1GwV1a5MOlaYiJKt0UhrsoA?pwd=1688">https://pan.baidu.com/s/1GwV1a5MOlaYiJKt0UhrsoA?pwd=1688</a> 提取码: 1688</p><p>将文件解压至/snap，终端或双击打开”Clash for Windows-0.20.9-x64-linux”里的“cfw”</p><p>“General”——“Service Mode”——“Manage”——“Install”</p><p>“General”——打开“TUN Mode”</p><p>“General”——打开“start with Linux”</p><p>“Profiles”——添加自己的配置</p><h4 id="11-添加-flatpak"><a href="#11-添加-flatpak" class="headerlink" title="11.添加 flatpak"></a>11.添加 flatpak</h4><p><a href="https://flatpak.org/setup/">https://flatpak.org/setup/</a></p><p>点击Ubuntu图标按照教程安装即可。</p><p>之后重启系统即可打开商店安装应用（打开可能需要科学上网）。</p><h4 id="12-GNOME-Tweaks-和扩展"><a href="#12-GNOME-Tweaks-和扩展" class="headerlink" title="12.GNOME Tweaks 和扩展"></a>12.GNOME Tweaks 和扩展</h4><p>打开系统的商店，左上角搜索“GNOME”，安装“GNOME Tweaks”,之后在程序坞中找到“优化”打开，即可进行更多系统设置。</p><p>同时可安装“扩展管理器”，寻找更多扩展插件。</p><h4 id="13-安装snap代理及client"><a href="#13-安装snap代理及client" class="headerlink" title="13.安装snap代理及client"></a>13.安装snap代理及client</h4><p>打开软件商店,搜索安装“snap-store-proxy”、“Snap Store Proxy Client ”</p><h2 id="二、软件安装"><a href="#二、软件安装" class="headerlink" title="二、软件安装"></a>二、软件安装</h2><h4 id="1-chrome"><a href="#1-chrome" class="headerlink" title="1.chrome"></a>1.chrome</h4><p><a href="https://www.google.cn/chrome/index.html">https://www.google.cn/chrome/index.html</a></p><h4 id="2-QQ"><a href="#2-QQ" class="headerlink" title="2.QQ"></a>2.QQ</h4><p><a href="https://im.qq.com/linuxqq/index.html">https://im.qq.com/linuxqq/index.html</a></p><h4 id="3-搜狗输入法"><a href="#3-搜狗输入法" class="headerlink" title="3.搜狗输入法"></a>3.搜狗输入法</h4><p><a href="https://shurufa.sogou.com/linux">https://shurufa.sogou.com/linux</a></p><p><a href="https://shurufa.sogou.com/linux/guide">https://shurufa.sogou.com/linux/guide</a></p><p>若输入法无法正常使用，换回之前使用的输入法</p><p>第一步，彻底卸载fcitx</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get remove fcitx*</span><br><span class="line">sudo apt-get purge fcitx*</span><br><span class="line">12</span><br></pre></td></tr></table></figure><p>第二步，将输入法系统设置为ibus</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">打开 设置 —&gt; 语言支持</span><br><span class="line">将键盘输入法系统选项设置为ibus</span><br><span class="line">12</span><br></pre></td></tr></table></figure><p>第三步 注销重新登录</p><h4 id="4-Typora"><a href="#4-Typora" class="headerlink" title="4.Typora"></a>4.Typora</h4><p>链接: <a href="https://pan.baidu.com/s/1atxTuNOmyeCL4cFiMd1BLg?pwd=jxsn">https://pan.baidu.com/s/1atxTuNOmyeCL4cFiMd1BLg?pwd=jxsn</a> 提取码: jxsn</p><h5 id="1-安装"><a href="#1-安装" class="headerlink" title="(1)安装"></a>(1)安装</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#在所下载包的文件夹打开终端</span></span><br><span class="line">tar xzvf Typora-linux-x64.tar.gz </span><br><span class="line"><span class="built_in">cd</span> bin</span><br><span class="line">sudo <span class="built_in">cp</span> -ar Typora-linux-x64 /opt</span><br><span class="line"><span class="built_in">cd</span> /opt/Typora-linux-x64/</span><br><span class="line"><span class="comment">#启动</span></span><br><span class="line">./Typora</span><br></pre></td></tr></table></figure><h5 id="（2）配置"><a href="#（2）配置" class="headerlink" title="（2）配置"></a>（2）配置</h5><p>设置环境变量</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo vim ~/.bashrc</span><br></pre></td></tr></table></figure><p>打开.bashrc配置文件，添加：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#Typora环境变量</span><br><span class="line"><span class="keyword">export</span> PATH=$PATH:/opt/Typora-linux-x64</span><br><span class="line"><span class="number">12</span></span><br></pre></td></tr></table></figure><p>source以下，让配置生效</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">source</span> ~/.bashrc</span><br></pre></td></tr></table></figure><p>之后可终端输入“Typora”直接打开。</p><h5 id="（3）添加桌面面标"><a href="#（3）添加桌面面标" class="headerlink" title="（3）添加桌面面标"></a>（3）添加桌面面标</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /usr/share/applications</span><br><span class="line">sudo vim typora.desktop</span><br></pre></td></tr></table></figure><p>添加以下内容，后重启系统：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[Desktop Entry]</span><br><span class="line">Name=Typora</span><br><span class="line">Comment=Typora</span><br><span class="line">Exec=/opt/Typora-linux-x64/Typora</span><br><span class="line">Icon=/opt/Typora-linux-x64/resources/app/asserts/icon/icon_256x256.png</span><br><span class="line">Terminal=false</span><br><span class="line">Type=Application</span><br><span class="line">Categories=Developer;</span><br></pre></td></tr></table></figure><p>打开终端输入下面内容：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gedit ~/.config/mimeapps.list</span><br></pre></td></tr></table></figure><p>添加  <code>text/markdown=typora.desktop;</code></p><p><img src="https://s3.bmp.ovh/imgs/2022/12/23/1e796a3cac96307d.png" alt=""></p><h4 id="5-OBS"><a href="#5-OBS" class="headerlink" title="5.OBS"></a>5.OBS</h4><p>应用商店下载即可</p><h4 id="6-java和我的世界"><a href="#6-java和我的世界" class="headerlink" title="6.java和我的世界"></a>6.java和我的世界</h4><h4 id="更新中…"><a href="#更新中…" class="headerlink" title="更新中…"></a>更新中…</h4>]]></content>
      
      
      
        <tags>
            
            <tag> Linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Ubuntu22.04安装</title>
      <link href="/posts/549a771b.html"/>
      <url>/posts/549a771b.html</url>
      
        <content type="html"><![CDATA[<h1 id="Ubuntu系统安装"><a href="#Ubuntu系统安装" class="headerlink" title="Ubuntu系统安装"></a>Ubuntu系统安装</h1><p><strong>准备内存至少为4G的U盘，最好8G及以上</strong></p><h2 id="1-预下载"><a href="#1-预下载" class="headerlink" title="1.预下载"></a>1.预下载</h2><p><strong>下载Ubuntu镜像：<a href="https://cn.ubuntu.com/download/desktop">下载Ubuntu桌面系统 | Ubuntu</a></strong></p><p><strong>下载U盘启动盘制作工具：</strong></p><p>​    <strong>官网：<a href="https://rufus.ie/zh/">Rufus - 轻松创建USB启动盘</a></strong></p><p>​    <strong>or网盘链接: <a href="https://pan.baidu.com/s/1BnXpb-07EtqTBXt28gpFHQ?pwd=1234">https://pan.baidu.com/s/1BnXpb-07EtqTBXt28gpFHQ?pwd=1234</a> 提取码: 1234</strong></p><h2 id="2-安装U盘制作"><a href="#2-安装U盘制作" class="headerlink" title="2.安装U盘制作"></a>2.安装U盘制作</h2><h4 id="1）将要制作的-U-盘插入电脑，打开Rufus"><a href="#1）将要制作的-U-盘插入电脑，打开Rufus" class="headerlink" title="1）将要制作的 U 盘插入电脑，打开Rufus"></a>1）将要制作的 U 盘插入电脑，打开Rufus</h4><h4 id="2）在分区方案和目标系统类型选项中选择用于UEFI计算机的GPT分区方案，文件系统-选择-NTFS"><a href="#2）在分区方案和目标系统类型选项中选择用于UEFI计算机的GPT分区方案，文件系统-选择-NTFS" class="headerlink" title="2）在分区方案和目标系统类型选项中选择用于UEFI计算机的GPT分区方案，文件系统 选择 NTFS"></a>2）在<strong>分区方案和目标系统类型</strong>选项中选择<strong>用于UEFI计算机的GPT分区方案</strong>，<strong>文件系统</strong> 选择 <strong>NTFS</strong></h4><h4 id="3）点击光盘图标选择好下载的光盘镜像文件"><a href="#3）点击光盘图标选择好下载的光盘镜像文件" class="headerlink" title="3）点击光盘图标选择好下载的光盘镜像文件"></a>3）点击光盘图标选择好下载的光盘镜像文件</h4><h4 id="4）点击“开始”进行制作，显示“准备就绪”后，关闭Rufus"><a href="#4）点击“开始”进行制作，显示“准备就绪”后，关闭Rufus" class="headerlink" title="4）点击“开始”进行制作，显示“准备就绪”后，关闭Rufus"></a>4）点击“开始”进行制作，显示“准备就绪”后，关闭Rufus</h4><h2 id="3-安装"><a href="#3-安装" class="headerlink" title="3.安装"></a>3.安装</h2><h4 id="1）重启，进BIOS（不同品牌电脑按键不同，可根据电脑型号去网上搜），选择U盘启动，保存重启"><a href="#1）重启，进BIOS（不同品牌电脑按键不同，可根据电脑型号去网上搜），选择U盘启动，保存重启" class="headerlink" title="1）重启，进BIOS（不同品牌电脑按键不同，可根据电脑型号去网上搜），选择U盘启动，保存重启"></a>1）重启，进BIOS（不同品牌电脑按键不同，可根据电脑型号去网上搜），选择U盘启动，保存重启</h4><h4 id="2）在欢迎页面左右选择「中文（简体）」，再点击右侧的「安装-Ubuntu」按钮。"><a href="#2）在欢迎页面左右选择「中文（简体）」，再点击右侧的「安装-Ubuntu」按钮。" class="headerlink" title="2）在欢迎页面左右选择「中文（简体）」，再点击右侧的「安装 Ubuntu」按钮。"></a>2）在欢迎页面左右选择「<strong>中文（简体）</strong>」，再点击右侧的「<strong>安装 Ubuntu</strong>」按钮。</h4><h4 id="3）选择chinese，最小安装，取消安装时下载更新，取消安装第三方软件（根据自身需要设置）"><a href="#3）选择chinese，最小安装，取消安装时下载更新，取消安装第三方软件（根据自身需要设置）" class="headerlink" title="3）选择chinese，最小安装，取消安装时下载更新，取消安装第三方软件（根据自身需要设置）"></a>3）选择chinese，最小安装，取消安装时下载更新，取消安装第三方软件（根据自身需要设置）</h4><h4 id="4）分区"><a href="#4）分区" class="headerlink" title="4）分区"></a>4）分区</h4><p>若只想保留Ubuntu而删除Windows，选择“清除整个磁盘并安装”即可</p><p>   由于本人磁盘安装有多个系统，因此选择“其他选项”自己分配空间</p><p>   本人方案：分配100G空间</p><div class="table-container"><table><thead><tr><th>名称</th><th>EFI分区</th><th>swap交换分区</th><th></th></tr></thead><tbody><tr><td>分配空间大小</td><td>500m</td><td>16G</td><td>剩余空间（约80G）</td></tr><tr><td>类型</td><td>逻辑分区</td><td>主分区</td><td>逻辑分区</td></tr><tr><td>位置</td><td>空间起始位置 固态硬盘</td><td>空间起始位置 固态硬盘</td><td>空间起始位置 固态硬盘</td></tr><tr><td>用于</td><td>EFI系统分区</td><td>交换空间</td><td>Ext4日志文件系统</td></tr></tbody></table></div><h4 id="5）之后经过一些设置，安装完成后重启电脑，重启时即可拔掉U盘"><a href="#5）之后经过一些设置，安装完成后重启电脑，重启时即可拔掉U盘" class="headerlink" title="5）之后经过一些设置，安装完成后重启电脑，重启时即可拔掉U盘"></a>5）之后经过一些设置，安装完成后重启电脑，重启时即可拔掉U盘</h4><h4 id="6-1）（多系统可能出现的问题）Ubuntu引导顶掉原先引导"><a href="#6-1）（多系统可能出现的问题）Ubuntu引导顶掉原先引导" class="headerlink" title="6.1）（多系统可能出现的问题）Ubuntu引导顶掉原先引导"></a>6.1）（多系统可能出现的问题）Ubuntu引导顶掉原先引导</h4><p>​    方法:重新进入BIOS将启动首选项改回</p><h5 id="6-2）-自用，请勿模仿，仅供自己参考：（本人使用OC引导）"><a href="#6-2）-自用，请勿模仿，仅供自己参考：（本人使用OC引导）" class="headerlink" title="6.2） 自用，请勿模仿，仅供自己参考：（本人使用OC引导）"></a>6.2） 自用，请勿模仿，仅供自己参考：（本人使用OC引导）</h5><p>①找到OC引导所在的位置，将\EFI\OC\config.plist备份</p><p>②将磁盘OC引导所在的EFI分区中的ubuntu文件夹移动到安装时分配的EFI分区（建立名为EFI的文件夹，将文件放入其中，原位置的ubuntu文件夹删除）</p><p>③重新替换OC分区的EFI文件夹，后将上面备份的config.plist导入新EFI文件夹（替换即可）</p><p>④重启进BIOS将启动首选项改为OC引导，保存退出重启即可进入系统选择界面</p><h3 id="至此Ubuntu系统安装步骤全部完成"><a href="#至此Ubuntu系统安装步骤全部完成" class="headerlink" title="至此Ubuntu系统安装步骤全部完成"></a>至此Ubuntu系统安装步骤全部完成</h3>]]></content>
      
      
      
        <tags>
            
            <tag> Linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ThoroughPyTorch——5</title>
      <link href="/posts/4419691d.html"/>
      <url>/posts/4419691d.html</url>
      
        <content type="html"><![CDATA[<h1 id="PyTorch生态与部署"><a href="#PyTorch生态与部署" class="headerlink" title=" PyTorch生态与部署 "></a><center> PyTorch生态与部署 </center></h1><h1 id="1-PyTorch生态简介"><a href="#1-PyTorch生态简介" class="headerlink" title="1.PyTorch生态简介"></a>1.PyTorch生态简介</h1><p><a href="https://datawhalechina.github.io/thorough-pytorch/第八章/index.html">https://datawhalechina.github.io/thorough-pytorch/第八章/index.html</a></p><p>PyTorch的强大很大程度上取决于它的生态。</p><h2 id="（1）torchvision"><a href="#（1）torchvision" class="headerlink" title="（1）torchvision"></a>（1）torchvision</h2><div class="table-container"><table><thead><tr><th><strong>torchvision.datasets *</strong></th><th><strong>包含了一些我们在计算机视觉中常见的数据集</strong></th></tr></thead><tbody><tr><td>torchvision.models *</td><td>提供一些预训练模型</td></tr><tr><td><strong>torchvision.tramsforms *</strong></td><td><strong>用于数据增强和处理</strong></td></tr><tr><td>torchvision.io</td><td>视频、图片和文件的 IO 操作（读取、写入、编解码）</td></tr><tr><td><strong>torchvision.ops</strong></td><td><a href="https://pytorch.org/vision/stable/ops.html"><strong>提供了许多计算机视觉的特定操作</strong></a></td></tr><tr><td>torchvision.utils</td><td><a href="https://pytorch.org/vision/stable/utils.html">提供了一些可视化的方法</a></td></tr></tbody></table></div><h2 id="（2）PyTorchVideo"><a href="#（2）PyTorchVideo" class="headerlink" title="（2）PyTorchVideo"></a>（2）PyTorchVideo</h2><p>PytorchVideo 提供了加速视频理解研究所需的模块化和高效的API。它还支持不同的深度学习视频组件，如视频模型、视频数据集和视频特定转换，最重要的是，PytorchVideo也提供了model zoo，使得人们可以使用各种先进的预训练视频模型及其评判基准。</p><p>基于 PyTorch，高质量model zoo，支持主流数据集及预处理，模块化设计，支持多模态，移动端部署优化</p><h2 id="（3）torchtext"><a href="#（3）torchtext" class="headerlink" title="（3）torchtext"></a>（3）torchtext</h2><ul><li>数据处理工具 torchtext.data.functional、torchtext.data.utils</li><li>数据集 torchtext.data.datasets</li><li>词表工具 torchtext.vocab</li><li>评测指标 torchtext.metrics</li></ul><h4 id="构建数据集"><a href="#构建数据集" class="headerlink" title="构建数据集"></a>构建数据集</h4><ul><li><strong>Field及其使用</strong></li></ul><p>①构建Field</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tokenize = lambda x: x.split()</span><br><span class="line">TEXT = data.Field(sequential=True, tokenize=tokenize, lower=True, fix_length=200)</span><br><span class="line">LABEL = data.Field(sequential=False, use_vocab=False)</span><br></pre></td></tr></table></figure><p>②进一步构建dataset</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">from torchtext import data</span><br><span class="line">def get_dataset(csv_data, text_field, label_field, test=False):</span><br><span class="line">    fields = [(&quot;id&quot;, None), # we won&#x27;t be needing the id, so we pass in None as the field</span><br><span class="line">                 (&quot;comment_text&quot;, text_field), (&quot;toxic&quot;, label_field)]       </span><br><span class="line">    examples = []</span><br><span class="line"></span><br><span class="line">    if test:</span><br><span class="line">        # 如果为测试集，则不加载label</span><br><span class="line">        for text in tqdm(csv_data[&#x27;comment_text&#x27;]):</span><br><span class="line">            examples.append(data.Example.fromlist([None, text, None], fields))</span><br><span class="line">    else:</span><br><span class="line">        for text, label in tqdm(zip(csv_data[&#x27;comment_text&#x27;], csv_data[&#x27;toxic&#x27;])):</span><br><span class="line">            examples.append(data.Example.fromlist([None, text, label], fields))</span><br><span class="line">    return examples, fields</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">train_data = pd.read_csv(&#x27;train_toxic_comments.csv&#x27;)</span><br><span class="line">valid_data = pd.read_csv(&#x27;valid_toxic_comments.csv&#x27;)</span><br><span class="line">test_data = pd.read_csv(&quot;test_toxic_comments.csv&quot;)</span><br><span class="line">TEXT = data.Field(sequential=True, tokenize=tokenize, lower=True)</span><br><span class="line">LABEL = data.Field(sequential=False, use_vocab=False)</span><br><span class="line"></span><br><span class="line"># 得到构建Dataset所需的examples和fields</span><br><span class="line">train_examples, train_fields = get_dataset(train_data, TEXT, LABEL)</span><br><span class="line">valid_examples, valid_fields = get_dataset(valid_data, TEXT, LABEL)</span><br><span class="line">test_examples, test_fields = get_dataset(test_data, TEXT, None, test=True)</span><br><span class="line"># 构建Dataset数据集</span><br><span class="line">train = data.Dataset(train_examples, train_fields)</span><br><span class="line">valid = data.Dataset(valid_examples, valid_fields)</span><br><span class="line">test = data.Dataset(test_examples, test_fields)</span><br><span class="line"></span><br><span class="line"># 检查keys是否正确</span><br><span class="line">print(train[0].__dict__.keys())</span><br><span class="line">print(test[0].__dict__.keys())</span><br><span class="line"># 抽查内容是否正确</span><br><span class="line">print(train[0].comment_text)</span><br></pre></td></tr></table></figure><ul><li><strong>词汇表（vocab）</strong></li></ul><p>构建词语到向量（或数字）的映射关系</p><p>在torchtext中可以使用Field自带的build_vocab函数完成词汇表构建。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">TEXT.build_vocab(train)</span><br></pre></td></tr></table></figure><ul><li><strong>数据迭代器</strong></li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">from torchtext.data import Iterator, BucketIterator</span><br><span class="line"># 若只针对训练集构造迭代器</span><br><span class="line"># train_iter = data.BucketIterator(dataset=train, batch_size=8, shuffle=True, sort_within_batch=False, repeat=False)</span><br><span class="line"></span><br><span class="line"># 同时对训练集和验证集进行迭代器的构建</span><br><span class="line">train_iter, val_iter = BucketIterator.splits(</span><br><span class="line">        (train, valid), # 构建数据集所需的数据集</span><br><span class="line">        batch_sizes=(8, 8),</span><br><span class="line">        device=-1, # 如果使用gpu，此处将-1更换为GPU的编号</span><br><span class="line">        sort_key=lambda x: len(x.comment_text), # the BucketIterator needs to be told what function it should use to group the data.</span><br><span class="line">        sort_within_batch=False</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">test_iter = Iterator(test, batch_size=8, device=-1, sort=False, sort_within_batch=False)</span><br></pre></td></tr></table></figure><h1 id="2-PyTorch模型部署"><a href="#2-PyTorch模型部署" class="headerlink" title="2.PyTorch模型部署"></a>2.PyTorch模型部署</h1><p><a href="https://datawhalechina.github.io/thorough-pytorch/第九章/index.html">https://datawhalechina.github.io/thorough-pytorch/第九章/index.html</a></p><p><img src="https://niuzhikang.oss-cn-chengdu.aliyuncs.com/figures/202208052139305.jpg" alt="微信图片_20220805213616"></p><h2 id="ONNX"><a href="#ONNX" class="headerlink" title="ONNX"></a>ONNX</h2><h4 id="（1）ONNX简介"><a href="#（1）ONNX简介" class="headerlink" title="（1）ONNX简介"></a>（1）ONNX简介</h4><p>①ONNX</p><ul><li>ONNX官网：<a href="https://onnx.ai/">https://onnx.ai/</a></li><li>ONNX GitHub：<a href="https://github.com/onnx/onnx">https://github.com/onnx/onnx</a></li></ul><p>通过定义一组与环境和平台无关的标准格式，使AI模型可以在不同框架和环境下交互使用。</p><p>使用不同框架训练的模型，转化为ONNX格式后，可以很容易的部署在兼容ONNX的运行环境中。</p><p>②ONNX Runtime</p><ul><li>ONNX Runtime官网：<a href="https://www.onnxruntime.ai/">https://www.onnxruntime.ai/</a></li><li>ONNX Runtime GitHub：<a href="https://github.com/microsoft/onnxruntime">https://github.com/microsoft/onnxruntime</a></li></ul><p>跨平台机器学习推理加速器，可直接读取 .onnx 格式的文件。</p><p>③安装</p><p>ONNX和ONNX Runtime的适配关系：<a href="https://github.com/microsoft/onnxruntime/blob/master/docs/Versioning.md">https://github.com/microsoft/onnxruntime/blob/master/docs/Versioning.md</a></p><p>使用GPU进行推理时，需要卸载onnxruntime，再安装onnxruntime-gpu，同时还需考虑ONNX Runtime与CUDA之间的适配关系，<a href="https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html">参考链接</a></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># 激活虚拟环境</span><br><span class="line">conda activate env_name # env_name换成环境名称</span><br><span class="line"># 安装onnx</span><br><span class="line">pip install onnx </span><br><span class="line"># 安装onnx runtime</span><br><span class="line">pip install onnxruntime # 使用CPU进行推理</span><br><span class="line"># pip install onnxruntime-gpu # 使用GPU进行推理</span><br></pre></td></tr></table></figure><h4 id="（2）模型导出为ONNX"><a href="#（2）模型导出为ONNX" class="headerlink" title="（2）模型导出为ONNX"></a>（2）模型导出为ONNX</h4><p>使用<code>torch.onnx.export()</code>把模型转换成 ONNX 格式的函数</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">import torch.onnx </span><br><span class="line"># 转换的onnx格式的名称，文件后缀需为.onnx</span><br><span class="line">onnx_file_name = &quot;xxxxxx.onnx&quot;</span><br><span class="line"># 我们需要转换的模型，将torch_model设置为自己的模型</span><br><span class="line">model = torch_model</span><br><span class="line"># 加载权重，将model.pth转换为自己的模型权重</span><br><span class="line"># 如果模型的权重是使用多卡训练出来，我们需要去除权重中多的module. 具体操作可以见5.4节</span><br><span class="line">model = model.load_state_dict(torch.load(&quot;model.pth&quot;))</span><br><span class="line"># 导出模型前，必须调用model.eval()或者model.train(False)</span><br><span class="line">model.eval()</span><br><span class="line"># dummy_input就是一个输入的实例，仅提供输入shape、type等信息 </span><br><span class="line">batch_size = 1 # 随机的取值，当设置dynamic_axes后影响不大</span><br><span class="line">dummy_input = torch.randn(batch_size, 1, 224, 224, requires_grad=True) </span><br><span class="line"># 这组输入对应的模型输出</span><br><span class="line">output = model(dummy_input)</span><br><span class="line"># 导出模型（需确保我们的模型处在推理模式）</span><br><span class="line">torch.onnx.export(model,        # 模型的名称</span><br><span class="line">                  dummy_input,   # 一组实例化输入</span><br><span class="line">                  onnx_file_name,   # 文件保存路径/名称</span><br><span class="line">                  export_params=True,        #  如果指定为True或默认, 参数也会被导出. 如果你要导出一个没训练过的就设为 False.</span><br><span class="line">                  opset_version=10,          # ONNX 算子集的版本，当前已更新到15</span><br><span class="line">                  do_constant_folding=True,  # 是否执行常量折叠优化</span><br><span class="line">                  input_names = [&#x27;input&#x27;],   # 输入模型的张量的名称</span><br><span class="line">                  output_names = [&#x27;output&#x27;], # 输出模型的张量的名称</span><br><span class="line">                  # dynamic_axes将batch_size的维度指定为动态，</span><br><span class="line">                  # 后续进行推理的数据可以与导出的dummy_input的batch_size不同</span><br><span class="line">                  dynamic_axes=&#123;&#x27;input&#x27; : &#123;0 : &#x27;batch_size&#x27;&#125;,    </span><br><span class="line">                                &#x27;output&#x27; : &#123;0 : &#x27;batch_size&#x27;&#125;&#125;)</span><br></pre></td></tr></table></figure><h4 id="（3）可用性检查"><a href="#（3）可用性检查" class="headerlink" title="（3）可用性检查"></a>（3）可用性检查</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">import onnx</span><br><span class="line"># 我们可以使用异常处理的方法进行检验</span><br><span class="line">try:</span><br><span class="line">    # 当我们的模型不可用时，将会报出异常</span><br><span class="line">    onnx.checker.check_model(self.onnx_model)</span><br><span class="line">except onnx.checker.ValidationError as e:</span><br><span class="line">    print(&quot;The model is invalid: %s&quot;%e)</span><br><span class="line">else:</span><br><span class="line">    # 模型可用时，将不会报出异常，并会输出“The model is valid!”</span><br><span class="line">    print(&quot;The model is valid!&quot;)</span><br></pre></td></tr></table></figure><h4 id="（4）可视化"><a href="#（4）可视化" class="headerlink" title="（4）可视化"></a>（4）可视化</h4><p><strong>Netron</strong></p><p><img src="https://datawhalechina.github.io/thorough-pytorch/_images/screenshot.png" alt="img"></p>]]></content>
      
      
      
        <tags>
            
            <tag> 笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ThoroughPytorch——4</title>
      <link href="/posts/2b2f8273.html"/>
      <url>/posts/2b2f8273.html</url>
      
        <content type="html"><![CDATA[<h1 id="第七章：PyTorch可视化"><a href="#第七章：PyTorch可视化" class="headerlink" title="第七章：PyTorch可视化"></a>第七章：PyTorch可视化</h1><p><a href="https://datawhalechina.github.io/thorough-pytorch/第七章/index.html">第七章：PyTorch可视化 — 深入浅出PyTorch (datawhalechina.github.io)</a></p><h2 id="7-1-可视化网络结构"><a href="#7-1-可视化网络结构" class="headerlink" title="7.1 可视化网络结构"></a>7.1 可视化网络结构</h2><p>使用torchinfo来可视化网络结构</p><ul><li><strong>torchinfo的安装</strong></li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 安装方法一</span><br><span class="line">pip install torchinfo </span><br><span class="line"># 安装方法二</span><br><span class="line">conda install -c conda-forge torchinfo</span><br></pre></td></tr></table></figure><ul><li><strong>torchinfo的使用</strong></li></ul><p>只需使用<code>torchinfo.summary()</code>，</p><p>必需的参数分别是model，input_size[batch_size,channel,h,w]</p><p>更多参数可以参考<a href="https://github.com/TylerYep/torchinfo#documentation">documentation</a></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 例</span><br><span class="line">import torchvision.models as models</span><br><span class="line">from torchinfo import summary</span><br><span class="line">resnet18 = models.resnet18() # 实例化模型</span><br><span class="line">summary(resnet18, (1, 3, 224, 224)) # 1：batch_size 3:图片的通道数 224: 图片的高宽</span><br></pre></td></tr></table></figure><p>torchinfo提供了更加详细的信息，包括模块信息（每一层的类型、输出shape和参数量）、模型整体的参数量、模型大小、一次前向或者反向传播需要的内存大小等。</p><h2 id="7-2-CNN可视化"><a href="#7-2-CNN可视化" class="headerlink" title="7.2 CNN可视化"></a>7.2 CNN可视化</h2><h4 id="7-2-1-卷积核可视化"><a href="#7-2-1-卷积核可视化" class="headerlink" title="7.2.1 卷积核可视化"></a>7.2.1 卷积核可视化</h4><p>卷积核在CNN中负责提取特征，可视化卷积核能够帮助人们理解CNN各个层在提取什么样的特征，进而理解模型的工作原理。</p><p>在PyTorch中可视化卷积核也非常方便，核心在于特定层的卷积核即特定层的模型权重，可视化卷积核就等价于可视化对应的权重矩阵。</p><p><strong>首先加载模型，并确定模型的层信息：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">from torchvision.models import vgg11</span><br><span class="line"></span><br><span class="line">model = vgg11(pretrained=True)</span><br><span class="line">print(dict(model.features.named_children()))</span><br></pre></td></tr></table></figure><p><strong>卷积核对应的应为卷积层（Conv2d），这里以第“3”层为例，可视化对应的参数：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">conv1 = dict(model.features.named_children())[&#x27;3&#x27;]</span><br><span class="line">kernel_set = conv1.weight.detach()</span><br><span class="line">num = len(conv1.weight.detach())</span><br><span class="line">print(kernel_set.shape)</span><br><span class="line">for i in range(0,num):</span><br><span class="line">    i_kernel = kernel_set[i]</span><br><span class="line">    plt.figure(figsize=(20, 17))</span><br><span class="line">    if (len(i_kernel)) &gt; 1:</span><br><span class="line">        for idx, filer in enumerate(i_kernel):</span><br><span class="line">            plt.subplot(9, 9, idx+1) </span><br><span class="line">            plt.axis(&#x27;off&#x27;)</span><br><span class="line">            plt.imshow(filer[ :, :].detach(),cmap=&#x27;bwr&#x27;)</span><br><span class="line">torch.Size([128, 64, 3, 3])</span><br></pre></td></tr></table></figure><p>由于第“3”层的特征图由64维变为128维，因此共有128*64个卷积核，其中部分卷积核可视化效果如下图所示：</p><p><img src="https://datawhalechina.github.io/thorough-pytorch/_images/kernel_vis.png" alt="kernel"></p><h4 id="7-2-2-特征图可视化"><a href="#7-2-2-特征图可视化" class="headerlink" title="7.2.2 特征图可视化"></a>7.2.2 特征图可视化</h4><p>输入的原始图像经过每次卷积层得到的数据称为特征图，可视化即查看模型提取到的特征是什么样的。</p><p><strong><em>hook</em></strong>：PyTorch提供的<strong>使得网络在前向传播过程中能够获取到特征图</strong>的一个专用接口。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">class Hook(object): #定义Hook类</span><br><span class="line">    def __init__(self):</span><br><span class="line">        self.module_name = []</span><br><span class="line">        self.features_in_hook = []</span><br><span class="line">        self.features_out_hook = []</span><br><span class="line"></span><br><span class="line">    def __call__(self,module, fea_in, fea_out):</span><br><span class="line">        print(&quot;hooker working&quot;, self)</span><br><span class="line">        self.module_name.append(module.__class__)</span><br><span class="line">        self.features_in_hook.append(fea_in)</span><br><span class="line">        self.features_out_hook.append(fea_out) #存储当前层的输入和输出</span><br><span class="line">        return None</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">def plot_feature(model, idx, inputs):</span><br><span class="line">    hh = Hook()</span><br><span class="line">    model.features[idx].register_forward_hook(hh)  #该hook类的对象注册到要进行可视化的网络的某层中</span><br><span class="line">    </span><br><span class="line">    # forward_model(model,False)</span><br><span class="line">    model.eval()</span><br><span class="line">    _ = model(inputs)</span><br><span class="line">    print(hh.module_name)</span><br><span class="line">    print((hh.features_in_hook[0][0].shape))</span><br><span class="line">    print((hh.features_out_hook[0].shape))</span><br><span class="line">    </span><br><span class="line">    out1 = hh.features_out_hook[0]</span><br><span class="line"></span><br><span class="line">    total_ft  = out1.shape[1]</span><br><span class="line">    first_item = out1[0].cpu().clone()    </span><br><span class="line"></span><br><span class="line">    plt.figure(figsize=(20, 17))</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">    for ftidx in range(total_ft):</span><br><span class="line">        if ftidx &gt; 99:</span><br><span class="line">            break</span><br><span class="line">        ft = first_item[ftidx]</span><br><span class="line">        plt.subplot(10, 10, ftidx+1) </span><br><span class="line">        </span><br><span class="line">        plt.axis(&#x27;off&#x27;)</span><br><span class="line">        #plt.imshow(ft[ :, :].detach(),cmap=&#x27;gray&#x27;)</span><br><span class="line">        plt.imshow(ft[ :, :].detach())</span><br></pre></td></tr></table></figure><h4 id="7-2-3-class-activation-map可视化"><a href="#7-2-3-class-activation-map可视化" class="headerlink" title="7.2.3 class activation map可视化"></a>7.2.3 class activation map可视化</h4><p>class activation map （CAM）的作用是判断哪些变量（可视化场景下为像素点）对模型来说是重要的。</p><p>同时为了判断重要区域的梯度等信息，衍生出了Grad-CAM等诸多变种。</p><p>相较于上两条，CAM能一目了然地确定重要区域，进而进行可解释性分析或模型优化改进。</p><p><img src="https://datawhalechina.github.io/thorough-pytorch/_images/cam.png" alt="cam"></p><p>实现方法：</p><p><strong>pytorch-grad-cam</strong></p><ul><li>安装</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install grad-cam</span><br></pre></td></tr></table></figure><ul><li>一个简单的例子</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">from torchvision.models import vgg11,resnet18,resnet101,resnext101_32x8d</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">from PIL import Image</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">model = vgg11(pretrained=True)</span><br><span class="line">img_path = &#x27;./dog.png&#x27;</span><br><span class="line"># resize操作是为了和传入神经网络训练图片大小一致</span><br><span class="line">img = Image.open(img_path).resize((224,224))</span><br><span class="line"># 需要将原始图片转为np.float32格式并且在0-1之间 </span><br><span class="line">rgb_img = np.float32(img)/255</span><br><span class="line">plt.imshow(img)</span><br></pre></td></tr></table></figure><p><img src="https://datawhalechina.github.io/thorough-pytorch/_images/dog.png" alt="dog"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">from pytorch_grad_cam import GradCAM,ScoreCAM,GradCAMPlusPlus,AblationCAM,XGradCAM,EigenCAM,FullGrad</span><br><span class="line">from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget</span><br><span class="line">from pytorch_grad_cam.utils.image import show_cam_on_image</span><br><span class="line"></span><br><span class="line">target_layers = [model.features[-1]]</span><br><span class="line"># 选取合适的类激活图，但是ScoreCAM和AblationCAM需要batch_size</span><br><span class="line">cam = GradCAM(model=model,target_layers=target_layers)</span><br><span class="line">targets = [ClassifierOutputTarget(preds)]   </span><br><span class="line"># 上方preds需要设定，比如ImageNet有1000类，这里可以设为200</span><br><span class="line">grayscale_cam = cam(input_tensor=img_tensor, targets=targets)</span><br><span class="line">grayscale_cam = grayscale_cam[0, :]</span><br><span class="line">cam_img = show_cam_on_image(rgb_img, grayscale_cam, use_rgb=True)</span><br><span class="line">print(type(cam_img))</span><br><span class="line">Image.fromarray(cam_img)</span><br></pre></td></tr></table></figure><p><img src="https://datawhalechina.github.io/thorough-pytorch/_images/cam_dog.png" alt="grad_cam"></p><h4 id="7-2-4-FlashTorch快速可视化"><a href="#7-2-4-FlashTorch快速可视化" class="headerlink" title="7.2.4 FlashTorch快速可视化"></a>7.2.4 FlashTorch快速可视化</h4><p>对环境有要求：<a href="https://github.com/MisaOgura/flashtorch/issues/39">https://github.com/MisaOgura/flashtorch/issues/39</a></p><ul><li>安装</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install flashtorch</span><br></pre></td></tr></table></figure><ul><li>可视化梯度</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"># Download example images</span><br><span class="line"># !mkdir -p images</span><br><span class="line"># !wget -nv \</span><br><span class="line">#    https://github.com/MisaOgura/flashtorch/raw/master/examples/images/great_grey_owl.jpg \</span><br><span class="line">#    https://github.com/MisaOgura/flashtorch/raw/master/examples/images/peacock.jpg   \</span><br><span class="line">#    https://github.com/MisaOgura/flashtorch/raw/master/examples/images/toucan.jpg    \</span><br><span class="line">#    -P /content/images</span><br><span class="line"></span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import torchvision.models as models</span><br><span class="line">from flashtorch.utils import apply_transforms, load_image</span><br><span class="line">from flashtorch.saliency import Backprop</span><br><span class="line"></span><br><span class="line">model = models.alexnet(pretrained=True)</span><br><span class="line">backprop = Backprop(model)</span><br><span class="line"></span><br><span class="line">image = load_image(&#x27;/content/images/great_grey_owl.jpg&#x27;)</span><br><span class="line">owl = apply_transforms(image)</span><br><span class="line"></span><br><span class="line">target_class = 24</span><br><span class="line">backprop.visualize(owl, target_class, guided=True, use_gpu=True)</span><br></pre></td></tr></table></figure><p><img src="https://datawhalechina.github.io/thorough-pytorch/_images/ft_gradient.png" alt="ft-gradient"></p><ul><li>可视化卷积核</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">import torchvision.models as models</span><br><span class="line">from flashtorch.activmax import GradientAscent</span><br><span class="line"></span><br><span class="line">model = models.vgg16(pretrained=True)</span><br><span class="line">g_ascent = GradientAscent(model.features)</span><br><span class="line"></span><br><span class="line"># specify layer and filter info</span><br><span class="line">conv5_1 = model.features[24]</span><br><span class="line">conv5_1_filters = [45, 271, 363, 489]</span><br><span class="line"></span><br><span class="line">g_ascent.visualize(conv5_1, conv5_1_filters, title=&quot;VGG16: conv5_1&quot;)</span><br></pre></td></tr></table></figure><p><img src="https://datawhalechina.github.io/thorough-pytorch/_images/ft_activate.png" alt="ft-activate"></p><h2 id="7-3-使用TensorBoard可视化训练过程"><a href="#7-3-使用TensorBoard可视化训练过程" class="headerlink" title="7.3 使用TensorBoard可视化训练过程"></a>7.3 使用TensorBoard可视化训练过程</h2><p>可视化你所想可视化的所有内容。</p><h4 id="7-3-1安装"><a href="#7-3-1安装" class="headerlink" title="7.3.1安装"></a>7.3.1安装</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install tensorboardX</span><br></pre></td></tr></table></figure><h4 id="7-3-2可视化的基本逻辑"><a href="#7-3-2可视化的基本逻辑" class="headerlink" title="7.3.2可视化的基本逻辑"></a>7.3.2可视化的基本逻辑</h4><p>TensorBoard会将模型每一层的数据保存在指定位置 并以网页的形式可视化。</p><h4 id="7-3-3-配置与启动"><a href="#7-3-3-配置与启动" class="headerlink" title="7.3.3 配置与启动"></a>7.3.3 配置与启动</h4><p>①首先指定一个文件夹供TensorBoard保存记录下来的数据，然后调用tensorboard中的SummaryWriter。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">from tensorboardX import SummaryWriter</span><br><span class="line"></span><br><span class="line">writer = SummaryWriter(&#x27;./指定位置&#x27;) #可手动往文件夹里添加数据，也可以提取到其他机器</span><br></pre></td></tr></table></figure><p>※如果使用PyTorch自带的tensorboard，则采用如下方式import：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">from torch.utils.tensorboard import SummaryWriter</span><br></pre></td></tr></table></figure><p>②启动tensorboard</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tensorboard --logdir=/path/to/logs/ --port=xxxx</span><br><span class="line">#“path/to/logs/&quot;是指定的保存tensorboard记录结果的文件路径</span><br><span class="line">#port是外部访问TensorBoard的端口号，可以通过访问ip:port访问tensorboard</span><br></pre></td></tr></table></figure><h4 id="7-3-4-模型结构可视化"><a href="#7-3-4-模型结构可视化" class="headerlink" title="7.3.4 模型结构可视化"></a>7.3.4 模型结构可视化</h4><p>首先定义模型：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">import torch.nn as nn</span><br><span class="line"></span><br><span class="line">class Net(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(Net, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(in_channels=3,out_channels=32,kernel_size = 3)</span><br><span class="line">        self.pool = nn.MaxPool2d(kernel_size = 2,stride = 2)</span><br><span class="line">        self.conv2 = nn.Conv2d(in_channels=32,out_channels=64,kernel_size = 5)</span><br><span class="line">        self.adaptive_pool = nn.AdaptiveMaxPool2d((1,1))</span><br><span class="line">        self.flatten = nn.Flatten()</span><br><span class="line">        self.linear1 = nn.Linear(64,32)</span><br><span class="line">        self.relu = nn.ReLU()</span><br><span class="line">        self.linear2 = nn.Linear(32,1)</span><br><span class="line">        self.sigmoid = nn.Sigmoid()</span><br><span class="line"></span><br><span class="line">    def forward(self,x):</span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        x = self.pool(x)</span><br><span class="line">        x = self.conv2(x)</span><br><span class="line">        x = self.pool(x)</span><br><span class="line">        x = self.adaptive_pool(x)</span><br><span class="line">        x = self.flatten(x)</span><br><span class="line">        x = self.linear1(x)</span><br><span class="line">        x = self.relu(x)</span><br><span class="line">        x = self.linear2(x)</span><br><span class="line">        y = self.sigmoid(x)</span><br><span class="line">        return y</span><br><span class="line"></span><br><span class="line">model = Net()</span><br><span class="line">print(model)</span><br></pre></td></tr></table></figure><p>输出如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Net(</span><br><span class="line">  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))</span><br><span class="line">  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)</span><br><span class="line">  (conv2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))</span><br><span class="line">  (adaptive_pool): AdaptiveMaxPool2d(output_size=(1, 1))</span><br><span class="line">  (flatten): Flatten(start_dim=1, end_dim=-1)</span><br><span class="line">  (linear1): Linear(in_features=64, out_features=32, bias=True)</span><br><span class="line">  (relu): ReLU()</span><br><span class="line">  (linear2): Linear(in_features=32, out_features=1, bias=True)</span><br><span class="line">  (sigmoid): Sigmoid()</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>可视化模型的思路和7.1中介绍的方法一样，都是给定一个输入数据，前向传播后得到模型的结构，再通过TensorBoard进行可视化，使用add_graph：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">writer.add_graph(model, input_to_model = torch.rand(1, 3, 224, 224))</span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure><p>展示结果如下（其中框内部分初始会显示为“Net”，需要双击后才会展开）：</p><p><img src="https://datawhalechina.github.io/thorough-pytorch/_images/tb_model.png" alt="tb_model"></p><h4 id="7-3-5-TensorBoard图像可视化"><a href="#7-3-5-TensorBoard图像可视化" class="headerlink" title="7.3.5 TensorBoard图像可视化"></a>7.3.5 TensorBoard图像可视化</h4><ul><li>对于单张图片的显示使用add_image</li><li>对于多张图片的显示使用add_images</li><li>有时需要使用torchvision.utils.make_grid将多张图片拼成一张图片后，用writer.add_image显示</li></ul><p>以torchvision的CIFAR10数据集为例：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">import torchvision</span><br><span class="line">from torchvision import datasets, transforms</span><br><span class="line">from torch.utils.data import DataLoader</span><br><span class="line"></span><br><span class="line">transform_train = transforms.Compose(</span><br><span class="line">    [transforms.ToTensor()])</span><br><span class="line">transform_test = transforms.Compose(</span><br><span class="line">    [transforms.ToTensor()])</span><br><span class="line"></span><br><span class="line">train_data = datasets.CIFAR10(&quot;.&quot;, train=True, download=True, transform=transform_train)</span><br><span class="line">test_data = datasets.CIFAR10(&quot;.&quot;, train=False, download=True, transform=transform_test)</span><br><span class="line">train_loader = DataLoader(train_data, batch_size=64, shuffle=True)</span><br><span class="line">test_loader = DataLoader(test_data, batch_size=64)</span><br><span class="line"></span><br><span class="line">images, labels = next(iter(train_loader))</span><br><span class="line"> </span><br><span class="line">#依次进行以下三组可视化</span><br><span class="line"># 仅查看一张图片</span><br><span class="line">writer = SummaryWriter(&#x27;./pytorch_tb&#x27;)</span><br><span class="line">writer.add_image(&#x27;images[0]&#x27;, images[0])</span><br><span class="line">writer.close()</span><br><span class="line"> </span><br><span class="line"># 将多张图片拼接成一张图片，中间用黑色网格分割</span><br><span class="line"># create grid of images</span><br><span class="line">writer = SummaryWriter(&#x27;./pytorch_tb&#x27;)</span><br><span class="line">img_grid = torchvision.utils.make_grid(images)</span><br><span class="line">writer.add_image(&#x27;image_grid&#x27;, img_grid)</span><br><span class="line">writer.close()</span><br><span class="line"> </span><br><span class="line"># 将多张图片直接写入</span><br><span class="line">writer = SummaryWriter(&#x27;./pytorch_tb&#x27;)</span><br><span class="line">writer.add_images(&quot;images&quot;,images,global_step = 0)</span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure><p><img src="https://datawhalechina.github.io/thorough-pytorch/_images/tb_image.png" alt="tb_image"></p><p><img src="img_/pytorch1.png" alt="image-20221125220114712"></p><p><img src="https://datawhalechina.github.io/thorough-pytorch/_images/tb_images.png" alt="tb_images"></p><h4 id="7-3-6-TensorBoard连续变量可视化"><a href="#7-3-6-TensorBoard连续变量可视化" class="headerlink" title="7.3.6 TensorBoard连续变量可视化"></a>7.3.6 TensorBoard连续变量可视化</h4><p>适合损失函数的可视化，可以更加直观地了解模型的训练情况，从而确定最佳的checkpoint。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">writer = SummaryWriter(&#x27;./pytorch_tb&#x27;)</span><br><span class="line">for i in range(500):</span><br><span class="line">    x = i</span><br><span class="line">    y = x**2</span><br><span class="line">    writer.add_scalar(&quot;x&quot;, x, i) #日志中记录x在第step i 的值</span><br><span class="line">    writer.add_scalar(&quot;y&quot;, y, i) #日志中记录y在第step i 的值</span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure><p>可视化结果如下：</p><p><img src="https://datawhalechina.github.io/thorough-pytorch/_images/tb_scalar.png" alt="tb_scalar"></p><p>如果想在同一张图中显示多个曲线，则需要分别建立存放子路径（使用SummaryWriter指定路径即可自动创建，但需要在tensorboard运行目录下），同时在add_scalar中修改曲线的标签使其一致即可：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">writer1 = SummaryWriter(&#x27;./pytorch_tb/x&#x27;)</span><br><span class="line">writer2 = SummaryWriter(&#x27;./pytorch_tb/y&#x27;)</span><br><span class="line">for i in range(500):</span><br><span class="line">    x = i</span><br><span class="line">    y = x*2</span><br><span class="line">    writer1.add_scalar(&quot;same&quot;, x, i) #日志中记录x在第step i 的值</span><br><span class="line">    writer2.add_scalar(&quot;same&quot;, y, i) #日志中记录y在第step i 的值</span><br><span class="line">writer1.close()</span><br><span class="line">writer2.close()</span><br></pre></td></tr></table></figure><p><img src="https://datawhalechina.github.io/thorough-pytorch/_images/tb_twolines.png" alt="tb_scalar_twolines"></p><h4 id="7-3-7-TensorBoard参数分布可视化"><a href="#7-3-7-TensorBoard参数分布可视化" class="headerlink" title="7.3.7 TensorBoard参数分布可视化"></a>7.3.7 TensorBoard参数分布可视化</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">#举例</span><br><span class="line">import torch</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line"># 创建正态分布的张量模拟参数矩阵</span><br><span class="line">def norm(mean, std):</span><br><span class="line">    t = std * torch.randn((100, 20)) + mean</span><br><span class="line">    return t</span><br><span class="line"> </span><br><span class="line">writer = SummaryWriter(&#x27;./pytorch_tb/&#x27;)</span><br><span class="line">for step, mean in enumerate(range(-10, 10, 1)):</span><br><span class="line">    w = norm(mean, 1)</span><br><span class="line">    writer.add_histogram(&quot;w&quot;, w, step)</span><br><span class="line">    writer.flush()</span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure><h4 id="7-3-8-服务器端使用TensorBoard"><a href="#7-3-8-服务器端使用TensorBoard" class="headerlink" title="7.3.8 服务器端使用TensorBoard"></a>7.3.8 服务器端使用TensorBoard</h4><p><strong>（1）MAC端</strong></p><p><strong>打开终端，输入的命令依次如下：</strong></p><p>打开tensorflow的运行环境:source activate tensorflow<br>进入log的目录文件夹：cd desktop/tensorflow/<br>输入tensorboard命令：tensorboard —logdir=”log”</p><p><strong>在浏览器中输入网址：http:localhost:6006</strong></p><p><strong>（2）MobaXterm</strong></p><ol><li>在MobaXterm点击Tunneling</li><li>选择New SSH tunnel，我们会出现以下界面。</li></ol><p><img src="img_/pytorch2.png" alt="image-20221125225501168"></p><p><img src="img_/pytorch3.png" alt="image-20221125230142013"></p><ol><li>对新建的SSH通道做以下设置，第一栏我们选择<code>Local port forwarding</code>，<code>&lt;Remote Server&gt;</code>我们填写<strong>localhost</strong>，<code>&lt; Remote port&gt;</code>填写6006，tensorboard默认会在6006端口进行显示，我们也可以根据 <strong>tensorboard —logdir=/path/to/logs/ —port=xxxx</strong>的命令中的port进行修改，<code>&lt; SSH server&gt;</code> 填写我们连接服务器的ip地址，<code>&lt;SSH login&gt;</code>填写我们连接的服务器的用户名，<code>&lt;SSH port&gt;</code>填写端口号（通常为22），<code>&lt; forwarded port&gt;</code>填写的是本地的一个端口号，以便我们后面可以对其进行访问。</li><li>设定好之后，点击Save，然后Start。在启动tensorboard，这样我们就可以在本地的浏览器输入<code>http://localhost:6006/</code>对其进行访问了</li></ol><p><strong>（3）Xshell</strong></p><ol><li>Xshell的连接方法与MobaXterm的连接方式本质上是一样的，具体操作如下：</li><li>连接上服务器后，打开当前会话属性，会出现下图，我们选择隧道，点击添加 <img src="https://datawhalechina.github.io/thorough-pytorch/_images/xshell_ui.png" alt="xhell_ui"></li><li>按照下方图进行选择，其中目标主机代表的是服务器，源主机代表的是本地，端口的选择根据实际情况而定。 <img src="https://datawhalechina.github.io/thorough-pytorch/_images/xshell_set.png" alt="xhell_set"></li><li>启动tensorboard，在本地127.0.0.1:6006 或者 localhost:6006进行访问。</li></ol><p><strong>（4）SSL</strong></p><p>该方法是将服务器的6006端口重定向到自己机器上来，我们可以在本地的终端里输入以下代码：其中16006代表映射到本地的端口，6006代表的是服务器上的端口。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh -L 16006:127.0.0.1:6006 username@remote_server_ip</span><br></pre></td></tr></table></figure><p>在服务上使用默认的6006端口正常启动tensorboard</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensorboard --logdir=xxx --port=6006</span><br></pre></td></tr></table></figure><p>在本地的浏览器输入地址</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">127.0.0.1:16006 或者 localhost:16006</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> 笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ThoroughPytorch——3</title>
      <link href="/posts/b54b17d0.html"/>
      <url>/posts/b54b17d0.html</url>
      
        <content type="html"><![CDATA[<h1 id="第六章：PyTorch进阶训练技巧"><a href="#第六章：PyTorch进阶训练技巧" class="headerlink" title="第六章：PyTorch进阶训练技巧"></a>第六章：PyTorch进阶训练技巧</h1><p>DataWhale在线文档：<a href="https://datawhalechina.github.io/thorough-pytorch/第六章/index.html">https://datawhalechina.github.io/thorough-pytorch/第六章/index.html</a></p><h2 id="6-1-自定义损失函数"><a href="#6-1-自定义损失函数" class="headerlink" title="6.1 自定义损失函数"></a>6.1 自定义损失函数</h2><p>在科学研究中，我们往往会提出全新的损失函数来提升模型的表现，此时我们需要自己设计损失函数。</p><h4 id="（1）以函数方式定义"><a href="#（1）以函数方式定义" class="headerlink" title="（1）以函数方式定义"></a>（1）以函数方式定义</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">def my_loss(output, target):</span><br><span class="line">    loss = torch.mean((output - target)**2)</span><br><span class="line">    return loss</span><br></pre></td></tr></table></figure><h4 id="（2）以类方式定义"><a href="#（2）以类方式定义" class="headerlink" title="（2）以类方式定义"></a>（2）以类方式定义</h4><p>在以类方式定义损失函数时，我们如果看每一个损失函数的继承关系我们就可以发现<code>Loss</code>函数部分继承自<code>_loss</code>, 部分继承自<code>_WeightedLoss</code>, 而<code>_WeightedLoss</code>继承自<code>_loss</code>，<code>_loss</code>继承自 <strong>nn.Module</strong>。我们可以将其当作神经网络的一层来对待，同样地，我们的损失函数类就需要继承自<strong>nn.Module</strong>类。</p><p>例：Dice Loss      [ DSC = \frac{2|X∩Y|}{|X|+|Y|} ]</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">class DiceLoss(nn.Module):</span><br><span class="line">    def __init__(self,weight=None,size_average=True):</span><br><span class="line">        super(DiceLoss,self).__init__()</span><br><span class="line">        </span><br><span class="line">    def forward(self,inputs,targets,smooth=1):</span><br><span class="line">        inputs = F.sigmoid(inputs)       </span><br><span class="line">        inputs = inputs.view(-1)</span><br><span class="line">        targets = targets.view(-1)</span><br><span class="line">        intersection = (inputs * targets).sum()                   </span><br><span class="line">        dice = (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)  </span><br><span class="line">        return 1 - dice</span><br><span class="line"></span><br><span class="line"># 使用方法    </span><br><span class="line">criterion = DiceLoss()</span><br><span class="line">loss = criterion(input,targets)</span><br></pre></td></tr></table></figure><h2 id="6-2-动态调整学习率"><a href="#6-2-动态调整学习率" class="headerlink" title="6.2 动态调整学习率"></a>6.2 动态调整学习率</h2><p>我们可以通过一个适当的学习率衰减策略来改善学习率不能满足模型调优需求的情况，提高我们的精度。这种方式称为scheduler。</p><h4 id="（1）使用官方scheduler"><a href="#（1）使用官方scheduler" class="headerlink" title="（1）使用官方scheduler"></a>（1）使用官方scheduler</h4><p>一些封装在torch.optim.lr_scheduler中的调整学习率的方法</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"># 选择一种优化器</span><br><span class="line">optimizer = torch.optim.Adam(...) </span><br><span class="line"># 选择上面提到的一种或多种动态调整学习率的方法</span><br><span class="line">scheduler1 = torch.optim.lr_scheduler.... </span><br><span class="line">scheduler2 = torch.optim.lr_scheduler....</span><br><span class="line">...</span><br><span class="line">schedulern = torch.optim.lr_scheduler....</span><br><span class="line"># 进行训练</span><br><span class="line">for epoch in range(100):</span><br><span class="line">    train(...)</span><br><span class="line">    validate(...)</span><br><span class="line">    optimizer.step()</span><br><span class="line">    # 需要在优化器参数更新之后再动态调整学习率</span><br><span class="line">scheduler1.step() </span><br><span class="line">...</span><br><span class="line">    schedulern.step()  #放在optimizer.step()后面进行使用</span><br></pre></td></tr></table></figure><h4 id="（2）自定义scheduler"><a href="#（2）自定义scheduler" class="headerlink" title="（2）自定义scheduler"></a>（2）自定义scheduler</h4><p>自定义函数<code>adjust_learning_rate</code>来改变<code>param_group</code>中<code>lr</code>的值</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">def adjust_learning_rate(optimizer, epoch): #根据需要改变</span><br><span class="line">    lr = ....</span><br><span class="line">    for param_group in optimizer.param_groups:</span><br><span class="line">        param_group[&#x27;lr&#x27;] = lr</span><br><span class="line">        </span><br><span class="line">def adjust_learning_rate(optimizer,...):</span><br><span class="line">    ...</span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(),lr = args.lr,momentum = 0.9)</span><br><span class="line">for epoch in range(10):</span><br><span class="line">    train(...)</span><br><span class="line">    validate(...)</span><br><span class="line">    adjust_learning_rate(optimizer,epoch)</span><br></pre></td></tr></table></figure><h2 id="6-3-模型微调-torchvision"><a href="#6-3-模型微调-torchvision" class="headerlink" title="6.3 模型微调-torchvision"></a>6.3 模型微调-torchvision</h2><p>为解决数据集不足或花费较大的情况，使用迁移学习方法。</p><p>迁移学习的一大应用场景——预训练模型微调</p><h4 id="6-3-1-模型微调的流程"><a href="#6-3-1-模型微调的流程" class="headerlink" title="6.3.1 模型微调的流程"></a>6.3.1 模型微调的流程</h4><ol><li>在源数据集上预训练一个模型，称源模型。</li><li>创建一个新的目标模型，复制源模型上除输出层外的所有模型设计及其参数。</li><li>为目标模型添加一个输出⼤小为⽬标数据集类别个数的输出层，并随机初始化该层的模型参数。</li><li>在目标数据集上训练目标模型。从头训练输出层，其余层的参数都是基于源模型的参数微调得到的。</li></ol><p>我们假设这些模型参数包含了源数据集上学习到的知识，且这些知识同样适用于目标数据集。我们还假设源模型的输出层跟源数据集的标签紧密相关，因此在目标模型中不予采用。</p><p><img src="https://datawhalechina.github.io/thorough-pytorch/_images/finetune.png" alt="finetune"></p><h4 id="（2）使用已有模型结构"><a href="#（2）使用已有模型结构" class="headerlink" title="（2）使用已有模型结构"></a>（2）使用已有模型结构</h4><ul><li><p>实例化网络</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">import torchvision.models as models</span><br><span class="line">resnet18 = models.resnet18()</span><br><span class="line"># resnet18 = models.resnet18(pretrained=False)  等价于与上面的表达式</span><br><span class="line">alexnet = models.alexnet()</span><br><span class="line">vgg16 = models.vgg16()</span><br><span class="line">squeezenet = models.squeezenet1_0()</span><br><span class="line">densenet = models.densenet161()</span><br><span class="line">inception = models.inception_v3()</span><br><span class="line">googlenet = models.googlenet()</span><br><span class="line">shufflenet = models.shufflenet_v2_x1_0()</span><br><span class="line">mobilenet_v2 = models.mobilenet_v2()</span><br><span class="line">mobilenet_v3_large = models.mobilenet_v3_large()</span><br><span class="line">mobilenet_v3_small = models.mobilenet_v3_small()</span><br><span class="line">resnext50_32x4d = models.resnext50_32x4d()</span><br><span class="line">wide_resnet50_2 = models.wide_resnet50_2()</span><br><span class="line">mnasnet = models.mnasnet1_0()</span><br></pre></td></tr></table></figure></li><li><p>传递<code>pretrained</code>参数</p></li></ul><p>通过<code>True</code>或者<code>False</code>来决定是否使用预训练好的权重，在默认状态下<code>pretrained = False</code>，意味着我们不使用预训练得到的权重，当<code>pretrained = True</code>，意味着我们将使用在一些数据集上预训练得到的权重。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">import torchvision.models as models</span><br><span class="line">resnet18 = models.resnet18(pretrained=True)</span><br></pre></td></tr></table></figure><p><strong>注意事项：</strong></p><ol><li><p>通常PyTorch模型的扩展为<code>.pt</code>或<code>.pth</code>，程序运行时会检查默认路径是否有下载好的模型权重，权重下载后，下次加载不再需要下载。</p></li><li><p>一般下载较慢，可以直接迅雷或者其他方式去 <a href="https://github.com/pytorch/vision/tree/master/torchvision/models">这里</a> 查看自己的模型里面<code>model_urls</code>，然后手动下载，预训练模型的权重在<code>Linux</code>和<code>Mac</code>的默认下载路径是用户根目录下的<code>.cache</code>文件夹。在<code>Windows</code>下就是<code>C:\Users\&lt;username&gt;\.cache\torch\hub\checkpoint</code>。可以通过使用 <a href="https://pytorch.org/docs/stable/model_zoo.html#torch.utils.model_zoo.load_url"><code>torch.utils.model_zoo.load_url()</code></a>设置权重的下载地址。</p></li><li><p>还可以将权重自己下载放到同文件夹下，然后再将参数加载网络。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">self.model = models.resnet50(pretrained=False)</span><br><span class="line">self.model.load_state_dict(torch.load(&#x27;./model/resnet50-19c8e357.pth&#x27;))</span><br></pre></td></tr></table></figure></li><li><p>如果中途强行停止下载，一定去对应路径下将权重文件删除干净，不然可能会报错。</p></li></ol><h4 id="（3）训练特定层"><a href="#（3）训练特定层" class="headerlink" title="（3）训练特定层"></a>（3）训练特定层</h4><p>提取特征并且只想为新初始化的层计算梯度，其他参数不改变，就需要通过设置<code>requires_grad = False</code>来冻结部分层。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">def set_parameter_requires_grad(model, feature_extracting):</span><br><span class="line">    if feature_extracting:</span><br><span class="line">        for param in model.parameters():</span><br><span class="line">            param.requires_grad = False</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">import torchvision.models as models</span><br><span class="line"># 冻结参数的梯度</span><br><span class="line">feature_extract = True</span><br><span class="line">model = models.resnet18(pretrained=True)</span><br><span class="line">set_parameter_requires_grad(model, feature_extract) #引用上面</span><br><span class="line"># 修改模型</span><br><span class="line">num_ftrs = model.fc.in_features</span><br><span class="line">model.fc = nn.Linear(in_features=num_ftrs, out_features=4, bias=True)</span><br></pre></td></tr></table></figure><p>仅改变最后一层的模型参数，不改变特征提取的模型参数；注意我们先冻结模型参数的梯度，再对模型输出部分的全连接层进行修改，这样修改后的全连接层的参数就是可计算梯度的。之后在训练过程中，model仍会进行梯度回传，但是参数更新则只会发生在fc层。通过设定参数的requires_grad属性，我们完成了指定训练模型的特定层的目标，这对实现模型微调非常重要。</p><h2 id="6-4-模型微调-timm"><a href="#6-4-模型微调-timm" class="headerlink" title="6.4 模型微调 - timm"></a>6.4 模型微调 - timm</h2><p>timm是另一个预训练模型库，提供了许多计算机视觉的SOTA模型，可以当作是torchvision的扩充版本，并且里面的模型在准确度上也较高。</p><p>原文：<a href="https://datawhalechina.github.io/thorough-pytorch/第六章/6.3%20模型微调-timm.html">https://datawhalechina.github.io/thorough-pytorch/第六章/6.3%20模型微调-timm.html</a></p><h2 id="6-5半精度训练"><a href="#6-5半精度训练" class="headerlink" title="6.5半精度训练"></a>6.5半精度训练</h2><p>PyTorch默认的浮点数存储方式用的是torch.float32,多数场景其实并不需要这么精确,因此可进行半精度训练（torch.float16）以减少显存使用。</p><p><img src="https://datawhalechina.github.io/thorough-pytorch/_images/float16.jpg" alt="amp"></p><p>如何设置：</p><ul><li><strong>import autocast</strong></li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">from torch.cuda.amp import autocast</span><br></pre></td></tr></table></figure><ul><li><strong>模型设置</strong></li></ul><p>在模型定义中，使用python的装饰器方法，用autocast装饰模型中的forward函数。关于装饰器的使用，可以参考<a href="https://www.cnblogs.com/jfdwd/p/11253925.html">这里</a>：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">@autocast()   </span><br><span class="line">def forward(self, x):</span><br><span class="line">    ...</span><br><span class="line">    return x</span><br></pre></td></tr></table></figure><ul><li><strong>训练过程</strong></li></ul><p>在训练过程中，只需在将数据输入模型及其之后的部分放入“with autocast():“即可：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">for x in train_loader:</span><br><span class="line">x = x.cuda()</span><br><span class="line">with autocast():</span><br><span class="line">       output = model(x)</span><br><span class="line">       ...</span><br></pre></td></tr></table></figure><h2 id="6-6-数据增强-imgaug"><a href="#6-6-数据增强-imgaug" class="headerlink" title="6.6 数据增强-imgaug"></a>6.6 数据增强-imgaug</h2><p>深度学习需要大量数据，当数据量不够时，可使用数据增强技术，提高训练数据集的大小和质量。</p><h4 id="（1）imgaug"><a href="#（1）imgaug" class="headerlink" title="（1）imgaug"></a>（1）imgaug</h4><p><code>imgaug</code>是计算机视觉任务中常用的一个数据增强的包，相比于<code>torchvision.transforms</code>，它提供了更多的数据增强方法。</p><ol><li>Github地址：<a href="https://github.com/aleju/imgaug">imgaug</a></li><li>Readthedocs：<a href="https://imgaug.readthedocs.io/en/latest/source/examples_basics.html">imgaug</a></li><li>官方提供notebook例程：<a href="https://github.com/aleju/imgaug-doc/tree/master/notebooks">notebook</a></li></ol><p><strong>安装：</strong></p><h4 id="conda"><a href="#conda" class="headerlink" title="conda"></a>conda</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conda config --add channels conda-forge</span><br><span class="line">conda install imgaug</span><br></pre></td></tr></table></figure><h4 id="pip"><a href="#pip" class="headerlink" title="pip"></a>pip</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">#  install imgaug either via pypi</span><br><span class="line"></span><br><span class="line">pip install imgaug</span><br><span class="line"></span><br><span class="line">#  install the latest version directly from github</span><br><span class="line"></span><br><span class="line">pip install git+https://github.com/aleju/imgaug.git</span><br></pre></td></tr></table></figure><p>具体：<a href="https://datawhalechina.github.io/thorough-pytorch/第六章/6.5%20数据增强-imgaug.html">https://datawhalechina.github.io/thorough-pytorch/第六章/6.5%20数据增强-imgaug.html</a></p><h2 id="6-7-使用argparse进行调参"><a href="#6-7-使用argparse进行调参" class="headerlink" title="6.7 使用argparse进行调参"></a>6.7 使用argparse进行调参</h2><p>解析我们输入的命令行参数再传入模型的超参数中</p><p>命令行输入<code>python file.py --lr 1e-4 --batch_size 32</code>来完成对常见超参数的设置</p><h4 id="（1）使用"><a href="#（1）使用" class="headerlink" title="（1）使用"></a>（1）使用</h4><ul><li>创建<code>ArgumentParser()</code>对象</li><li>调用<code>add_argument()</code>方法添加参数</li><li>使用<code>parse_args()</code>解析参数</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"># demo.py</span><br><span class="line">import argparse</span><br><span class="line"></span><br><span class="line"># 创建ArgumentParser()对象</span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line"></span><br><span class="line"># 添加参数</span><br><span class="line">parser.add_argument(&#x27;-o&#x27;, &#x27;--output&#x27;, action=&#x27;store_true&#x27;, </span><br><span class="line">    help=&quot;shows output&quot;)</span><br><span class="line"># action = `store_true` 会将output参数记录为True</span><br><span class="line"># type 规定了参数的格式</span><br><span class="line"># default 规定了默认值</span><br><span class="line">parser.add_argument(&#x27;--lr&#x27;, type=float, default=3e-5, help=&#x27;select the learning rate, default=1e-3&#x27;) </span><br><span class="line"></span><br><span class="line">parser.add_argument(&#x27;--batch_size&#x27;, type=int, required=True, help=&#x27;input batch size&#x27;)  </span><br><span class="line"># 使用parse_args()解析函数</span><br><span class="line">args = parser.parse_args()</span><br><span class="line"></span><br><span class="line">if args.output:</span><br><span class="line">    print(&quot;This is some output&quot;)</span><br><span class="line">    print(f&quot;learning rate:&#123;args.lr&#125; &quot;)</span><br></pre></td></tr></table></figure><p>我们在命令行使用<code>python demo.py --lr 3e-4 --batch_size 32</code>，就可以看到以下的输出</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">This is some output</span><br><span class="line">learning rate: 3e-4</span><br></pre></td></tr></table></figure><h4 id="（2）原文作者的方法"><a href="#（2）原文作者的方法" class="headerlink" title="（2）原文作者的方法"></a>（2）原文作者的方法</h4><p>每个人都有着不同的超参数管理方式，在这里我将分享我使用argparse管理超参数的方式，希望可以对大家有一些借鉴意义。通常情况下，为了使代码更加简洁和模块化，我一般会将有关超参数的操作写在<code>config.py</code>，然后在<code>train.py</code>或者其他文件导入就可以。具体的<code>config.py</code>可以参考如下内容。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">import argparse  </span><br><span class="line">  </span><br><span class="line">def get_options(parser=argparse.ArgumentParser()):  </span><br><span class="line">  </span><br><span class="line">    parser.add_argument(&#x27;--workers&#x27;, type=int, default=0,  </span><br><span class="line">                        help=&#x27;number of data loading workers, you had better put it &#x27;  </span><br><span class="line">                              &#x27;4 times of your gpu&#x27;)  </span><br><span class="line">  </span><br><span class="line">    parser.add_argument(&#x27;--batch_size&#x27;, type=int, default=4, help=&#x27;input batch size, default=64&#x27;)  </span><br><span class="line">  </span><br><span class="line">    parser.add_argument(&#x27;--niter&#x27;, type=int, default=10, help=&#x27;number of epochs to train for, default=10&#x27;)  </span><br><span class="line">  </span><br><span class="line">    parser.add_argument(&#x27;--lr&#x27;, type=float, default=3e-5, help=&#x27;select the learning rate, default=1e-3&#x27;)  </span><br><span class="line">  </span><br><span class="line">    parser.add_argument(&#x27;--seed&#x27;, type=int, default=118, help=&quot;random seed&quot;)  </span><br><span class="line">  </span><br><span class="line">    parser.add_argument(&#x27;--cuda&#x27;, action=&#x27;store_true&#x27;, default=True, help=&#x27;enables cuda&#x27;)  </span><br><span class="line">    parser.add_argument(&#x27;--checkpoint_path&#x27;,type=str,default=&#x27;&#x27;,  </span><br><span class="line">                        help=&#x27;Path to load a previous trained model if not empty (default empty)&#x27;)  </span><br><span class="line">    parser.add_argument(&#x27;--output&#x27;,action=&#x27;store_true&#x27;,default=True,help=&quot;shows output&quot;)  </span><br><span class="line">  </span><br><span class="line">    opt = parser.parse_args()  </span><br><span class="line">  </span><br><span class="line">    if opt.output:  </span><br><span class="line">        print(f&#x27;num_workers: &#123;opt.workers&#125;&#x27;)  </span><br><span class="line">        print(f&#x27;batch_size: &#123;opt.batch_size&#125;&#x27;)  </span><br><span class="line">        print(f&#x27;epochs (niters) : &#123;opt.niter&#125;&#x27;)  </span><br><span class="line">        print(f&#x27;learning rate : &#123;opt.lr&#125;&#x27;)  </span><br><span class="line">        print(f&#x27;manual_seed: &#123;opt.seed&#125;&#x27;)  </span><br><span class="line">        print(f&#x27;cuda enable: &#123;opt.cuda&#125;&#x27;)  </span><br><span class="line">        print(f&#x27;checkpoint_path: &#123;opt.checkpoint_path&#125;&#x27;)  </span><br><span class="line">  </span><br><span class="line">    return opt  </span><br><span class="line">  </span><br><span class="line">if __name__ == &#x27;__main__&#x27;:  </span><br><span class="line">    opt = get_options()</span><br><span class="line">$ python config.py</span><br><span class="line"></span><br><span class="line">num_workers: 0</span><br><span class="line">batch_size: 4</span><br><span class="line">epochs (niters) : 10</span><br><span class="line">learning rate : 3e-05</span><br><span class="line">manual_seed: 118</span><br><span class="line">cuda enable: True</span><br><span class="line">checkpoint_path:</span><br></pre></td></tr></table></figure><p>随后在<code>train.py</code>等其他文件，我们就可以使用下面的这样的结构来调用参数。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"># 导入必要库</span><br><span class="line">...</span><br><span class="line">import config</span><br><span class="line"></span><br><span class="line">opt = config.get_options()</span><br><span class="line"></span><br><span class="line">manual_seed = opt.seed</span><br><span class="line">num_workers = opt.workers</span><br><span class="line">batch_size = opt.batch_size</span><br><span class="line">lr = opt.lr</span><br><span class="line">niters = opt.niters</span><br><span class="line">checkpoint_path = opt.checkpoint_path</span><br><span class="line"></span><br><span class="line"># 随机数的设置，保证复现结果</span><br><span class="line">def set_seed(seed):</span><br><span class="line">    torch.manual_seed(seed)</span><br><span class="line">    torch.cuda.manual_seed_all(seed)</span><br><span class="line">    random.seed(seed)</span><br><span class="line">    np.random.seed(seed)</span><br><span class="line">    torch.backends.cudnn.benchmark = False</span><br><span class="line">    torch.backends.cudnn.deterministic = True</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &#x27;__main__&#x27;:</span><br><span class="line">set_seed(manual_seed)</span><br><span class="line">for epoch in range(niters):</span><br><span class="line">train(model,lr,batch_size,num_workers,checkpoint_path)</span><br><span class="line">val(model,lr,batch_size,num_workers,checkpoint_path)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> 笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ThoroughPytorch——2</title>
      <link href="/posts/8987048d.html"/>
      <url>/posts/8987048d.html</url>
      
        <content type="html"><![CDATA[<h1 id="PyTorch模型定义"><a href="#PyTorch模型定义" class="headerlink" title="PyTorch模型定义"></a>PyTorch模型定义</h1><p>DataWhale:<a href="https://datawhalechina.github.io/thorough-pytorch/">https://datawhalechina.github.io/thorough-pytorch/</a>  </p><h2 id="一、模型定义"><a href="#一、模型定义" class="headerlink" title="一、模型定义"></a>一、模型定义</h2><ul><li><code>Module</code> 类是 <code>torch.nn</code> 模块里提供的一个模型构造类 (<code>nn.Module</code>)，是所有神经⽹网络模块的基类，我们可以继承它来定义我们想要的模型；</li><li>PyTorch模型定义应包括两个主要部分：各个部分的初始化（<code>__init__</code>）；数据流向定义（<code>forward</code>）</li></ul><p>基于<code>nn.Module</code>，我们可以通过<code>Sequential</code>，<code>ModuleList</code>和<code>ModuleDict</code>三种方式定义PyTorch模型。</p><h3 id="1-Sequential"><a href="#1-Sequential" class="headerlink" title="1. Sequential"></a>1. Sequential</h3><p>   可更加简单地定义前向计算为简单串联各层的模型。</p><p>   接收子模块或其有序字典作为参数逐一添加作为实例以进行前向计算。</p><p>   灵活性差，不适合加入外部输入。</p>   <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">import torch.nn as nn</span><br><span class="line">net = nn.Sequential(</span><br><span class="line">        nn.Linear(784, 256),</span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        nn.Linear(256, 10), </span><br><span class="line">        )    #直接排列</span><br></pre></td></tr></table></figure>   <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">import collections</span><br><span class="line">import torch.nn as nn</span><br><span class="line">net2 = nn.Sequential(collections.OrderedDict([</span><br><span class="line">          (&#x27;fc1&#x27;, nn.Linear(784, 256)),</span><br><span class="line">          (&#x27;relu1&#x27;, nn.ReLU()),</span><br><span class="line">          (&#x27;fc2&#x27;, nn.Linear(256, 10))</span><br><span class="line">          ]))    #使用OrderedDict</span><br></pre></td></tr></table></figure><h3 id="2-ModuleList"><a href="#2-ModuleList" class="headerlink" title="2.ModuleList"></a>2.ModuleList</h3><p>接收一个子模块（或层，需属于<code>nn.Module</code>类）的列表作为输入</p><p>可以进行append和extend操作</p><p>需要经过forward函数指定各个层的先后顺序</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">net = nn.ModuleList([nn.Linear(784, 256), nn.ReLU()])</span><br><span class="line">net.append(nn.Linear(256, 10)) # # 类似List的append操作</span><br><span class="line">print(net[-1])  # 类似List的索引访问</span><br></pre></td></tr></table></figure><h3 id="3-ModuleDict"><a href="#3-ModuleDict" class="headerlink" title="3.ModuleDict"></a>3.ModuleDict</h3><p>和<code>ModuleList</code>类似，只是<code>ModuleDict</code>能够更方便地为神经网络的层添加名称</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">net = nn.ModuleDict(&#123;</span><br><span class="line">    &#x27;linear&#x27;: nn.Linear(784, 256),</span><br><span class="line">    &#x27;act&#x27;: nn.ReLU(),</span><br><span class="line">&#125;)</span><br><span class="line">net[&#x27;output&#x27;] = nn.Linear(256, 10) # 添加</span><br><span class="line">print(net[&#x27;linear&#x27;]) # 访问</span><br><span class="line">print(net.output)</span><br></pre></td></tr></table></figure><h2 id="二、利用模型块快速搭建复杂网络"><a href="#二、利用模型块快速搭建复杂网络" class="headerlink" title="二、利用模型块快速搭建复杂网络"></a>二、利用模型块快速搭建复杂网络</h2><p>以U-Net为例</p><h3 id="1-U-Net"><a href="#1-U-Net" class="headerlink" title="1.U-Net"></a>1.U-Net</h3><p>通过残差连接结构解决了模型学习中的退化问题，使得神经网络的深度能够不断扩展。</p><p>1）梯度消失问题</p><p>我们发现很深的网络层，由于参数初始化一般更靠近0，这样在训练的过程中更新浅层网络的参数时，很容易随着网络的深入而导致梯度消失，浅层的参数无法更新。</p><p>2）网络退化问题</p><p>举个例子，假设已经有了一个最优化的网络结构，是18层。当我们设计网络结构的时候，我们并不知道具体多少层次的网络时最优化的网络结构，假设设计了34层网络结构。那么多出来的16层其实是冗余的，我们希望训练网络的过程中，模型能够自己将这16层冗余层训练为恒等映射，也就是经过这层时的输入与输出完全一样。但是往往模型很难将这16层恒等映射的参数学习正确，那么就不如最优化的18层网络结构的性能，这就是随着网络深度增加，模型会产生退化现象。它不是由过拟合产生的，而是由冗余的网络层学习了不是恒等映射的参数造成的。</p><p><img src="https://datawhalechina.github.io/thorough-pytorch/_images/5.2.1unet.png" alt="unet"></p><p>组成U-Net的模型块主要有如下几个部分：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">import torch.nn as nn</span><br><span class="line">import torch.nn.functional as F</span><br></pre></td></tr></table></figure><p>1）每个子块内部的两次卷积（Double Convolution）</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">class DoubleConv(nn.Module):</span><br><span class="line">    &quot;&quot;&quot;(convolution =&gt; [BN] =&gt; ReLU) * 2&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    def __init__(self, in_channels, out_channels, mid_channels=None):</span><br><span class="line">        super().__init__()</span><br><span class="line">        if not mid_channels:</span><br><span class="line">            mid_channels = out_channels</span><br><span class="line">        self.double_conv = nn.Sequential(</span><br><span class="line">            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),</span><br><span class="line">            nn.BatchNorm2d(mid_channels),</span><br><span class="line">            nn.ReLU(inplace=True),</span><br><span class="line">            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),</span><br><span class="line">            nn.BatchNorm2d(out_channels),</span><br><span class="line">            nn.ReLU(inplace=True)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        return self.double_conv(x)</span><br></pre></td></tr></table></figure><p>2）左侧模型块之间的下采样连接，即最大池化（Max pooling）</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">class Down(nn.Module):</span><br><span class="line">    &quot;&quot;&quot;Downscaling with maxpool then double conv&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    def __init__(self, in_channels, out_channels):</span><br><span class="line">        super().__init__()</span><br><span class="line">        self.maxpool_conv = nn.Sequential(</span><br><span class="line">            nn.MaxPool2d(2),</span><br><span class="line">            DoubleConv(in_channels, out_channels)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        return self.maxpool_conv(x)</span><br></pre></td></tr></table></figure><p>3）右侧模型块之间的上采样连接（Up sampling）</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">class Up(nn.Module):</span><br><span class="line">    &quot;&quot;&quot;Upscaling then double conv&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    def __init__(self, in_channels, out_channels, bilinear=False):</span><br><span class="line">        super().__init__()</span><br><span class="line"></span><br><span class="line">        # if bilinear, use the normal convolutions to reduce the number of channels</span><br><span class="line">        if bilinear:</span><br><span class="line">            self.up = nn.Upsample(scale_factor=2, mode=&#x27;bilinear&#x27;, align_corners=True)</span><br><span class="line">            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)</span><br><span class="line">        else:</span><br><span class="line">            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)</span><br><span class="line">            self.conv = DoubleConv(in_channels, out_channels)</span><br><span class="line"></span><br><span class="line">    def forward(self, x1, x2):</span><br><span class="line">        x1 = self.up(x1)</span><br><span class="line">        # input is CHW</span><br><span class="line">        diffY = x2.size()[2] - x1.size()[2]</span><br><span class="line">        diffX = x2.size()[3] - x1.size()[3]</span><br><span class="line"></span><br><span class="line">        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,</span><br><span class="line">                        diffY // 2, diffY - diffY // 2])</span><br><span class="line">        # if you have padding issues, see</span><br><span class="line">        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a</span><br><span class="line">        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd</span><br><span class="line">        x = torch.cat([x2, x1], dim=1)</span><br><span class="line">        return self.conv(x)</span><br></pre></td></tr></table></figure><p>4）输出层的处理</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">class OutConv(nn.Module):</span><br><span class="line">    def __init__(self, in_channels, out_channels):</span><br><span class="line">        super(OutConv, self).__init__()</span><br><span class="line">        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        return self.conv(x)</span><br></pre></td></tr></table></figure><h2 id="三、修改模型"><a href="#三、修改模型" class="headerlink" title="三、修改模型"></a>三、修改模型</h2><p>我们有时需要对模型结构进行必要的修改。</p><h3 id="1-修改模型层"><a href="#1-修改模型层" class="headerlink" title="1.修改模型层"></a>1.修改模型层</h3><p>可修改输出节点数、层数等。</p><h3 id="2-添加外部输入"><a href="#2-添加外部输入" class="headerlink" title="2.添加外部输入"></a>2.添加外部输入</h3><p>将原模型添加输入位置前的部分作为一个整体，同时在forward中定义好原模型不变的部分、添加的输入和后续层之间的连接关系，从而完成模型的修改。</p><h3 id="3-添加额外输出"><a href="#3-添加额外输出" class="headerlink" title="3.添加额外输出"></a>3.添加额外输出</h3><p>输出模型某一中间层的结果，以施加额外的监督，获得更好的中间层结果。基本的思路是修改模型定义中forward函数的return变量。</p><h2 id="四、PyTorch模型保存与读取"><a href="#四、PyTorch模型保存与读取" class="headerlink" title="四、PyTorch模型保存与读取"></a>四、PyTorch模型保存与读取</h2><p>一个PyTorch模型主要包含两个部分：模型结构和权重。</p><p>模型是继承nn.Module的类，权重的数据结构是一个字典（key是层名，value是权重向量）。</p><p>两种形式：存储整个模型（包括结构和权重），和只存储模型权重。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 保存整个模型</span><br><span class="line">torch.save(model, save_dir)</span><br><span class="line"># 保存模型权重</span><br><span class="line">torch.save(model.state_dict, save_dir)</span><br></pre></td></tr></table></figure><p>关于单卡和多卡的问题：（DataWhale在线文档）<a href="https://datawhalechina.github.io/thorough-pytorch/第五章/5.4%20PyTorh模型保存与读取.html">https://datawhalechina.github.io/thorough-pytorch/第五章/5.4%20PyTorh模型保存与读取.html</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ThoroughPytorch——1</title>
      <link href="/posts/5b4576fc.html"/>
      <url>/posts/5b4576fc.html</url>
      
        <content type="html"><![CDATA[<h1 id="三、Pytorch主要组成"><a href="#三、Pytorch主要组成" class="headerlink" title="三、Pytorch主要组成"></a>三、Pytorch主要组成</h1><h2 id="3-1深度学习总览"><a href="#3-1深度学习总览" class="headerlink" title="3.1深度学习总览"></a>3.1深度学习总览</h2><h3 id="完成一项机器学习的步骤：数据预处理——数据划分——模型选择——损失函数和优化方法及对应超参数的设定——模型拟合，计算模型表现"><a href="#完成一项机器学习的步骤：数据预处理——数据划分——模型选择——损失函数和优化方法及对应超参数的设定——模型拟合，计算模型表现" class="headerlink" title="完成一项机器学习的步骤：数据预处理——数据划分——模型选择——损失函数和优化方法及对应超参数的设定——模型拟合，计算模型表现"></a>完成一项机器学习的步骤：数据预处理——数据划分——模型选择——损失函数和优化方法及对应超参数的设定——模型拟合，计算模型表现</h3><p>深度学习与机器学习在流程上类似，但由于深度学习样本量大可能会超出内存，同时还有批训练等，因此须有另外的设计。</p><p>深度神经网络往往需要“逐层”搭建一些用于实现特定功能的层，或者预先定义好可以实现特定功能的模块，再把这些模块组装起来。</p><p>损失函数和优化器要能够保证反向传播能够在用户自行定义的模型结构上实现。</p><p>按批读入数据，放入GPU训练，然后将损失函数反向传播回网络最前面的层，同时使用优化器调整网络参数。之后需要根据设定好的指标计算模型表现。</p><h2 id="3-2基本配置"><a href="#3-2基本配置" class="headerlink" title="3.2基本配置"></a>3.2基本配置</h2><h3 id="（1）包的导入"><a href="#（1）包的导入" class="headerlink" title="（1）包的导入"></a>（1）包的导入</h3><p>建议导入了一些常用的包</p><h3 id="（2）习惯统一设置的参数"><a href="#（2）习惯统一设置的参数" class="headerlink" title="（2）习惯统一设置的参数"></a>（2）习惯统一设置的参数</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">batch_size = 16</span><br><span class="line"># 批次的大小</span><br><span class="line">lr = 1e-4</span><br><span class="line"># 优化器的学习率</span><br><span class="line">max_epochs = 100</span><br></pre></td></tr></table></figure><h3 id="（3）GPU的设置"><a href="#（3）GPU的设置" class="headerlink" title="（3）GPU的设置"></a>（3）GPU的设置</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 方案一：使用os.environ，这种情况如果使用GPU不需要设置</span><br><span class="line">os.environ[&#x27;CUDA_VISIBLE_DEVICES&#x27;] = &#x27;0,1&#x27;</span><br><span class="line"></span><br><span class="line"># 方案二：使用“device”，后续对要使用GPU的变量用.to(device)即可</span><br><span class="line">device = torch.device(&quot;cuda:1&quot; if torch.cuda.is_available() else &quot;cpu&quot;)</span><br></pre></td></tr></table></figure><h2 id="3-3数据读入"><a href="#3-3数据读入" class="headerlink" title="3.3数据读入"></a>3.3数据读入</h2><p>PyTorch数据读入是通过Dataset+DataLoader的方式完成的，Dataset定义好数据的格式和数据变换形式，DataLoader用iterative的方式不断读入批次数据。</p><h3 id="自定义类三个函数："><a href="#自定义类三个函数：" class="headerlink" title="自定义类三个函数："></a>自定义类三个函数：</h3><ul><li><code>__init__</code>: 用于向类中传入外部参数，同时定义样本集</li><li><code>__getitem__</code>: 用于逐个读取样本集合中的元素，可以进行一定的变换，并将返回训练/验证所需的数据</li><li><code>__len__</code>: 用于返回数据集的样本数</li></ul><h2 id="3-4模型构建"><a href="#3-4模型构建" class="headerlink" title="3.4模型构建"></a>3.4模型构建</h2><h3 id="（1）继承Module类构造模型"><a href="#（1）继承Module类构造模型" class="headerlink" title="（1）继承Module类构造模型"></a>（1）继承Module类构造模型</h3><p>例：继承 Module 类构造多层感知机</p><p>MLP 类重载了 Module 类的 init 函数和 forward 函数，分别用于创建模型参数和定义前向计算（正向传播）。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">from torch import nn</span><br><span class="line"></span><br><span class="line">class MLP(nn.Module):</span><br><span class="line">  # 声明带有模型参数的层，这里声明了两个全连接层</span><br><span class="line">  def __init__(self, **kwargs):</span><br><span class="line">    # 调用MLP父类Block的构造函数来进行必要的初始化。这样在构造实例时还可以指定其他函数</span><br><span class="line">    super(MLP, self).__init__(**kwargs)</span><br><span class="line">    self.hidden = nn.Linear(784, 256)</span><br><span class="line">    self.act = nn.ReLU()</span><br><span class="line">    self.output = nn.Linear(256,10)</span><br><span class="line">    </span><br><span class="line">   # 定义模型的前向计算，即如何根据输入x计算返回所需要的模型输出</span><br><span class="line">  def forward(self, x):</span><br><span class="line">    o = self.act(self.hidden(x))</span><br><span class="line">    return self.output(o)   </span><br><span class="line">    以上的 MLP 类中⽆须定义反向传播函数。系统将通过⾃动求梯度⽽自动⽣成反向传播所需的 backward 函数。</span><br></pre></td></tr></table></figure><h3 id="（2）自定义层"><a href="#（2）自定义层" class="headerlink" title="（2）自定义层"></a>（2）自定义层</h3><ul><li>不含模型参数的层</li></ul><p>例：将输入减掉均值后输出</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">from torch import nn</span><br><span class="line"></span><br><span class="line">class MyLayer(nn.Module):</span><br><span class="line">    def __init__(self, **kwargs):</span><br><span class="line">        super(MyLayer, self).__init__(**kwargs)</span><br><span class="line">    def forward(self, x):</span><br><span class="line">        return x - x.mean()  </span><br></pre></td></tr></table></figure><ul><li>含模型参数的层</li></ul><p>通过训练得到模型参数</p><h3 id="常见层"><a href="#常见层" class="headerlink" title="常见层"></a><strong>常见层</strong></h3><ul><li><p>二维卷积层</p><p>二维卷积层将输入和卷积核做互相关运算，并加上一个标量偏差来得到输出。卷积层的模型参数包括了卷积核和标量偏差。在训练模型的时候，通常我们先对卷积核随机初始化，然后不断迭代卷积核和偏差。</p></li><li><p>池化层</p><p>池化层每次对输入数据的一个固定形状窗口(⼜称池化窗口)中的元素计算输出。不同于卷积层里计算输⼊和核的互相关性，池化层直接计算池化窗口内元素的最大值或者平均值。该运算也 分别叫做最大池化或平均池化。在二维最⼤池化中，池化窗口从输入数组的最左上方开始，按从左往右、从上往下的顺序，依次在输⼊数组上滑动。当池化窗口滑动到某⼀位置时，窗口中的输入子数组的最大值即输出数组中相应位置的元素。</p></li></ul><h2 id="3-5-模型初始化"><a href="#3-5-模型初始化" class="headerlink" title="3.5 模型初始化"></a>3.5 模型初始化</h2><p>权重的初始值极为重要。一个好的权重值，会使模型收敛速度提高，使模型准确率更精确。</p><h3 id="torch-nn-init中为我们提供了常用的初始化方法。"><a href="#torch-nn-init中为我们提供了常用的初始化方法。" class="headerlink" title="torch.nn.init中为我们提供了常用的初始化方法。"></a><code>torch.nn.init</code>中为我们提供了常用的初始化方法。</h3><h2 id="3-6损失函数"><a href="#3-6损失函数" class="headerlink" title="3.6损失函数"></a>3.6损失函数</h2><p>损失函数即对模型效果的负反馈，很重要，模型可按此改进。</p><h3 id="原文对损失函数进行了简单举例：3-6-损失函数-—-深入浅出PyTorch-datawhalechina-github-io"><a href="#原文对损失函数进行了简单举例：3-6-损失函数-—-深入浅出PyTorch-datawhalechina-github-io" class="headerlink" title="原文对损失函数进行了简单举例：3.6 损失函数 — 深入浅出PyTorch (datawhalechina.github.io)"></a>原文对损失函数进行了简单举例：<a href="https://datawhalechina.github.io/thorough-pytorch/第三章/3.6 损失函数.html">3.6 损失函数 — 深入浅出PyTorch (datawhalechina.github.io)</a></h3><h2 id="3-7训练和评估"><a href="#3-7训练和评估" class="headerlink" title="3.7训练和评估"></a>3.7训练和评估</h2><h3 id="（1）训练"><a href="#（1）训练" class="headerlink" title="（1）训练"></a>（1）训练</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.train()   # 训练状态</span><br></pre></td></tr></table></figure><p>用for循环读取DataLoader中的全部数据。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">for data, label in train_loader:</span><br></pre></td></tr></table></figure><p>之后将数据放到GPU上用于后续计算，此处以.cuda()为例</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data, label = data.cuda(), label.cuda()</span><br></pre></td></tr></table></figure><p>开始用当前批次数据做训练时，应当先将优化器的梯度置零：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer.zero_grad()</span><br></pre></td></tr></table></figure><p>之后将data送入模型中训练：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">output = model(data)</span><br></pre></td></tr></table></figure><p>根据预先定义的criterion计算损失函数：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss = criterion(output, label)</span><br></pre></td></tr></table></figure><p>将loss反向传播回网络：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss.backward()</span><br></pre></td></tr></table></figure><p>使用优化器更新模型参数：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer.step()</span><br></pre></td></tr></table></figure><h3 id="（2）评估"><a href="#（2）评估" class="headerlink" title="（2）评估"></a>（2）评估</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.eval()   # 验证/测试状态</span><br></pre></td></tr></table></figure><p>验证/测试的流程基本与训练过程一致，不同点在于：</p><ul><li>需要预先设置torch.no_grad，以及将model调至eval模式</li><li>不需要将优化器的梯度置零</li><li>不需要将loss反向回传到网络</li><li>不需要更新optimizer</li></ul><h2 id="3-8可视化"><a href="#3-8可视化" class="headerlink" title="3.8可视化"></a>3.8可视化</h2><p>将训练过程中的一些参数可视化能更好的反映模型相关的内容。</p><h2 id="3-9Pytorch优化器"><a href="#3-9Pytorch优化器" class="headerlink" title="3.9Pytorch优化器"></a>3.9Pytorch优化器</h2><p>深度学习就是寻找最优参数的过程，对多参数的寻找有俩各种方法：</p><h3 id="（1）暴力穷举"><a href="#（1）暴力穷举" class="headerlink" title="（1）暴力穷举"></a>（1）暴力穷举</h3><h3 id="（2）BP-优化器（根据网络反向传播的梯度信息来更新网络的参数，以起到降低loss函数计算值，使得模型输出更加接近真实标签。）"><a href="#（2）BP-优化器（根据网络反向传播的梯度信息来更新网络的参数，以起到降低loss函数计算值，使得模型输出更加接近真实标签。）" class="headerlink" title="（2）BP+优化器（根据网络反向传播的梯度信息来更新网络的参数，以起到降低loss函数计算值，使得模型输出更加接近真实标签。）"></a>（2）BP+优化器（根据网络反向传播的梯度信息来更新网络的参数，以起到降低loss函数计算值，使得模型输出更加接近真实标签。）</h3><p>Pytorch的优化器库（torch.optim）</p><p>原文：<a href="https://datawhalechina.github.io/thorough-pytorch/第三章/3.9 优化器.html">3.9 Pytorch优化器 — 深入浅出PyTorch (datawhalechina.github.io)</a></p><h1 id="四、基础实战——FashionMNIST时装分类"><a href="#四、基础实战——FashionMNIST时装分类" class="headerlink" title="四、基础实战——FashionMNIST时装分类"></a>四、基础实战——FashionMNIST时装分类</h1><h3 id="十类服装分类，使用FashionMNIST数据集。（已预划分好训练和测试集，训练集共60-000张图像，测试集共10-000张图像。每张图像均为单通道黑白图像，大小为28-28pixel，分属10个类别。）"><a href="#十类服装分类，使用FashionMNIST数据集。（已预划分好训练和测试集，训练集共60-000张图像，测试集共10-000张图像。每张图像均为单通道黑白图像，大小为28-28pixel，分属10个类别。）" class="headerlink" title="十类服装分类，使用FashionMNIST数据集。（已预划分好训练和测试集，训练集共60,000张图像，测试集共10,000张图像。每张图像均为单通道黑白图像，大小为28*28pixel，分属10个类别。）"></a>十类服装分类，使用<a href="https://github.com/zalandoresearch/fashion-mnist/tree/master/data/fashion">FashionMNIST数据集</a>。（已预划分好训练和测试集，训练集共60,000张图像，测试集共10,000张图像。每张图像均为单通道黑白图像，大小为28*28pixel，分属10个类别。）</h3><p><strong>过程讲解详细，训练结果待补充。。。</strong></p>]]></content>
      
      
      
        <tags>
            
            <tag> 笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pytorch基础</title>
      <link href="/posts/787ba630.html"/>
      <url>/posts/787ba630.html</url>
      
        <content type="html"><![CDATA[<p>DataWhale在线阅读地址：<a href="https://datawhalechina.github.io/thorough-pytorch/">https://datawhalechina.github.io/thorough-pytorch/</a>  </p><h1 id="PyTorch"><a href="#PyTorch" class="headerlink" title="PyTorch"></a>PyTorch</h1><h2 id="PyTorch是由Facebook人工智能研究小组开发的一种基于Lua编写的Torch库的Python实现的深度学习库，目前已被广泛应用，并正在取代TensorFlow的主导地位。"><a href="#PyTorch是由Facebook人工智能研究小组开发的一种基于Lua编写的Torch库的Python实现的深度学习库，目前已被广泛应用，并正在取代TensorFlow的主导地位。" class="headerlink" title="PyTorch是由Facebook人工智能研究小组开发的一种基于Lua编写的Torch库的Python实现的深度学习库，目前已被广泛应用，并正在取代TensorFlow的主导地位。"></a>PyTorch是由Facebook人工智能研究小组开发的一种基于Lua编写的Torch库的Python实现的深度学习库，目前已被广泛应用，并正在取代TensorFlow的主导地位。</h2><h2 id="1-优势"><a href="#1-优势" class="headerlink" title="1.优势"></a>1.优势</h2><h3 id="1-简洁易理解"><a href="#1-简洁易理解" class="headerlink" title="1)简洁易理解"></a>1)简洁易理解</h3><h3 id="2）上手难度小"><a href="#2）上手难度小" class="headerlink" title="2）上手难度小"></a>2）上手难度小</h3><h3 id="3）有良好的文档和社区支持，更新有保障"><a href="#3）有良好的文档和社区支持，更新有保障" class="headerlink" title="3）有良好的文档和社区支持，更新有保障"></a>3）有良好的文档和社区支持，更新有保障</h3><h3 id="4）项目开源"><a href="#4）项目开源" class="headerlink" title="4）项目开源"></a>4）项目开源</h3><h3 id="5）更好的代码调试"><a href="#5）更好的代码调试" class="headerlink" title="5）更好的代码调试"></a>5）更好的代码调试</h3><h3 id="6）愈加完善的库"><a href="#6）愈加完善的库" class="headerlink" title="6）愈加完善的库"></a>6）愈加完善的库</h3><h2 id="2-安装（将在下一文中记录过程）"><a href="#2-安装（将在下一文中记录过程）" class="headerlink" title="2.安装（将在下一文中记录过程）"></a>2.安装（将在下一文中记录过程）</h2><h3 id="2-1Anaconda的安装"><a href="#2-1Anaconda的安装" class="headerlink" title="2.1Anaconda的安装"></a>2.1Anaconda的安装</h3><h3 id="2-2-查看显卡"><a href="#2-2-查看显卡" class="headerlink" title="2.2 查看显卡"></a>2.2 查看显卡</h3><h3 id="2-3-安装PyTorch"><a href="#2-3-安装PyTorch" class="headerlink" title="2.3 安装PyTorch"></a>2.3 安装PyTorch</h3><h3 id="2-4-PyCharm安装"><a href="#2-4-PyCharm安装" class="headerlink" title="2.4 PyCharm安装"></a>2.4 PyCharm安装</h3><h2 id="3-学习资源"><a href="#3-学习资源" class="headerlink" title="3.学习资源"></a>3.学习资源</h2><h3 id="1）Awesome-pytorch-list：目前已获12K-Star，包含了NLP-CV-常见库，论文实现以及Pytorch的其他项目。"><a href="#1）Awesome-pytorch-list：目前已获12K-Star，包含了NLP-CV-常见库，论文实现以及Pytorch的其他项目。" class="headerlink" title="1）Awesome-pytorch-list：目前已获12K Star，包含了NLP,CV,常见库，论文实现以及Pytorch的其他项目。"></a>1）<a href="https://github.com/bharathgs/Awesome-pytorch-list">Awesome-pytorch-list</a>：目前已获12K Star，包含了NLP,CV,常见库，论文实现以及Pytorch的其他项目。</h3><h3 id="2）PyTorch官方文档：官方发布的文档，十分丰富。"><a href="#2）PyTorch官方文档：官方发布的文档，十分丰富。" class="headerlink" title="2）PyTorch官方文档：官方发布的文档，十分丰富。"></a>2）<a href="https://pytorch.org/docs/stable/index.html">PyTorch官方文档</a>：官方发布的文档，十分丰富。</h3><h3 id="3）Pytorch-handbook：GitHub上已经收获14-8K，pytorch手中书。"><a href="#3）Pytorch-handbook：GitHub上已经收获14-8K，pytorch手中书。" class="headerlink" title="3）Pytorch-handbook：GitHub上已经收获14.8K，pytorch手中书。"></a>3）<a href="https://github.com/zergtant/pytorch-handbook">Pytorch-handbook</a>：GitHub上已经收获14.8K，pytorch手中书。</h3><h3 id="4）PyTorch官方社区：PyTorch拥有一个活跃的社区，在这里你可以和开发pytorch的人们进行交流。"><a href="#4）PyTorch官方社区：PyTorch拥有一个活跃的社区，在这里你可以和开发pytorch的人们进行交流。" class="headerlink" title="4）PyTorch官方社区：PyTorch拥有一个活跃的社区，在这里你可以和开发pytorch的人们进行交流。"></a>4）<a href="https://discuss.pytorch.org/">PyTorch官方社区</a>：PyTorch拥有一个活跃的社区，在这里你可以和开发pytorch的人们进行交流。</h3><h3 id="5）PyTorch官方tutorials：官方编写的tutorials，可以结合colab边动手边学习"><a href="#5）PyTorch官方tutorials：官方编写的tutorials，可以结合colab边动手边学习" class="headerlink" title="5）PyTorch官方tutorials：官方编写的tutorials，可以结合colab边动手边学习"></a>5）<a href="https://pytorch.org/tutorials/">PyTorch官方tutorials</a>：官方编写的tutorials，可以结合colab边动手边学习</h3><h3 id="6）动手学深度学习：动手学深度学习是由李沐老师主讲的一门深度学习入门课，拥有成熟的书籍资源和课程资源，在B站，Youtube均有回放。"><a href="#6）动手学深度学习：动手学深度学习是由李沐老师主讲的一门深度学习入门课，拥有成熟的书籍资源和课程资源，在B站，Youtube均有回放。" class="headerlink" title="6）动手学深度学习：动手学深度学习是由李沐老师主讲的一门深度学习入门课，拥有成熟的书籍资源和课程资源，在B站，Youtube均有回放。"></a>6）<a href="https://zh.d2l.ai/">动手学深度学习</a>：动手学深度学习是由李沐老师主讲的一门深度学习入门课，拥有成熟的书籍资源和课程资源，在B站，Youtube均有回放。</h3><h3 id="7）Awesome-PyTorch-Chinese：常见的中文优质PyTorch资源。"><a href="#7）Awesome-PyTorch-Chinese：常见的中文优质PyTorch资源。" class="headerlink" title="7）Awesome-PyTorch-Chinese：常见的中文优质PyTorch资源。"></a>7）<a href="https://github.com/INTERMT/Awesome-PyTorch-Chinese">Awesome-PyTorch-Chinese</a>：常见的中文优质PyTorch资源。</h3><h2 id="4-基础知识"><a href="#4-基础知识" class="headerlink" title="4.基础知识"></a>4.基础知识</h2><h3 id="4-1-张量"><a href="#4-1-张量" class="headerlink" title="4.1 张量"></a>4.1 张量</h3><p>张量是基于向量和矩阵的推广，核心是一个数据容器，多数情况下只包含数字。</p><div class="table-container"><table><thead><tr><th>张量维度</th><th style="text-align:left">代表含义</th></tr></thead><tbody><tr><td>0维张量</td><td style="text-align:left">代表的是标量（数字）</td></tr><tr><td>1维张量</td><td style="text-align:left">代表的是向量</td></tr><tr><td>2维张量</td><td style="text-align:left">代表的是矩阵</td></tr><tr><td>3维张量</td><td style="text-align:left">时间序列数据 股价 文本数据 单张彩色图片(<strong>RGB</strong>)</td></tr><tr><td>4维张量</td><td style="text-align:left">图像集合</td></tr><tr><td>5维张量</td><td style="text-align:left">视频</td></tr></tbody></table></div><h3 id="4-2-Tensor"><a href="#4-2-Tensor" class="headerlink" title="4.2 Tensor"></a>4.2 Tensor</h3><p> <code>torch.Tensor</code> 是存储和变换数据的主要工具，功能类似NumPy的多维数组。</p><p>1）创建tensor</p><p>①随机初始化矩阵</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">x = torch.rand(4, 3) </span><br><span class="line">print(x)</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tensor([[0.7569, 0.4281, 0.4722],</span><br><span class="line">        [0.9513, 0.5168, 0.1659],</span><br><span class="line">        [0.4493, 0.2846, 0.4363],</span><br><span class="line">        [0.5043, 0.9637, 0.1469]])</span><br></pre></td></tr></table></figure><p>②全零矩阵</p><p>torch.zeros()构建</p><p>torch.zero_()转换</p><p>torch.zeros_like()转换</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">x = torch.zeros(4, 3, dtype=torch.long)</span><br><span class="line">print(x)</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tensor([[0, 0, 0],</span><br><span class="line">        [0, 0, 0],</span><br><span class="line">        [0, 0, 0],</span><br><span class="line">        [0, 0, 0]])</span><br></pre></td></tr></table></figure><p>③ 张量的构建</p><p>1）使用数据直接构建</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">x = torch.tensor([5.5, 3]) </span><br><span class="line">print(x)</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([5.5000, 3.0000])</span><br></pre></td></tr></table></figure><p>2）基于已经存在的 tensor，创建一个 tensor </p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">x = x.new_ones(4, 3, dtype=torch.double) </span><br><span class="line"># 创建一个新的全1矩阵tensor，返回的tensor默认具有相同的torch.dtype和torch.device</span><br><span class="line"># 也可以像之前的写法 x = torch.ones(4, 3, dtype=torch.double)</span><br><span class="line">print(x)</span><br><span class="line"></span><br><span class="line">x = torch.randn_like(x, dtype=torch.float)</span><br><span class="line"># 重置数据类型</span><br><span class="line">print(x)</span><br><span class="line"># 结果会有一样的size</span><br><span class="line"># 获取它的维度信息</span><br><span class="line"></span><br><span class="line">print(x.size())</span><br><span class="line">print(x.shape)</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">tensor([[1., 1., 1.],</span><br><span class="line">        [1., 1., 1.],</span><br><span class="line">        [1., 1., 1.],</span><br><span class="line">        [1., 1., 1.]], dtype=torch.float64)</span><br><span class="line">        </span><br><span class="line">tensor([[ 2.7311, -0.0720,  0.2497],</span><br><span class="line">        [-2.3141,  0.0666, -0.5934],</span><br><span class="line">        [ 1.5253,  1.0336,  1.3859],</span><br><span class="line">        [ 1.3806, -0.6965, -1.2255]])</span><br><span class="line">        </span><br><span class="line">torch.Size([4, 3])</span><br><span class="line">torch.Size([4, 3])</span><br><span class="line">#返回的torch.Size其实是一个tuple，⽀持所有tuple的操作。我们可以使用索引操作取得张量的长、宽等数据维度。</span><br></pre></td></tr></table></figure><div class="table-container"><table><thead><tr><th style="text-align:right">函数</th><th>功能</th></tr></thead><tbody><tr><td style="text-align:right">Tensor(sizes)</td><td>基础构造函数</td></tr><tr><td style="text-align:right">tensor(data)</td><td>类似于np.array</td></tr><tr><td style="text-align:right">ones(sizes)</td><td>全1</td></tr><tr><td style="text-align:right">zeros(sizes)</td><td>全0</td></tr><tr><td style="text-align:right">eye(sizes)</td><td>对角为1，其余为0</td></tr><tr><td style="text-align:right">arange(s,e,step)</td><td>从s到e，步长为step</td></tr><tr><td style="text-align:right">linspace(s,e,steps)</td><td>从s到e，均匀分成step份</td></tr><tr><td style="text-align:right">rand/randn(sizes)</td><td>rand是[0,1)均匀分布；randn是服从N(0，1)的正态分布</td></tr><tr><td style="text-align:right">normal(mean,std)</td><td>正态分布(均值为mean，标准差是std)</td></tr><tr><td style="text-align:right">randperm(m)</td><td>随机排列</td></tr></tbody></table></div><p>④张量的操作</p><p>1）加法</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line"># 方式1</span><br><span class="line">y = torch.rand(4, 3) </span><br><span class="line">print(x + y)</span><br><span class="line"></span><br><span class="line"># 方式2</span><br><span class="line">print(torch.add(x, y))</span><br><span class="line"></span><br><span class="line"># 方式3 in-place，原值修改</span><br><span class="line">y.add_(x) </span><br><span class="line">print(y)</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">tensor([[ 2.8977,  0.6581,  0.5856],</span><br><span class="line">        [-1.3604,  0.1656, -0.0823],</span><br><span class="line">        [ 2.1387,  1.7959,  1.5275],</span><br><span class="line">        [ 2.2427, -0.3100, -0.4826]])</span><br><span class="line">tensor([[ 2.8977,  0.6581,  0.5856],</span><br><span class="line">        [-1.3604,  0.1656, -0.0823],</span><br><span class="line">        [ 2.1387,  1.7959,  1.5275],</span><br><span class="line">        [ 2.2427, -0.3100, -0.4826]])</span><br><span class="line">tensor([[ 2.8977,  0.6581,  0.5856],</span><br><span class="line">        [-1.3604,  0.1656, -0.0823],</span><br><span class="line">        [ 2.1387,  1.7959,  1.5275],</span><br><span class="line">        [ 2.2427, -0.3100, -0.4826]])</span><br></pre></td></tr></table></figure><p>2）索引</p><p>结果与原数据共享内存，使用copy()则不会修改源数据。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">x = torch.rand(4,3)</span><br><span class="line"># 取第二列</span><br><span class="line">print(x[:, 1]) </span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([-0.0720,  0.0666,  1.0336, -0.6965])</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">y = x[0,:]</span><br><span class="line">y += 1</span><br><span class="line">print(y)</span><br><span class="line">print(x[0, :]) # 源tensor也被改了了</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([3.7311, 0.9280, 1.2497])</span><br><span class="line">tensor([3.7311, 0.9280, 1.2497])</span><br></pre></td></tr></table></figure><p>3）维度变换</p><p><em>*</em>torch.view()   共享内存，可理解为view()仅仅是改变了对这个张量的观察角度。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(4, 4)</span><br><span class="line">y = x.view(16)</span><br><span class="line">z = x.view(-1, 8) # -1是指这一维的维数由其他维度决定</span><br><span class="line">print(x.size(), y.size(), z.size())</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])</span><br></pre></td></tr></table></figure><p><em>*</em>torch.reshape()  不推荐</p><p><em>*</em>推荐方案：先用 <code>clone()</code> 创造一个张量副本然后再使用 <code>torch.view()</code>进行函数维度变换。</p><p>我们可以使用.item()来获得元素tensor的value，而不获得其他性质。</p><p>⑤广播机制</p><p>形状不同的 Tensor 按元素运算时，可能会触发 广播机制 ：先复制元素使这两个 Tensor 形状相同，后再按元素运算。</p><p>5.自动求导</p><p>autograd包为张量上的所有操作提供自动求导机制。它是在运行时定义的框架，这意味着反向传播是根据代码运行决定，并且每次迭代可以不同。</p><p><code>torch.Tensor</code>是这个包的核心类。如果设置它的属性<code>.requires_grad</code> 为 <code>True</code>，那么它将会追踪对于该张量的所有操作。当完成计算后可以通过调用<code>.backward()</code>，来自动计算所有的梯度。这个张量的所有梯度将会自动累加到<code>.grad</code>属性。在 y.backward() 时，如果 y 是标量，则不需要为 backward() 传入任何参数；否则，需要传入一个与 y 同形的Tensor。</p><p>防止跟踪：调用<code>.detach()</code>方法；将代码块包装在 <code>with torch.no_grad():</code>中。</p><p>6.并行计算</p><p>遇到数据量较大或者需要提升计算速度的场景时需要 并行计算。</p><p>并行计算可充分利用GPU的性能，能提高效率。</p><p><code>CUDA</code>是NVIDIA提供的GPU并行计算框架，我们可使用.cuda()，让我们的模型或者数据从CPU迁移到GPU(0)当中，通过GPU开始计算。</p><p>并行方法：</p><p>网络结构分布到不同的设备中(Network partitioning)——GPU的通信在密集任务中困难，此方式正被遗弃；</p><p>同一层的任务分布到不同数据中(Layer-wise partitioning)——同步任务加重时会出现第一种方式同样的问题；</p><p>不同的数据分布到不同的设备中，执行相同的任务(Data parallelism)——拆分数据，主流方式。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>吃瓜教程-P5</title>
      <link href="/posts/4e2c74be.html"/>
      <url>/posts/4e2c74be.html</url>
      
        <content type="html"><![CDATA[<span class='p center logo large'>第6章 支持向量机</span><h1 id="1-前言"><a href="#1-前言" class="headerlink" title="1.前言"></a>1.前言</h1><p><img src="https://s3.bmp.ovh/imgs/2022/10/26/db8b489b31508f85.png" alt="1"></p><h3 id="如上图所示，我们希望找到一个边界使两类数据分开，显然可以找到许多符合条件的边界，但对于看不见的点或者验证数据集，就不一定能很好地分隔两类。支持向量机（SVM）就是为了寻找最佳的决策边界，既能将两类很好的分隔开来，而且还保持了两个类的最极端点之间的最宽距离。"><a href="#如上图所示，我们希望找到一个边界使两类数据分开，显然可以找到许多符合条件的边界，但对于看不见的点或者验证数据集，就不一定能很好地分隔两类。支持向量机（SVM）就是为了寻找最佳的决策边界，既能将两类很好的分隔开来，而且还保持了两个类的最极端点之间的最宽距离。" class="headerlink" title="如上图所示，我们希望找到一个边界使两类数据分开，显然可以找到许多符合条件的边界，但对于看不见的点或者验证数据集，就不一定能很好地分隔两类。支持向量机（SVM）就是为了寻找最佳的决策边界，既能将两类很好的分隔开来，而且还保持了两个类的最极端点之间的最宽距离。"></a>如上图所示，我们希望找到一个边界使两类数据分开，显然可以找到许多符合条件的边界，但对于看不见的点或者验证数据集，就不一定能很好地分隔两类。<strong>支持向量机（SVM）</strong>就是为了寻找最佳的决策边界，既能将两类很好的分隔开来，而且还保持了两个类的最极端点之间的最宽距离。</h3><h3 id="挖坑：如果无法找到那个边界呢？"><a href="#挖坑：如果无法找到那个边界呢？" class="headerlink" title="挖坑：如果无法找到那个边界呢？"></a>挖坑：如果无法找到那个边界呢？</h3><p><img src="https://s3.bmp.ovh/imgs/2022/10/26/2a78cba5cb3f3ed4.png" alt="2"></p><h1 id="2-什么是支持向量机"><a href="#2-什么是支持向量机" class="headerlink" title="2.什么是支持向量机"></a>2.什么是支持向量机</h1><p><img src="https://s3.bmp.ovh/imgs/2022/10/26/97acdede2d025252.png" alt="3"></p><h3 id="支持向量机（SVM）本质上是尝试拟合两个类别之间最宽的间距，使得图1中的两条虚线之间的距离最大，那么分布在虚线上的点就叫支持向量。也可以说，支持向量决定了虚线的位置，非支持向量，即图中不在虚线上的点不会影响决策边界的位置。"><a href="#支持向量机（SVM）本质上是尝试拟合两个类别之间最宽的间距，使得图1中的两条虚线之间的距离最大，那么分布在虚线上的点就叫支持向量。也可以说，支持向量决定了虚线的位置，非支持向量，即图中不在虚线上的点不会影响决策边界的位置。" class="headerlink" title="支持向量机（SVM）本质上是尝试拟合两个类别之间最宽的间距，使得图1中的两条虚线之间的距离最大，那么分布在虚线上的点就叫支持向量。也可以说，支持向量决定了虚线的位置，非支持向量，即图中不在虚线上的点不会影响决策边界的位置。"></a>支持向量机（SVM）本质上是尝试拟合两个类别之间最宽的间距，使得图1中的两条虚线之间的距离最大，那么分布在虚线上的点就叫<strong>支持向量</strong>。也可以说，支持向量决定了虚线的位置，非支持向量，即图中不在虚线上的点不会影响决策边界的位置。</h3><h3 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h3><p><img src="https://s3.bmp.ovh/imgs/2022/10/26/0a9a68565aa986ca.jpeg" alt="4"></p><h3 id="根据最大几何间隔选择最佳超平面"><a href="#根据最大几何间隔选择最佳超平面" class="headerlink" title="根据最大几何间隔选择最佳超平面"></a>根据最大几何间隔选择最佳超平面</h3><p><img src="https://s3.bmp.ovh/imgs/2022/10/26/6c9d9b222b8e2684.png" alt="5"></p><p><img src="https://s3.bmp.ovh/imgs/2022/10/26/eb6345a9c98fbc09.jpeg" alt="6"></p><h1 id="3-核函数"><a href="#3-核函数" class="headerlink" title="3.核函数"></a>3.核函数</h1><h3 id="填坑"><a href="#填坑" class="headerlink" title="填坑"></a>填坑</h3><p>在本章前面的讨论中，我们假设训练样本是线性可分的，即存在一个划分超平面能将训练样本正确分类.然而在现实任务中，原始样本空间内也许并不存在一个能正确划分两类样本的超平面。对这样的问题，可将样本从原始空间映射到一个<strong>更高维</strong>的特征空间，使得样本在这个特征空间内线性可分。如果原始空间是有限维，即属性数有限，那么一定存在一个高维特征空间使样本可分.</p><p><img src="https://s3.bmp.ovh/imgs/2022/10/26/9a85ab0534f35c48.png" alt="7"></p><h3 id="3-1直接计算"><a href="#3-1直接计算" class="headerlink" title="3.1直接计算"></a>3.1直接计算</h3><p><img src="https://s3.bmp.ovh/imgs/2022/10/26/d80a7c09c2ba88b3.png" alt="8"></p><h3 id="3-2通过设想函数避开直接计算"><a href="#3-2通过设想函数避开直接计算" class="headerlink" title="3.2通过设想函数避开直接计算"></a>3.2通过设想函数避开直接计算</h3><p><img src="https://s3.bmp.ovh/imgs/2022/10/26/721c0b1c7e5cd133.png" alt="9"></p><h3 id="3-3核函数"><a href="#3-3核函数" class="headerlink" title="3.3核函数"></a>3.3核函数</h3><p>上面的k(~,~)即为核函数。</p><p>若已知合适映射的具体形式，则可写出核函数，但在现实任务中我们通常不知道。</p><p>“核函数选择”成为支持向量机的最大变数，若核函数选择不合适，则意味着将样本映射到了一个不合适的特征空间，很可能导致性能不佳。</p><p><strong>常用核函数</strong></p><p><img src="https://s3.bmp.ovh/imgs/2022/10/26/f16bfff1f8f711bc.png" alt="10"></p><h1 id="4-软间隔"><a href="#4-软间隔" class="headerlink" title="4.软间隔"></a>4.软间隔</h1><p>对于<strong>线性不可分</strong>系统，无法找到合适的核函数。</p><p>缓解该问题的一个办法是允许支持向量机在一些样本上出错，即<strong>允许某些样本不满足约束</strong>。为此引入 “软间隔”(soft margin)的概念。</p><p><img src="https://s3.bmp.ovh/imgs/2022/10/26/554bccdc0b70b02e.png" alt="11"></p><p><img src="https://s3.bmp.ovh/imgs/2022/10/26/a1e368115c11a270.png" alt="12"></p><h1 id="5-支持向量回归"><a href="#5-支持向量回归" class="headerlink" title="5.支持向量回归"></a>5.支持向量回归</h1><p>支持向量回归(SVR)假设能容忍 f(x) 与 y 之间最多有ε的偏差，即仅当 f(x) 与 y 之间的差别绝对值大于ε时才计算损失。这相当于以 f(x) 为中心，构建了一个宽度为2ε的间隔带,若训练样本落入此间隔带，则认为被预测正确的。</p><p><img src="https://s3.bmp.ovh/imgs/2022/10/26/f4683d9bf2e53793.png" alt="13"></p><h1 id="6-核方法"><a href="#6-核方法" class="headerlink" title="6.核方法"></a>6.核方法</h1><p><img src="https://s3.bmp.ovh/imgs/2022/10/26/c309a59fde7f2a14.png" alt="14"></p><p>人们发展出一系列<strong>基于核函数</strong>的学习方法，统称为 “核方法”(kernel methods).</p><p>通过核化来对其进行非线性拓展，从而得到”核线性判别分析”(KLDA).</p><p><img src="https://s3.bmp.ovh/imgs/2022/10/26/9d142a80b88a725d.png" alt="15"></p><p>使用线性判别分析求解方法即可得到α ,进而可由式(6.64)得到投影函数h(x).</p>]]></content>
      
      
      
        <tags>
            
            <tag> 笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>吃瓜教程-P4</title>
      <link href="/posts/392b4428.html"/>
      <url>/posts/392b4428.html</url>
      
        <content type="html"><![CDATA[<p>第5章  神经网络</p><h1 id="5-1神经元模型"><a href="#5-1神经元模型" class="headerlink" title="5.1神经元模型"></a>5.1神经元模型</h1><h3 id="神经网络是由具有适应性的简单单元（神经元模型）组成的广泛并行互连的网络，它的组织能够模拟生物神经系统对真实世界物体所作出的交互反应。"><a href="#神经网络是由具有适应性的简单单元（神经元模型）组成的广泛并行互连的网络，它的组织能够模拟生物神经系统对真实世界物体所作出的交互反应。" class="headerlink" title="神经网络是由具有适应性的简单单元（神经元模型）组成的广泛并行互连的网络，它的组织能够模拟生物神经系统对真实世界物体所作出的交互反应。"></a>神经网络是由具有适应性的简单单元（神经元模型）组成的广泛并行互连的网络，它的组织能够模拟生物神经系统对真实世界物体所作出的交互反应。</h3><p>神经元接收到来自几个其他神经元传递过来的输入信号，这些输入信号通过带权重的连接(connection)进行传递，神经元接收到的总输入值将与神经元的阈值进行比较，然后通过“激活函数”处理以产生神经元的输出。</p><h2 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h2><h3 id="1）阶跃函数"><a href="#1）阶跃函数" class="headerlink" title="1）阶跃函数"></a>1）阶跃函数</h3><p>将输入值映射为输出值 “0” 或 “1”，不连续、不光滑</p><h3 id="2）Sigmoid函数"><a href="#2）Sigmoid函数" class="headerlink" title="2）Sigmoid函数"></a>2）Sigmoid函数</h3><p>把可能在较大范围内变化的输入值挤压到(0 ,1 )输出值范围内</p><p><img src="https://s3.bmp.ovh/imgs/2022/10/23/c520f292ffd431f6.png" alt="1"></p><p>把许多个这样的神经元按一定的层次结构连接起来，就得到了神经网络.</p><h1 id="5-2-感知机与多层网络"><a href="#5-2-感知机与多层网络" class="headerlink" title="5.2 感知机与多层网络"></a>5.2 感知机与多层网络</h1><h3 id="感知机"><a href="#感知机" class="headerlink" title="感知机"></a>感知机</h3><p>感知机(Perceptron)由两层神经元组成，输入层接收外界输入信号后传递给输出层，输出层是M-P神经元，亦称“阈值逻辑单元”。</p><p><img src="https://s3.bmp.ovh/imgs/2022/10/23/02d73256161ffd86.png" alt="2"></p><p>将阈值看做“哑节点”所对应的连接权重，权重和阈值的学习就可统一为权重的学习。</p><p>只拥有一层功能神经元,其学习能力非常有限.</p><p>若两类模式是线性可分的，即存在一个线性超平面能将它们分开，则感知机的学习过程一定会收敛(converge)。</p><h3 id="多层网络"><a href="#多层网络" class="headerlink" title="多层网络"></a>多层网络</h3><p>解决非线性问题</p><p>输出层与输入层之间的一层神经元，被称为隐层或隐含层(hidden layer),隐含层和输出层神经元都是拥有激活函数的功能神经元.</p><p>每层神经元与下一层神经元全互连，神经元之间不存在同层连接，也不存在跨层连接.这样的 神经网络结构通常称为“多层前馈神经网络” </p><p><img src="https://s3.bmp.ovh/imgs/2022/10/23/de2437e68b8e2581.png" alt="3"></p><p>神经网络的学习过程，就是根据训练数据来调整神经元之间的“连接权”(connection weight)以及每个功能神经元的阈值。</p><h1 id="5-3-误差逆传播算法"><a href="#5-3-误差逆传播算法" class="headerlink" title="5.3  误差逆传播算法"></a>5.3  误差逆传播算法</h1><p>又称反向传播算法、BP</p><p>BP 算法基于梯度下降策略，以目标的负梯度方向对参数进行调整。</p><p><img src="https://s3.bmp.ovh/imgs/2022/10/23/cd74de2a370906e1.png" alt="4"></p><p>学习率控制着算法每一轮迭代中的更新步长，若太大则容易振荡，太小则收敛速度又会过慢.</p><p><img src="https://s3.bmp.ovh/imgs/2022/10/23/36aca735839c2da6.png" alt="5"></p><h1 id="5-4-全局最小与局部极小"><a href="#5-4-全局最小与局部极小" class="headerlink" title="5.4  全局最小与局部极小"></a>5.4  全局最小与局部极小</h1><p><img src="https://s3.bmp.ovh/imgs/2022/10/23/93753981ff6350fa.png" alt="6"></p><p>由上图可知二者概念。</p><p>局部最小即此点误差函数值小于周围点的误差函数值，全局最小即此点误差函数值小于参数空间所有点的误差函数值。</p><p>从某些初始解出发,迭代寻找最优参数值.</p><h3 id="如何跳出局部最小？"><a href="#如何跳出局部最小？" class="headerlink" title="如何跳出局部最小？"></a>如何跳出局部最小？</h3><p>1）以多组不同参数值初始化多个神经网络，按标准方法训练后，取其中误差 最小的解作为最终参数；</p><p>2）“模拟退火”，在每一步都以一定的概率接受比当前解更差的结果；</p><p>3）随机梯度下降，即便陷入局部极小点，它计算出的梯度仍可能不为零。</p><h1 id="5-5-其他常见神经网络"><a href="#5-5-其他常见神经网络" class="headerlink" title="5.5  其他常见神经网络"></a>5.5  其他常见神经网络</h1><p>5.5.1 RBF 网络</p><p>5.5.2 ART网络</p><p>5.5.3 SOM网络</p><p>5.5.4级联相关网络</p><p>5.5.5 Elman网络</p><p>5.5.6 Boltzmann机</p><p>。。。</p><h1 id="5-6-深度学习"><a href="#5-6-深度学习" class="headerlink" title="5.6 深度学习"></a>5.6 深度学习</h1><h3 id="以-“深度学习”-deep-learning-为代表的复杂模型，能完成更复杂的学习任务，缓解训练低效性，训练数据的大幅增加则可降低过拟合风险。"><a href="#以-“深度学习”-deep-learning-为代表的复杂模型，能完成更复杂的学习任务，缓解训练低效性，训练数据的大幅增加则可降低过拟合风险。" class="headerlink" title="以 “深度学习”(deep learning)为代表的复杂模型，能完成更复杂的学习任务，缓解训练低效性，训练数据的大幅增加则可降低过拟合风险。"></a>以 “深度学习”(deep learning)为代表的复杂模型，能完成更复杂的学习任务，缓解训练低效性，训练数据的大幅增加则可降低过拟合风险。</h3><p>增加隐层的数目显然比增加隐层神经元的数目更有效。</p><h3 id="“预训练-微调”"><a href="#“预训练-微调”" class="headerlink" title="“预训练+ 微调”"></a>“预训练+ 微调”</h3><p>将大量参数分组,对每组先找到局 部看来比较好的设置，然后再基于这些局部较优的结果联合起来进行全局寻优.</p><h3 id="“权共享”"><a href="#“权共享”" class="headerlink" title="“权共享”"></a>“权共享”</h3><p>相同的连接权</p><p>对输入信号进行 逐层加工，从而把初始的、与输出目标之间联系不太密切的输入表示，转化 成与输出目标联系更密切的表示，使得原来仅基于最后一层输出映射难以完 成的任务成为可能.</p><p>描述样本的特征通常需由人类专家来设 计，这称为“特征工程”。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>吃瓜教程-P3</title>
      <link href="/posts/a74fd18b.html"/>
      <url>/posts/a74fd18b.html</url>
      
        <content type="html"><![CDATA[<h1 id="《机器学习》第四章-决策树"><a href="#《机器学习》第四章-决策树" class="headerlink" title="《机器学习》第四章 决策树"></a><center>《机器学习》第四章 决策树</center></h1><h1 id="4-1-基本流程"><a href="#4-1-基本流程" class="headerlink" title="4.1 基本流程"></a>4.1 基本流程</h1><h3 id="决策树-decision-tree-是一类常见的机器学习方法。基于树结构来进行决策，进行一系列的判断或“子决策”。"><a href="#决策树-decision-tree-是一类常见的机器学习方法。基于树结构来进行决策，进行一系列的判断或“子决策”。" class="headerlink" title="决策树(decision tree)是一类常见的机器学习方法。基于树结构来进行决策，进行一系列的判断或“子决策”。"></a>决策树(decision tree)是一类常见的机器学习方法。基于树结构来进行决策，进行一系列的判断或“子决策”。</h3><p><img src="https://s3.bmp.ovh/imgs/2022/10/20/158926c6979f66f2.png" alt="1"></p><h3 id="一般的，一棵决策树包含一个根结点、若干个内部结点和若干个叶结点-叶结点对应于决策结果，其他每个结点则对应于一个属性测试-每个结点包含的样本集合根据属性测试的结果被划分到子结点中-根结点包含样本全集。"><a href="#一般的，一棵决策树包含一个根结点、若干个内部结点和若干个叶结点-叶结点对应于决策结果，其他每个结点则对应于一个属性测试-每个结点包含的样本集合根据属性测试的结果被划分到子结点中-根结点包含样本全集。" class="headerlink" title="一般的，一棵决策树包含一个根结点、若干个内部结点和若干个叶结点;叶结点对应于决策结果，其他每个结点则对应于一个属性测试;每个结点包含的样本集合根据属性测试的结果被划分到子结点中;根结点包含样本全集。"></a>一般的，一棵决策树包含一个根结点、若干个内部结点和若干个叶结点;叶结点对应于决策结果，其他每个结点则对应于一个属性测试;每个结点包含的样本集合根据属性测试的结果被划分到子结点中;根结点包含样本全集。</h3><h3 id="决策树学习的目的是为了产生一棵泛化能力强，即处理未见示例能力强的决策树。"><a href="#决策树学习的目的是为了产生一棵泛化能力强，即处理未见示例能力强的决策树。" class="headerlink" title="决策树学习的目的是为了产生一棵泛化能力强，即处理未见示例能力强的决策树。"></a>决策树学习的目的是为了产生一棵泛化能力强，即处理未见示例能力强的决策树。</h3><p>基本流程：</p><p><img src="https://s3.bmp.ovh/imgs/2022/10/20/46252ad10e6dfd33.png" alt="2"></p><h3 id="区别：情形-2-是在利用当前结点的后验分布，而情形-3-则是把父结点的样本分布-作为当前结点的先验分布"><a href="#区别：情形-2-是在利用当前结点的后验分布，而情形-3-则是把父结点的样本分布-作为当前结点的先验分布" class="headerlink" title="区别：情形(2)是在利用当前结点的后验分布，而情形(3)则是把父结点的样本分布 作为当前结点的先验分布."></a>区别：情形(2)是在利用当前结点的后验分布，而情形(3)则是把父结点的样本分布 作为当前结点的先验分布.</h3><h1 id="4-2-划分选择"><a href="#4-2-划分选择" class="headerlink" title="4.2 划分选择"></a>4.2 划分选择</h1><h3 id="决策树学习的关键是选择最优划分属性"><a href="#决策树学习的关键是选择最优划分属性" class="headerlink" title="决策树学习的关键是选择最优划分属性."></a>决策树学习的关键是选择最优划分属性.</h3><h3 id="一般而言，随着划分过程不断进行，我们希望决策树的分支结点所包含的样-本尽可能属于同一类别，即结点的“纯度”-purity-越来越高"><a href="#一般而言，随着划分过程不断进行，我们希望决策树的分支结点所包含的样-本尽可能属于同一类别，即结点的“纯度”-purity-越来越高" class="headerlink" title="一般而言，随着划分过程不断进行，我们希望决策树的分支结点所包含的样 本尽可能属于同一类别，即结点的“纯度”(purity)越来越高."></a>一般而言，随着划分过程不断进行，我们希望决策树的分支结点所包含的样 本尽可能属于同一类别，即结点的“纯度”(purity)越来越高.</h3><h2 id="信息增益"><a href="#信息增益" class="headerlink" title="信息增益"></a>信息增益</h2><p><img src="https://s3.bmp.ovh/imgs/2022/10/20/ba7caa08b6bc228b.png" alt="3"></p><h2 id="增益率"><a href="#增益率" class="headerlink" title="增益率"></a>增益率</h2><p><img src="https://s3.bmp.ovh/imgs/2022/10/20/c025b4e33146b3ea.png" alt="4"></p><p>增益率准则对可取值数目较少的属性有所偏好，因 此 C4.5 算法并不是直接选择增益率最大的候选划分属性，而是使用了一个启发式：先从候选划分属性中找出信息增益高于平均水平的属性，再从中选择增益率最高的.</p><h2 id="基尼指数"><a href="#基尼指数" class="headerlink" title="基尼指数"></a>基尼指数</h2><p><img src="https://s3.bmp.ovh/imgs/2022/10/20/fb0568b488f38654.png" alt="5"></p><h2 id="4-3-剪枝处理"><a href="#4-3-剪枝处理" class="headerlink" title="4.3 剪枝处理"></a>4.3 剪枝处理</h2><h3 id="剪枝主要为了对付“过拟合”。"><a href="#剪枝主要为了对付“过拟合”。" class="headerlink" title="剪枝主要为了对付“过拟合”。"></a>剪枝主要为了对付“过拟合”。</h3><p>决策树分支过多，导致模型把训练集自身的一些特点当作所有数据都具有的一般性质而导致过拟合，因此可主动去掉一些分支来降低过拟合的风险.</p><h3 id="基本策略：1）-预剪枝-和-2）后剪枝"><a href="#基本策略：1）-预剪枝-和-2）后剪枝" class="headerlink" title="基本策略：1） 预剪枝 和 2）后剪枝"></a>基本策略：1） 预剪枝 和 2）后剪枝</h3><p>1）预剪枝</p><p>指在决策树生成过程中，对每个结点在划分前先进行估计，若当前结点的划分不能带来决策树泛化性能提升，则停止划分并将当前结点标记为叶结点;</p><p><img src="https://s3.bmp.ovh/imgs/2022/10/20/3b5e96d92f3bda0f.png" alt="6"></p><p>①降低了过拟合的风险</p><p>②减少了决策树的训练时间开销和测试时间开销.</p><p>③未将一些有“潜力”的分支展开，有欠拟合风险.</p><p>2）后剪枝</p><p>先从训练集生成一棵完整的决策树, 然后自底向上地对非叶结点进行考察，若将该结点对应的子树替换为叶结点能带来决策树泛化性能提升，则将该子树替换为叶结点.</p><p><img src="https://s3.bmp.ovh/imgs/2022/10/20/c48c208d20cbdecf.png" alt="7"></p><p>保留更多分支，欠拟合风险小，泛化性能优训练时间开销大.</p><h2 id="4-4-连续与缺失值"><a href="#4-4-连续与缺失值" class="headerlink" title="4.4 连续与缺失值"></a>4.4 连续与缺失值</h2><p>连续属性离散化技术实现在决策树学习中使用连续属性</p><h3 id="二分法"><a href="#二分法" class="headerlink" title="二分法"></a>二分法</h3><p><img src="https://s3.bmp.ovh/imgs/2022/10/20/cd665a38407ecf4d.png" alt="8"></p><p>与离散属性不同，若当前结点划分属性为连续属性，该属性还可作为其后代结点的划分属性.</p><h3 id="缺失值处理"><a href="#缺失值处理" class="headerlink" title="缺失值处理"></a>缺失值处理</h3><p><img src="https://s3.bmp.ovh/imgs/2022/10/20/bb39c0b30b17ce90.png" alt="9"></p><h2 id="4-5-多变量决策树"><a href="#4-5-多变量决策树" class="headerlink" title="4.5 多变量决策树"></a>4.5 多变量决策树</h2><p>把每个属性视为坐标空间中的一个坐标轴，则 d 个属性描述的样本 就对应了 d 维空间中的一个数据点，对样本分类则意味着在这个坐标空间中寻 找不同类样本之间的分类边界.</p><p>在分类边界比较复杂时，单分类边界使得决策树相当复杂，“多变量决策树”使其形成斜的边界，使模型大大简化。非叶结点不再是仅对某个属性,而是对属性的线性组合进行测试。</p><p><img src="https://s3.bmp.ovh/imgs/2022/10/20/a1e3e4e98b42c27e.png" alt="10"></p>]]></content>
      
      
      
        <tags>
            
            <tag> 笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>吃瓜教程-P2</title>
      <link href="/posts/d048e11d.html"/>
      <url>/posts/d048e11d.html</url>
      
        <content type="html"><![CDATA[<h1 id="《机器学习》第三章-线性模型"><a href="#《机器学习》第三章-线性模型" class="headerlink" title="《机器学习》第三章 线性模型"></a><center>《机器学习》第三章 线性模型</center></h1><h1 id="3-1基本形式"><a href="#3-1基本形式" class="headerlink" title="3.1基本形式"></a>3.1基本形式</h1><h2 id="f-x-w1x1-w2x2-…-wdxd-b"><a href="#f-x-w1x1-w2x2-…-wdxd-b" class="headerlink" title="f(x)= w1x1+w2x2+…+wdxd+b"></a>f(x)= w1x1+w2x2+…+wdxd+b</h2><h2 id="向量形式-f-x-w-T-x-b"><a href="#向量形式-f-x-w-T-x-b" class="headerlink" title="向量形式: f(x) = w T x + b"></a>向量形式: f(x) = w T x + b</h2><p>其中x表示由d个属性描述的示例x=(x1;x2;…;xd) ,许多非线性模型可在线性模型的基础上通过引入层级结构或高维映射而得。<br>线性模型形式简单、易于建模，有很好的可解释性。</p><h1 id="3-2线性回归"><a href="#3-2线性回归" class="headerlink" title="3.2线性回归"></a>3.2线性回归</h1><h2 id="“线性回归”试图学得一个线性模型以尽可能准确地预测实值输出标记。"><a href="#“线性回归”试图学得一个线性模型以尽可能准确地预测实值输出标记。" class="headerlink" title="“线性回归”试图学得一个线性模型以尽可能准确地预测实值输出标记。"></a>“线性回归”试图学得一个线性模型以尽可能准确地预测实值输出标记。</h2><p>① 若属性值间存在“序”(order)关系，可通过连续化将其转化为连续值，例如三值属性“高度”的取值“高”“中”“低”可转化为{1.0,0.5,0.0}。<br>② 若属性值间不存在序关系，假定有k个属性值，则通常转化为k维向量,例如属性“瓜类”的取值“西瓜” “南瓜” “黄瓜”可转化为(0,0,1), (0,1,0),(1,0,0)。若将无序属性连续化, 则会不恰当地引入序关系, 对后续处理如距离计算等造成误导。</p><h2 id="当样本由单个属性描述时："><a href="#当样本由单个属性描述时：" class="headerlink" title="当样本由单个属性描述时："></a>当样本由单个属性描述时：</h2><h3 id="线性回归试图学得f-xi-wxi-b使得f-xi-→yi-。"><a href="#线性回归试图学得f-xi-wxi-b使得f-xi-→yi-。" class="headerlink" title="线性回归试图学得f(xi)= wxi+b使得f(xi)→yi 。"></a>线性回归试图学得f(xi)= wxi+b使得f(xi)→yi 。</h3><h3 id="如何求w和b？"><a href="#如何求w和b？" class="headerlink" title="如何求w和b？"></a>如何求w和b？</h3><h3 id="最小二乘法——基于均方误差最小化"><a href="#最小二乘法——基于均方误差最小化" class="headerlink" title="最小二乘法——基于均方误差最小化"></a>最小二乘法——基于均方误差最小化</h3><p>试图找到一条直线，使所有样本到直线上的欧氏距离之和最小。</p><p><img src="https://z4a.net/images/2022/10/27/21.jpg" alt="1"></p><h2 id="当样本由多个属性描述时："><a href="#当样本由多个属性描述时：" class="headerlink" title="当样本由多个属性描述时："></a>当样本由多个属性描述时：</h2><h3 id="线性回归试图学得f-xi-w-Txi-b使得f-xi-→yi-称多元线性回归。"><a href="#线性回归试图学得f-xi-w-Txi-b使得f-xi-→yi-称多元线性回归。" class="headerlink" title="线性回归试图学得f(xi)= w^Txi+b使得f(xi)→yi ,称多元线性回归。"></a>线性回归试图学得f(xi)= w^Txi+b使得f(xi)→yi ,称多元线性回归。</h3><p><img src="https://img1.imgtp.com/2022/10/17/8nclYkMl.png" alt="2"></p><h3 id="①当-X-T-X-为满秩矩阵或正定矩阵"><a href="#①当-X-T-X-为满秩矩阵或正定矩阵" class="headerlink" title="①当 X T X 为满秩矩阵或正定矩阵"></a>①当 X T X 为满秩矩阵或正定矩阵</h3><p><img src="https://img1.imgtp.com/2022/10/17/q1B9vkWH.png" alt="3"></p><h3 id="②不满秩时"><a href="#②不满秩时" class="headerlink" title="②不满秩时"></a>②不满秩时</h3><p>此时可解出多个w^，它们都能使均方误差最小化，如何选择解作为输出？</p><h3 id="引入正则化项"><a href="#引入正则化项" class="headerlink" title="引入正则化项"></a>引入正则化项</h3><p><img src="https://img1.imgtp.com/2022/10/17/SrMQqCJU.png" alt="4"></p><h3 id="③一般情况"><a href="#③一般情况" class="headerlink" title="③一般情况"></a>③一般情况</h3><p><img src="https://img1.imgtp.com/2022/10/17/Exq7rHdf.png" alt="5"></p><h1 id="3-3-对数几率回归"><a href="#3-3-对数几率回归" class="headerlink" title="3.3 对数几率回归"></a>3.3 对数几率回归</h1><h3 id="若进行分类任务时怎么办？→找一个单调可微函数将分类任务的真实标记y与线性回归模型的预测值联系起来"><a href="#若进行分类任务时怎么办？→找一个单调可微函数将分类任务的真实标记y与线性回归模型的预测值联系起来" class="headerlink" title="若进行分类任务时怎么办？→找一个单调可微函数将分类任务的真实标记y与线性回归模型的预测值联系起来."></a>若进行分类任务时怎么办？→找一个单调可微函数将分类任务的真实标记y与线性回归模型的预测值联系起来.</h3><p>二分类任务时，我们的输出标记为{0,1}，而线性回归模型产生的预测值是实值，需 将实值z转换为 0 / 1 值，此时考虑“单位阶跃函数”。</p><p><img src="https://z4a.net/images/2022/10/27/26.jpg" alt="6"></p><p>新问题：单位阶跃函数不连续，不能直接用作式g^-1( • ) ,对数几率函数为替代函数。</p><p><img src="https://z4a.net/images/2022/10/27/27.jpg" alt="7"></p><p>“对数几率回归”： 用线性回归模型的预测结果去逼近真实标记的对数几率。</p><h1 id="3-4-线性判别分析（LDA）（Fisher判别分析）"><a href="#3-4-线性判别分析（LDA）（Fisher判别分析）" class="headerlink" title="3.4 线性判别分析（LDA）（Fisher判别分析）"></a>3.4 线性判别分析（LDA）（Fisher判别分析）</h1><p>LDA的思想非常朴素:给定训练样例集，设法将样例投影到一条直线上, 使得同类样例的投影点尽可能接近、异类样例的投影点尽可能远离;在对新样 本进行分类时，将其投影到同样的这条直线上，再根据投影点的位置来确定新样本的类别。</p><p><img src="https://img1.imgtp.com/2022/10/17/iwyqcN8d.png" alt="8"></p><p><img src="https://img1.imgtp.com/2022/10/17/jczxY1mo.png" alt="9"></p><h1 id="3-5-多分类学习"><a href="#3-5-多分类学习" class="headerlink" title="3.5 多分类学习"></a>3.5 多分类学习</h1><h3 id="·将多分类任务拆为若干个二分类任务求解"><a href="#·将多分类任务拆为若干个二分类任务求解" class="headerlink" title="·将多分类任务拆为若干个二分类任务求解."></a>·将多分类任务拆为若干个二分类任务求解.</h3><h3 id="·三种拆分策略：“一对一”-One-vs-One-简称OvO-、-“一对其余“-One-vs-Rest-简称-OvR-和”多对多”-Many-vs-Many-简称-MvM"><a href="#·三种拆分策略：“一对一”-One-vs-One-简称OvO-、-“一对其余“-One-vs-Rest-简称-OvR-和”多对多”-Many-vs-Many-简称-MvM" class="headerlink" title="·三种拆分策略：“一对一”(One vs. One,简称OvO)、 “一对其余“(One vs. Rest,简称 OvR)和”多对多” (Many vs. Many,简称 MvM)."></a>·三种拆分策略：“一对一”(One vs. One,简称OvO)、 “一对其余“(One vs. Rest,简称 OvR)和”多对多” (Many vs. Many,简称 MvM).</h3><h1 id="3-6-类别不平衡问题"><a href="#3-6-类别不平衡问题" class="headerlink" title="3.6 类别不平衡问题"></a>3.6 类别不平衡问题</h1><h3 id="类别不平衡-class-imbalance-就是指分类任务中不同类别的训练样例数目差别很大的情况。"><a href="#类别不平衡-class-imbalance-就是指分类任务中不同类别的训练样例数目差别很大的情况。" class="headerlink" title="类别不平衡(class-imbalance)就是指分类任务中不同类别的训练样例数目差别很大的情况。"></a>类别不平衡(class-imbalance)就是指分类任务中不同类别的训练样例数目差别很大的情况。</h3><h3 id="基本策略—-“再缩放”-："><a href="#基本策略—-“再缩放”-：" class="headerlink" title="基本策略— “再缩放” ："></a>基本策略— “再缩放” ：</h3><p><img src="https://z4a.net/images/2022/10/27/210.jpg" alt="10"></p><h3 id="再缩放的三类做法：欠采样、过采样、阈值移动。"><a href="#再缩放的三类做法：欠采样、过采样、阈值移动。" class="headerlink" title="再缩放的三类做法：欠采样、过采样、阈值移动。"></a>再缩放的三类做法：欠采样、过采样、阈值移动。</h3>]]></content>
      
      
      
        <tags>
            
            <tag> 笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>吃瓜教程-p1</title>
      <link href="/posts/dcc59405.html"/>
      <url>/posts/dcc59405.html</url>
      
        <content type="html"><![CDATA[<h1 id="第一章-绪论"><a href="#第一章-绪论" class="headerlink" title="第一章 绪论"></a>第一章 绪论</h1><h3 id="本章主要是对机器学习相关内容做了概述，通过西瓜外观判断瓜的质量的例子，将基本术语和假设空间的概念讲述清楚。"><a href="#本章主要是对机器学习相关内容做了概述，通过西瓜外观判断瓜的质量的例子，将基本术语和假设空间的概念讲述清楚。" class="headerlink" title="本章主要是对机器学习相关内容做了概述，通过西瓜外观判断瓜的质量的例子，将基本术语和假设空间的概念讲述清楚。"></a>本章主要是对机器学习相关内容做了概述，通过西瓜外观判断瓜的质量的例子，将基本术语和假设空间的概念讲述清楚。</h3><p>—————<br>数据集（关于西瓜数据的集合，例如【(色泽=青绿;根蒂=蜷缩;敲声=浊响)，(色泽=乌黑;根蒂= 稍蜷;敲声=沉闷)，(色泽=浅白;根蒂=硬挺;敲声=清脆)，……）】<br>⬇️<br>示例/样本（特征向量）（关于一 个事件或对象(这里是一个西瓜)的描述）<br>⬇️<br>属性/特征（反映事件或对象在某方面的表现或性质的事项，例如“色泽”<br>“根蒂” “敲 声 ）<br>⬇️<br>属性值（属性上的取值，例如“青绿”“乌黑”）<br>⬇️<br>属性空间/样本空间/输入空间（属性张成的空间，例如我们把“色泽” “根蒂” “敲声”作为三个坐标轴，则它们张成一个用于描述西瓜的三维空间，每个西瓜都可在这个空间中找到自己的坐标位置.由于空间中的每个点对应一个坐标向量，因此我们也把一个示例称为一个“特征向量”.）<br>—————<br>从数据中学得模型的过程称为“学习”(learning)或 “训练”(training), 这个过程通过执行某个学习算法来完成.训练过程中使用的数据称为“训练 数 据 “(training data), 其中每个样本称为一个“训练样本” (training sample), 训练样本组成的集合称为“训练集”(training set). 学得模型对应了关于数据 的某种潜在的规律，因此亦称“假设”(hypothesis); 这种潜在规律自身,则称 为 “ 真 相 ” 或 “ 真 实 ” , 学习过程就是为了找出或逼近真相 . 本书有时将模型称为 “ 学 习 器 ” , 可看作学习算法在给定数据和参数空间上的实例化 .<br>—————</p><h3 id="标记"><a href="#标记" class="headerlink" title="标记"></a>标记</h3><p>(色泽= 青绿;根蒂= 蜷缩; 敲声=浊响)，好瓜)，这里关于示例结果的信息，例 如 “好瓜”，称 为 “标 记”(label);拥有了标记信息的示例，则称为“样例”(example). 一般地，用（xi,hi）表示第i个样例，其中班G，是示例Xi的标记，J 是所有标记的集合, 亦称“标记空间”(label space)或“输出空间”.</p><h3 id="聚类"><a href="#聚类" class="headerlink" title="聚类"></a>聚类</h3><p>将训练集中的样本分为若干组（簇），这些自动形成的簇存在一些潜在的概念划分.<br>—————</p><h3 id="欲预测的是离散值———分类"><a href="#欲预测的是离散值———分类" class="headerlink" title="欲预测的是离散值———分类"></a>欲预测的是离散值———分类</h3><pre><code>    ###连续值———回归</code></pre><h3 id="训练数据有标记信息——-监督学习——分类、回归"><a href="#训练数据有标记信息——-监督学习——分类、回归" class="headerlink" title="训练数据有标记信息——-监督学习——分类、回归"></a>训练数据有标记信息——-监督学习——分类、回归</h3><pre><code>    ###无标记信息——无监督学习—聚类</code></pre><p>—————</p><p><img src="https://s3.bmp.ovh/imgs/2022/10/27/78d820df3b678f35.jpg" alt="1"></p><p>机器学习的目标是使学得的模型能很好地适用于“新样本 “ 而不是仅仅在训练样本上工作得很好;即便对聚类这样的无监督学习任务，我们也希望学得的簇划分能适用于没在训练集中出现的样本.学得模型适用于 新样本 的能力，称为 “泛化 “(generalization)能力.具有强泛化能力的模型能很好地适用于整个样本空间.<br>—————</p><h3 id="假设空间"><a href="#假设空间" class="headerlink" title="假设空间"></a>假设空间</h3><p>有多个假设与训练集一致时，即存在着一个与训练集一致的“假设集合”，我们称之为“版本空间”.<br>—————</p><h1 id="第二章-模型评估与选择"><a href="#第二章-模型评估与选择" class="headerlink" title="第二章 模型评估与选择"></a>第二章 模型评估与选择</h1><p><img src="https://s3.bmp.ovh/imgs/2022/10/27/207ce70184005f6d.jpg" alt="2"></p><p>—————<br>把训练样本自身的一些特点当作了所有潜在样本都 会具有的一般性质，这样就会导致泛化性能下降.这种现象在机器学习中称为“过拟合” (overfitting).<br>与 “过拟合”相对的是“欠拟合” (imderRtting),这 是指对训练样本的一般性质尚未学好.<br>—————</p><h3 id="评估方法"><a href="#评估方法" class="headerlink" title="评估方法"></a>评估方法</h3><p>需使用一个“测试集”(testing set)来测试学习器对新样本的判别能力，然后以测试集上的“测试误差”(testing error)作为泛化误差的近似.测试样本尽量不在训练集中出现、未在训练过程中使用过.<br>—————</p><h2 id="数据集的处理方法（当数据集有限但需要划分训练集T和测试集S时）"><a href="#数据集的处理方法（当数据集有限但需要划分训练集T和测试集S时）" class="headerlink" title="数据集的处理方法（当数据集有限但需要划分训练集T和测试集S时）"></a>数据集的处理方法（当数据集有限但需要划分训练集T和测试集S时）</h2><h3 id="1-留出法"><a href="#1-留出法" class="headerlink" title="1.留出法"></a>1.留出法</h3><p>直接将数据集划分为两个互斥的集合，其中一个 集合作为训练集S,另一个作为测试集T,即0 =SUT,SnT=0.在S上训 练出模型后，用 T 来评估其测试误差，作为对泛化误差的估计.<br>需要注意（1）训练/测试集的划分要尽可能保持数据分布的一致性，（2）使用留出法时，一般要采用若干次随机划分、重复进行实验评估后取平均值作 为留出法的评估结果。<br>此方法受训练集和测试集的划分比例影响较大。</p><h3 id="2-交叉验证法（k-fold）"><a href="#2-交叉验证法（k-fold）" class="headerlink" title="2.交叉验证法（k-fold）"></a>2.交叉验证法（k-fold）</h3><p>将数据集 分层采样 划分为k个 大小相似 的互斥子集，将其中一个子集作为测试集，其余作为训练集，如此进行n次后取平均。</p><p><img src="https://s3.bmp.ovh/imgs/2022/10/27/7d0fb214b1218984.jpg" alt="3"></p><p>—————</p><p>交叉验证法评估结果的稳定性和保真性在很大程度上取决于k 的取值。</p><p>与留出法相似，将数据集。划分为k 个子集同样存在多种划分方式.为 减小因样本划分不同而引入的差别，k 折交叉验证通常要随机使用不同的划分 重复p 次，最终的评估结果是这p 次 k 折交叉验证结果的均值。<br>特例：留一法（k=数据集长度m，样本只有唯一的方式划分为m个子集,每个子集包含一个样本;留一法使用的训练集与初始数据集相比只少了一个样本，这就使得 在绝大多数情况下，留一法中被实际评估的模型与期望评估的用D 训练出的模 型很相似.因此，留一法的评估结果往往被认为比较准确.缺陷:在数据集比较大时，训练M 个模型的计算开销难以忍受，且准确度不一定高）</p><h3 id="3-自助法"><a href="#3-自助法" class="headerlink" title="3.自助法"></a>3.自助法</h3><p>给定包含 m 个样 本的数据集。，我们对它进行采样产生数据集每次随机从。中挑选一个 样本,将其拷贝放入少 ,然后再将该样本放回初始数据集D 中,使得该样本在 下次采样时仍有可能被采到;这个过程重复执行m 次后，我们就得到了包含m 个样本的数据集D,这就是自助采样的结果.<br>估计，样本在m次采样中始终不被采到的概率取极限约为0.368,实际评估的模型与 期望评估的模型都使用馆个训练样本,而我们仍有数据总量约1 / 3 的、没在训 练集中出现的样本用于测试.这样的测试结果，亦 称 “包外估计”.<br>自助法在数据集较小、难以有效划分训练/测试集时很有用;此外，自助法 能从初始数据集中产生多个不同的训练集,这对集成学习等方法有很大的好处. 然而，自助法产生的数据集改变了初始数据集的分布，这会引入估计偏差.在初始数据量足够时，留出法和交叉验证法更常用一些.</p><h3 id="4-调参"><a href="#4-调参" class="headerlink" title="4.调参"></a>4.调参</h3><p>大多数学习算法都有些参数(parameter)需要设定，参数配置不同，学得模 型的性能往往有显著差别.因此,在进行模型评估与选择时，除了要对适用学习 算法进行选择，还需对算法参数进行设定，这就是通常所说的“参数调节”或 简 称 “调 参 “<br>参数调得好不好往往对最终模型性能有关键性影响.<br>—————</p><h2 id="性能度量"><a href="#性能度量" class="headerlink" title="性能度量"></a>性能度量</h2><p>衡量模型泛化能力的评价标准，这就是性能度量<br>性能度量反映了任务需求，在对比不同模型的能力时，使用不同的性能度量往 往会导致不同的评判结果;这意味着模型的“好坏”是相对的，什么样的模型 是好的，不仅取决于算法和数据，还决定于任务需求.</p><h3 id="1-错误率和精度：分类任务中最常用的两种性能度量-既适用于二分类任务，也适用于多分类任务"><a href="#1-错误率和精度：分类任务中最常用的两种性能度量-既适用于二分类任务，也适用于多分类任务" class="headerlink" title="1.错误率和精度：分类任务中最常用的两种性能度量, 既适用于二分类任务，也适用于多分类任务"></a>1.错误率和精度：分类任务中最常用的两种性能度量, 既适用于二分类任务，也适用于多分类任务</h3><p><img src="https://s3.bmp.ovh/imgs/2022/10/27/e184f060743b847a.jpg" alt="4"></p><h3 id="2-查准率、查全率与F1"><a href="#2-查准率、查全率与F1" class="headerlink" title="2.查准率、查全率与F1"></a>2.查准率、查全率与F1</h3><p><img src="https://s3.bmp.ovh/imgs/2022/10/27/326fe296d887b673.jpg" alt="5"></p><p>根据学习器的预测结果对样例进行排序，排在前面 的是学习器认为“最可能”是正例的样本，排在最后的则是学习器认为“最 不可能”是正例的样本.按此顺序逐个把样本作为正例进行预测，则每次可以计算出当前的查全率、查准率.以查准率为纵轴、查全率为横轴作图，就得到了查准率-查全率曲计算出当前的查全率、查准率.以查准率为纵轴、查全率为横轴作图，就得到了查准率-查全率曲线，简称“P-R曲线〃，显示该曲线的图称为“P-R图”.</p><p><img src="https://s3.bmp.ovh/imgs/2022/10/27/d5780e12a1040044.jpg" alt="6"></p><p>若一个学习器的P - R 曲线被另一个学习器的曲线完全“包住”，则可断言 后者的性能优于前者</p><h3 id="当出现交叉时，有两种性能度量："><a href="#当出现交叉时，有两种性能度量：" class="headerlink" title="当出现交叉时，有两种性能度量："></a>当出现交叉时，有两种性能度量：</h3><p>1.平衡点（BEP）它是“查准率 = 查全率，时的取值。<br>2.F1度量</p><p><img src="https://s3.bmp.ovh/imgs/2022/10/27/38a7a310baa5867a.jpg" alt="7"></p><p> 其中B &gt; 0度量了查全率对查准率的相对重要性[VanRijsbergen, 1979].<br> B = 1 时退化为标准的F1;^ &gt; 1时查全率有更大影响;B&lt; 1时查准率有更大影响.</p><p>很多时候我们有多个二分类混淆矩阵，例如进行多次训练/测试，每次得到 一个混淆矩阵;或是在多个数据集上进行训练/测试，希望估计算法的“全局” 性能;甚或是执行多分类任务，每两两类别的组合都对应一个混淆矩阵;•…・ 总之,我们希望在n 个二分类混淆矩阵上综合考察查准率和查全率.</p><p><img src="https://s3.bmp.ovh/imgs/2022/10/27/5bb2dea754c00f56.jpg" alt="8"></p><p><img src="https://s3.bmp.ovh/imgs/2022/10/27/dbc965e160b3393a.jpg" alt="9"></p>]]></content>
      
      
      
        <tags>
            
            <tag> 笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>卫生健康委关于”摒弃酒桌陋习，传承正确酒文化“改革的指导意见</title>
      <link href="/posts/85d6240d.html"/>
      <url>/posts/85d6240d.html</url>
      
        <content type="html"><![CDATA[<h1 id="卫生健康委关于“摒弃酒桌陋习，传承正确酒文化”改革的指导意见"><a href="#卫生健康委关于“摒弃酒桌陋习，传承正确酒文化”改革的指导意见" class="headerlink" title="卫生健康委关于“摒弃酒桌陋习，传承正确酒文化”改革的指导意见"></a><center>卫生健康委关于“摒弃酒桌陋习，传承正确酒文化”改革的指导意见</center></h1><p align="right">204舍卫办函〔2022〕1号</p><p>各卫健委成员、各部委、各直属机构：<p>&emsp;&emsp;中国酒文化源远流长，“酒桌文化”更是成了人们日常交往的重要纽带。不论是喜庆筵席、亲朋往来，还是逢年过节、日常家宴，人们都要举杯畅饮，以增添一些喜庆气氛。同时由于酒有一种微妙的“神奇”作用，故千百年来，人们喜欢以酒祭祖、以酒提神、以酒助胆、以酒御寒等。酒文化也是中华文化的见证者之一，本身是值得我们去了解与尊重的。但近年来，酒桌文化却显现出一些不正常的现象，各种规矩、礼节层出不穷，原本清风正气的酒桌渐露腐败之像，经卫健委审议，现提出以下意见：</p><p>&emsp;&emsp;一、适量饮酒，倡导以酒怡情，莫要因酒伤身；</p><p>&emsp;&emsp;二、合理劝酒，根据对方饮酒能力选择劝酒方式；</p><p>&emsp;&emsp;三、适度敬酒，做到对自己的能力心中有数，勿饮酒伤身；</p><p>&emsp;&emsp;四、以茶代酒、饮料代酒，只要感情有，喝啥都是酒。</p><p>&emsp;&emsp;各部门各有关人员要加强组织领导，将推进酒桌文化改革纳入宿舍文化建设相关规划。建立完善多部门协同推进机制，动员力量广泛参与，以酒桌文化改革为基础，以新酒桌文化发展为支撑，推动新旧转化，完善和落实各项政策措施。加强政策培训和宣传引导，组织实施相关示范项目，及时总结推广典型经验，推动酒桌文化高质量发展。</p><p align="right">204舍卫生健康委</p><p align="right">2022年10月15日</p>]]></content>
      
      
      
        <tags>
            
            <tag> 杂文 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>无人艇障碍AI检测实战案例分享</title>
      <link href="/posts/b462b3fa.html"/>
      <url>/posts/b462b3fa.html</url>
      
        <content type="html"><![CDATA[<h1 id="无人艇障碍AI检测实战案例分享"><a href="#无人艇障碍AI检测实战案例分享" class="headerlink" title="无人艇障碍AI检测实战案例分享"></a>无人艇障碍AI检测实战案例分享</h1><h2 id="以下内容来自海洋仪器仪表研究所的副研究员刘寿生老师主持的线上分享会。"><a href="#以下内容来自海洋仪器仪表研究所的副研究员刘寿生老师主持的线上分享会。" class="headerlink" title="以下内容来自海洋仪器仪表研究所的副研究员刘寿生老师主持的线上分享会。"></a>以下内容来自海洋仪器仪表研究所的副研究员刘寿生老师主持的线上分享会。</h2><h3 id="由AIphaGo战胜李世石的例子引出机器学习，又由AIphaZero战胜AIphaGo的例子引出会议的主题——深度学习。"><a href="#由AIphaGo战胜李世石的例子引出机器学习，又由AIphaZero战胜AIphaGo的例子引出会议的主题——深度学习。" class="headerlink" title="由AIphaGo战胜李世石的例子引出机器学习，又由AIphaZero战胜AIphaGo的例子引出会议的主题——深度学习。"></a>由AIphaGo战胜李世石的例子引出机器学习，又由AIphaZero战胜AIphaGo的例子引出会议的主题——深度学习。</h3><h3 id="深度学习的三要素：数据，算法，算力"><a href="#深度学习的三要素：数据，算法，算力" class="headerlink" title="深度学习的三要素：数据，算法，算力"></a>深度学习的三要素：数据，算法，算力</h3><p><img src="" alt="0"></p><h3 id="举出了其所在研究所的海洋浮标和传感器项目。"><a href="#举出了其所在研究所的海洋浮标和传感器项目。" class="headerlink" title="举出了其所在研究所的海洋浮标和传感器项目。"></a>举出了其所在研究所的海洋浮标和传感器项目。</h3><p><img src="" alt="b1"></p><h2 id="开始讲解深度学习到的内容。"><a href="#开始讲解深度学习到的内容。" class="headerlink" title="开始讲解深度学习到的内容。"></a>开始讲解深度学习到的内容。</h2><h2 id="1-数据集"><a href="#1-数据集" class="headerlink" title="1.数据集"></a>1.数据集</h2><h3 id="1-1介绍了一些知名的开源数据集"><a href="#1-1介绍了一些知名的开源数据集" class="headerlink" title="1.1介绍了一些知名的开源数据集"></a>1.1介绍了一些知名的开源数据集</h3><p><img src="" alt="111"></p><p><img src="https://s3.bmp.ovh/imgs/2022/10/31/1f51317596c4247b.jpg" alt="1121"></p><p><img src="" alt="1122"></p><p><img src="" alt="1131"></p><h3 id="1-23测试集、训练集补充"><a href="#1-23测试集、训练集补充" class="headerlink" title="1.23测试集、训练集补充"></a>1.23测试集、训练集补充</h3><p>当测试集数量不够时，如何补充？</p><p>当船头船尾检测不到时，怎么解决？</p><p><img src="https://s3.bmp.ovh/imgs/2022/10/31/1b3d7c2e4836b9e5.jpg" alt="121"></p><p><img src="https://s3.bmp.ovh/imgs/2022/10/31/6f2208e4f76bc96b.jpg" alt="122"></p><p><img src="https://s3.bmp.ovh/imgs/2022/10/31/f0b88355412a7035.jpg" alt="123"></p><p><img src="https://s3.bmp.ovh/imgs/2022/10/31/2457098c443a7922.jpg" alt="131"></p><p><img src="https://s3.bmp.ovh/imgs/2022/10/31/7bc79706e8ace7fb.jpg" alt="132"></p><h3 id="1-4虚拟样本的使用"><a href="#1-4虚拟样本的使用" class="headerlink" title="1.4虚拟样本的使用"></a>1.4虚拟样本的使用</h3><p>我们可以通过构建VR场景来扩充数据集</p><p><img src="https://s1.328888.xyz/2022/10/01/MVdMi.jpg" alt="141"></p><p><img src="https://z4a.net/image/28VgbC" alt="142"></p><p><img src="https://s1.328888.xyz/2022/10/01/MsLuh.jpg" alt="143"></p><p><img src="https://s1.328888.xyz/2022/10/01/MsNcn.jpg" alt="144"></p><h2 id="2-算法（模型）"><a href="#2-算法（模型）" class="headerlink" title="2.算法（模型）"></a>2.算法（模型）</h2><h3 id="2-1开源网络，我们可以直接使用训练好的开源模型"><a href="#2-1开源网络，我们可以直接使用训练好的开源模型" class="headerlink" title="2.1开源网络，我们可以直接使用训练好的开源模型"></a>2.1开源网络，我们可以直接使用训练好的开源模型</h3><p><img src="https://s1.328888.xyz/2022/10/01/MslSs.jpg" alt="211"></p><h3 id="2-2我们可以通过微调参数来优化模型"><a href="#2-2我们可以通过微调参数来优化模型" class="headerlink" title="2.2我们可以通过微调参数来优化模型"></a>2.2我们可以通过微调参数来优化模型</h3><p><img src="https://s1.328888.xyz/2022/10/01/MsDl0.jpg" alt="221"></p><p><img src="https://s1.328888.xyz/2022/10/01/MsFap.jpg" alt="222"></p><p><img src="https://s1.328888.xyz/2022/10/01/MsOJo.jpg" alt="223"></p><h2 id="3-算力"><a href="#3-算力" class="headerlink" title="3.算力"></a>3.算力</h2><h3 id="提到了工控机和小型机的部署和使用NVIDIA-CUDA配置GPU训练可大幅提升速度。"><a href="#提到了工控机和小型机的部署和使用NVIDIA-CUDA配置GPU训练可大幅提升速度。" class="headerlink" title="提到了工控机和小型机的部署和使用NVIDIA-CUDA配置GPU训练可大幅提升速度。"></a>提到了工控机和小型机的部署和使用NVIDIA-CUDA配置GPU训练可大幅提升速度。</h3><p><img src="https://s1.328888.xyz/2022/10/01/Ms4P5.jpg" alt="312"></p><h2 id="4-对前面内容做了总结，并针对同学们的发展给出了一些启示。"><a href="#4-对前面内容做了总结，并针对同学们的发展给出了一些启示。" class="headerlink" title="4.对前面内容做了总结，并针对同学们的发展给出了一些启示。"></a>4.对前面内容做了总结，并针对同学们的发展给出了一些启示。</h2><p><img src="https://s1.328888.xyz/2022/10/01/Ms03S.jpg" alt="41"></p><p><img src="https://s1.328888.xyz/2022/10/01/Ms6YN.jpg" alt="42"></p>]]></content>
      
      
      
        <tags>
            
            <tag> CV </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>李宏毅-P5卷积神经网络</title>
      <link href="/posts/b94d788.html"/>
      <url>/posts/b94d788.html</url>
      
        <content type="html"><![CDATA[<div class="tag link"><a class="link-card" title="DataWhale李宏毅机器学习" href="https://linklearner.com/datawhale-homepage/#/learn/detail/93"><div class="left"><img src="/img/siteicon/64.png"/></div><div class="right"><p class="text">DataWhale李宏毅机器学习</p><p class="url">https://linklearner.com/datawhale-homepage/#/learn/detail/93</p></div></a></div><h1 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h1><h2 id="为什么用CNN？"><a href="#为什么用CNN？" class="headerlink" title="为什么用CNN？"></a>为什么用CNN？</h2><p><img src="https://s1.328888.xyz/2022/09/25/VKrtN.png" alt="1"></p><p>  在training neural network时，我们会期待说：在network的structure里面，每一个neural就是代表了一个最基本的classifier，事实在文件上根据训练的结果，你有可能会得到很多这样的结论。举例来说：第一层的neural是最简单的classifier，它做的事情就是detain有没有绿色出现，有没有黄色出现，有没有斜的条纹。第二个layer是做比这个更复杂的东西，根据第一个layer的output，它看到直线横线就是窗框的一部分，看到棕色纹就是木纹，看到斜条纹+灰色可能是很多的东西(轮胎的一部分等等)再根据第二个hidden layer的outpost，第三个hidden layer会做更加复杂的事情。<br>  但现在的问题是这样的，当我们一般直接用fully connect feedforward network来做影像处理的时候，往往我们会需要太多的参数，举例来说，假设这是一张100 <em>100的彩色图(一张很小的imgage)，你把这个拉成一个vector，(它有多少个pixel)，它有100 </em>100 3的pixel。如果是彩色图的话，每个pixel需要三个value来描述它，就是30000维(30000 dimension)，那input vector假如是30000dimension，那这个hidden layer假设是1000个neural，那么这个hidden layer的参数就是有30000 *1000，那这样就太多了。那么CNN做的事就是简化neural network的架构。我们把这里面一些根据人的知识，我们根据我们对影像就知道，某些weight用不上的，我们一开始就把它滤掉。<br>  总之，CNN就是用比较少的参数来进行影像处理。所以CNN比一般的DNN还要简单的。</p><h2 id="1·Small-region-一些图片的patterns会比整张图小很多"><a href="#1·Small-region-一些图片的patterns会比整张图小很多" class="headerlink" title="1·Small region  一些图片的patterns会比整张图小很多"></a>1·Small region  一些图片的patterns会比整张图小很多</h2><p><img src="https://s1.328888.xyz/2022/09/25/VKek5.png" alt="2"></p><p>  我们说第一层的 hidden layer那些neural要做的事就是侦测某一种pattern，有没有某一种patter出现。大部分的pattern其实要比整张的image还要小，对一个neural来说，假设它要知道一个image里面有没有某一个pattern出现，它其实是不需要看整张image，它只要看image的一小部分。<br>举例来说，假设我们现在有一张图片，第一个hidden layer的某一种neural的工作就是要侦测有没有鸟嘴的存在(有一些neural侦测有没有爪子的存在，有没有一些neural侦测有没有翅膀的存在，有没有尾巴的存在，合起来就可以侦测图片中某一只鸟)。假设有一个neural的工作是要侦测有没有鸟嘴的存在，那并不需要看整张图，其实我们只需要给neural看着一小红色方框的区域(鸟嘴)，它其实就可以知道说，它是不是一个鸟嘴。对人来说也是一样，看这一小块区域这是鸟嘴，不需要去看整张图才知道这件事情。所以，每一个neural连接到每一个小块的区域就好了，不需要连接到整张完整的图。</p><h2 id="2·Same-Patterns"><a href="#2·Same-Patterns" class="headerlink" title="2·Same Patterns"></a>2·Same Patterns</h2><p><img src="https://s1.328888.xyz/2022/09/25/VKHWy.png" alt="3"></p><p>同样的pattern在image里面，可能会出现在image不同的部分，但是代表的是同样的含义，它们有同样的形状，可以用同样的neural，同样的参数就可以把pattern侦测出来。</p><h2 id="3·Subsampling"><a href="#3·Subsampling" class="headerlink" title="3·Subsampling"></a>3·Subsampling</h2><p><img src="https://s1.328888.xyz/2022/09/25/VKqBC.png" alt="4"></p><p>我们知道一个image你可以做subsampling，你把一个image的奇数行，偶数列的pixel拿掉，变成原来十分之一的大小，它其实不会影响人对这张image的理解。对你来说：这张image跟这张image看起来可能没有太大的差别。是没有太大的影响的，所以我们就可以用这样的概念把image变小，这样就可以减少你需要的参数。</p><h2 id="CNN架构"><a href="#CNN架构" class="headerlink" title="CNN架构"></a>CNN架构</h2><p><img src="https://s1.328888.xyz/2022/09/25/VKdkX.png" alt="5"></p><p>  首先input一张image以后，这张image会通过convolution layer，接下里做max pooling这件事，然后在做convolution，再做max pooling这件事。这个process可以反复无数次，反复的次数你觉得够多之后，(但是反复多少次你是要事先决定的，它就是network的架构(就像你的neural有几层一样)，你要做几层的convolution，做几层的Max Pooling，你再定neural架构的时候，你要事先决定好)。你做完决定要做的convolution和Max Pooling以后，你要做另外一件事，这件事情叫做flatten，再把flatten的output丢到一般fully connected feedforward network，然后得到影像辨识的结果。</p><p><img src="https://s1.328888.xyz/2022/09/25/VKXPd.png" alt="6"></p><p>前面的两个property可以用convolution来处理掉，最后的property可以用Max Pooling这件事来处理。</p><h3 id="（1）Convolution"><a href="#（1）Convolution" class="headerlink" title="（1）Convolution"></a>（1）Convolution</h3><p>Propetry1</p><p><img src="https://s1.328888.xyz/2022/09/25/VKA0B.png" alt="7"></p><p>假设现在我们的network的input是一张6<em>6的Image，如果是黑白的，一个pixel就只需要用一个value去描述它，1就代表有涂墨水，0就代表没有涂到墨水。那在convolution layer里面，它由一组的filter，(其中每一个filter其实就等同于是fully connect layer里面的一个neuron)，每一个filter其实就是一个matrix(3 </em>3)，这每个filter里面的参数(matrix里面每一个element值)就是network的parameter(这些parameter是要学习出来的，并不是需要人去设计的)<br>每个filter如果是3<em> 3的detects意味着它就是再侦测一个3 </em>3的pattern(看3 <em>3的一个范围)。在侦测pattern的时候不看整张image，只看一个3 </em>3的范围内就可以决定有没有某一个pattern的出现。这个就是我们考虑的第一个Property。</p><p>Propetry2</p><p><img src="https://s1.328888.xyz/2022/09/25/VKYYU.png" alt="8"></p><p>filter跟image运作。首先第一个filter是一个3* 3的matrix，把这个filter放在image的左上角，把filter的9个值和image的9个值做内积，两边都是1,1,1(斜对角)，内积的结果就得到3。(移动多少是事先决定的)，移动的距离叫做stride(stride等于多少，自己来设计)，内积等于-1。stride等于2，内积等于-3。我们先设stride等于1。</p><p><img src="https://s1.328888.xyz/2022/09/25/VKRoR.png" alt="9"></p><p>你把filter往右移动一格得到-1，再往右移一格得到-3，再往右移动一格得到-1。接下里往下移动一格，得到-3。以此类推(每次都移动一格)，直到你把filter移到右下角的时候，得到-1(得到的值如图所示)<br>经过这件事情以后，本来是6 <em>6的matrix，经过convolution process就得到4 </em>4的matrix。如果你看filter的值，斜对角的值是1,1,1。所以它的工作就是detain1有没有1,1,1(连续左上到右下的出现在这个image里面)。比如说：出现在这里(如图所示蓝色的直线)，所以这个filter就会告诉你：左上跟左下出现最大的值<br>就代表说这个filter要侦测的pattern，出现在这张image的左上角和左下角，这件事情就考虑了propetry2。同一个pattern出现在了左上角的位置跟左下角的位置，我们就可以用filter 1侦测出来，并不需要不同的filter来做这件事。</p><p><img src="https://s1.328888.xyz/2022/09/25/VKvtI.png" alt="10"></p><p>在一个convolution layer 里面会有很多的filter(刚才只是一个filter的结果)，那另外的filter会有不同的参数(图中显示的filter2)，它也做跟filter1一模一样的事情，在filter放到左上角再内积得到结果-1，依次类推。你把filter2跟 input image做完convolution之后，你就得到了另一个4<em>4的matrix，红色4 </em>4的matrix跟蓝色的matrix合起来就叫做feature map，看你有几个filter，你就得到多少个image(你有100个filter，你就得到100个4 *4的image)</p><p><img src="https://s1.328888.xyz/2022/09/25/VKZeP.png" alt="11"></p><p>刚才举的例子是一张黑白的image，所以input是一个matrix。若今天换成彩色的image,彩色的image是由RGB组成的，所以，一个彩色的image就是好几个matrix叠在一起，就是一个立方体。如果要处理彩色image，这时候filter不是一个matrix，filter而是一个立方体。如果今天是RGB表示一个pixel的话，那input就是3<em>6 </em>6，那filter就是3 <em>3 </em>3。<br>在做convolution的话，就是将filter的9个值和image的9个值做内积(不是把每一个channel分开来算，而是合在一起来算，一个filter就考虑了不同颜色所代表的channel)</p><p>未完待续…</p>]]></content>
      
      
      
        <tags>
            
            <tag> 笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>李宏毅机器学习-P4网络设计的技巧</title>
      <link href="/posts/7ada666d.html"/>
      <url>/posts/7ada666d.html</url>
      
        <content type="html"><![CDATA[<span class='p center logo large'>待填坑。。。</span><div class="tag link"><a class="link-card" title="DataWhale李宏毅机器学习" href="https://github.com/datawhalechina/leeml-notes"><div class="left"><img src="https://linklearner.com/datawhale-homepage/#/favicon.ico"/></div><div class="right"><p class="text">DataWhale李宏毅机器学习</p><p class="url">https://github.com/datawhalechina/leeml-notes</p></div></a></div>]]></content>
      
      
      
        <tags>
            
            <tag> 笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>李宏毅机器学习-P3深度学习介绍和反向传播机制</title>
      <link href="/posts/bfe949fd.html"/>
      <url>/posts/bfe949fd.html</url>
      
        <content type="html"><![CDATA[<div class="tag link"><a class="link-card" title="DataWhale李宏毅机器学习" href="https://linklearner.com/datawhale-homepage/#/learn/detail/93"><div class="left"><img src="/img/siteicon/64.png"/></div><div class="right"><p class="text">DataWhale李宏毅机器学习</p><p class="url">https://linklearner.com/datawhale-homepage/#/learn/detail/93</p></div></a></div><h3 id="deep-learning的历史："><a href="#deep-learning的历史：" class="headerlink" title="deep learning的历史："></a>deep learning的历史：</h3><pre><code>•    1958: Perceptron (linear model)•    1969: Perceptron has limitation•    1980s: Multi-layer perceptron        Do not have significant difference from DNN today•    1986: Backpropagation        Usually more than 3 hidden layers is not helpful•    1989: 1 hidden layer is “good enough”, why deep?•    2006: RBM initialization (breakthrough)•    2009: GPU 加速•    2011: Start to be popular in speech recognition•    2012: win ILSVRC image competition</code></pre><p>深度学习三步骤<br>    •    Step1：神经网络（Neural network）<br>    •    Step2：模型评估（Goodness of function）<br>    •    Step3：选择最优函数（Pick best function）</p><h2 id="Step1：神经网络"><a href="#Step1：神经网络" class="headerlink" title="Step1：神经网络"></a>Step1：神经网络</h2><p><img src="https://s1.328888.xyz/2022/09/18/oyTQn.png" alt="1"><br>不同Neural的连接方式，会产生不同的结构。<br>我们有很多逻辑回归函数，其中每个逻辑回归都有自己的权重和自己的偏差，这些权重和偏差就是参数。</p><h3 id="完全连接前馈神经网络"><a href="#完全连接前馈神经网络" class="headerlink" title="完全连接前馈神经网络"></a>完全连接前馈神经网络</h3><p>Neural排成一排，其中的参数根据TrainingData找出。<br>概念：前馈（feedforward）也可以称为前向，从信号流向来理解就是输入信号进入网络后，信号流动是单向的，即信号从前一层流向后一层，一直到输出层，其中任意两层之间的连接并没有反馈（feedback），亦即信号没有从后一层又返回到前一层。</p><p>下图展示已知参数时输入（1，-1）的结果：<br><img src="https://s1.328888.xyz/2022/09/18/oy4jg.png" alt="2"><br><img src="https://s1.328888.xyz/2022/09/18/oy6ch.png" alt="3"></p><p>当输入0和0时，则得到0.51和0.85<br><img src="https://s1.328888.xyz/2022/09/18/oy3Ns.png" alt="4"></p><p>一个神经网络如果权重和偏差都知道的话就可以看成一个函数，他的输入是一个向量，对应的输出也是一个向量。不论是做回归模型（linear model）还是逻辑回归（logistics regression）都是定义了一个函数集（function set）。我们可以给上面的结构的参数设置为不同的数，就是不同的函数（function）。这些可能的函数（function）结合起来就是一个函数集（function set）。这个时候你的函数集（function set）是比较大的，是以前的回归模型（linear model）等没有办法包含的函数（function），所以说深度学习（Deep Learning）能表达出以前所不能表达的情况。</p><h3 id="全链接和前馈"><a href="#全链接和前馈" class="headerlink" title="全链接和前馈"></a>全链接和前馈</h3><p>全链接(Fully Connect)：layer1与layer2之间两两都有连接<br>前馈(Feedforward)：传递的方向是由后往前传<br>“前馈”并不指信号不能向后传，而是指网络拓扑结构上不存在环或回路<br><img src="https://s1.328888.xyz/2022/09/18/oyBa0.png" alt="5"><br>    •    输入层（Input Layer）：1层<br>    •    隐藏层（Hidden Layer）：N层<br>    •    输出层（Output Layer）：1层</p><h3 id="深度的理解"><a href="#深度的理解" class="headerlink" title="深度的理解"></a>深度的理解</h3><p>Deep = Many hidden layer<br>到底几层算deep？很难说<br>以下是几个例子：<br>    •    2012 AlexNet：8层<br>    •    2014 VGG：19层<br>    •    2014 GoogleNet：22层<br>    •    2015 Residual Net：152层</p><h3 id="矩阵计算（Matrix-Operation）"><a href="#矩阵计算（Matrix-Operation）" class="headerlink" title="矩阵计算（Matrix Operation）"></a>矩阵计算（Matrix Operation）</h3><p>随着层数变多，错误率降低，随之运算量增大，通常都是超过亿万级的计算。对于这样复杂的结构，我们一定不会一个一个的计算，对于亿万级的计算，使用loop循环效率很低。<br>这里我们就引入矩阵计算（Matrix Operation）能使得我们的运算的速度以及效率高很多：<br>计算方法就是：sigmoid（权重w【黄色】 * 输入【蓝色】+ 偏移量b【绿色】）= 输出<br><img src="https://s1.328888.xyz/2022/09/18/oybxp.png" alt="6"><br>如果有很多层呢？计算方法就像是嵌套。<br>这样写成矩阵运算的好处是，你可以使用GPU加速。<br><img src="https://s1.328888.xyz/2022/09/18/oyC3F.png" alt="7"></p><p class='p red'>本质：通过隐藏层进行特征转换</p><p>把隐藏层通过特征提取来替代原来的特征工程，这样在最后一个隐藏层输出的就是一组新的特征（相当于黑箱操作）而对于输出层，其实是把前面的隐藏层的输出当做输入（经过特征提取得到的一组最好的特征）然后通过一个多分类器（可以是softmax函数）得到最后的输出y。<br><img src="https://s1.328888.xyz/2022/09/18/oywio.png" alt="8"></p><p>示例：手写数字识别<br>输入：一个16*16=256维的向量，每个pixel对应一个dimension，有颜色用（ink）用1表示，没有颜色（no ink）用0表示输出：10个维度，每个维度代表一个数字的置信度。<br><img src="https://s1.328888.xyz/2022/09/18/oyaMS.png" alt="9"></p><p>在这个问题中，唯一需要的就是一个函数，输入是256维的向量，输出是10维的向量，我们所需要求的函数就是神经网络这个函数。<br><img src="https://s1.328888.xyz/2022/09/18/oykX5.png" alt="10"></p><p>神经网络的结构决定了函数集（function set），所以说网络结构（network structured）很关键。<br><img src="https://s1.328888.xyz/2022/09/18/oy9cy.png" alt="11"></p><h3 id="Q-amp-A"><a href="#Q-amp-A" class="headerlink" title="Q&amp;A"></a>Q&amp;A</h3><pre><code>•    多少层？ 每层有多少神经元？这个问我们需要用尝试加上直觉的方法来进行调试。对于有些机器学习相关的问题，我们一般用特征工程来提取特征，但是对于深度学习，我们只需要设计神经网络模型来进行就可以了。对于语音识别和影像识别，深度学习是个好的方法，因为特征工程提取特征并不容易。•    结构可以自动确定吗？有很多设计方法可以让机器自动找到神经网络的结构的，比如进化人工神经网络（Evolutionary Artificial Neural Networks）但是这些方法并不是很普及 。•    我们可以设计网络结构吗？可以的，比如 CNN卷积神经网络（Convolutional Neural Network ）</code></pre><h2 id="Step2-模型评估"><a href="#Step2-模型评估" class="headerlink" title="Step2: 模型评估"></a>Step2: 模型评估</h2><p>对于模型的评估，我们一般采用损失函数来反应模型的好差，所以对于神经网络来说，我们采用交叉熵（cross entropy）函数来对y和y^的损失进行计算，接下来我们就是调整参数，让交叉熵越小越好。<br><img src="https://s1.328888.xyz/2022/09/18/oy5uN.png" alt="12"></p><p>对于损失，我们不单单要计算一笔数据的，而是要计算整体所有训练数据的损失，然后把所有的训练数据的损失都加起来，得到一个总体损失L。接下来就是在function set里面找到一组函数能最小化这个总体损失L，或者是找一组神经网络的参数θ，来最小化总体损失L。<br><img src="https://s1.328888.xyz/2022/09/18/oyKSC.png" alt="13"></p><h2 id="Step3：选择最优函数"><a href="#Step3：选择最优函数" class="headerlink" title="Step3：选择最优函数"></a>Step3：选择最优函数</h2><h3 id="用梯度下降找到最优的函数和最好的一组参数。"><a href="#用梯度下降找到最优的函数和最好的一组参数。" class="headerlink" title="用梯度下降找到最优的函数和最好的一组参数。"></a>用梯度下降找到最优的函数和最好的一组参数。</h3><h3 id="具体流程："><a href="#具体流程：" class="headerlink" title="具体流程："></a>具体流程：</h3><p>θ是一组包含权重和偏差的参数集合，随机找一个初试值，接下来计算一下每个参数对应偏微分，得到的一个偏微分的集合∇L就是梯度,有了这些偏微分，我们就可以不断更新梯度得到新的参数，这样不断反复进行，就能得到一组最好的参数使得损失函数的值最小。<br><img src="https://s1.328888.xyz/2022/09/18/oyoNd.png" alt="14"><br><img src="https://s1.328888.xyz/2022/09/18/oy2aU.png" alt="15"></p><p>神经网络中计算损失最好的方法就是反向传播，我们可以用很多框架来进行计算损失<br><img src="https://s1.328888.xyz/2022/09/18/oyIJB.png" alt="16"></p><div class="tag link"><a class="link-card" title="DataWhale李宏毅机器学习" href="https://github.com/datawhalechina/leeml-notes"><div class="left"><img src="https://linklearner.com/datawhale-homepage/#/favicon.ico"/></div><div class="right"><p class="text">DataWhale李宏毅机器学习</p><p class="url">https://github.com/datawhalechina/leeml-notes</p></div></a></div>]]></content>
      
      
      
        <tags>
            
            <tag> 笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>李宏毅机器学习-P2误差和梯度分析</title>
      <link href="/posts/663340e1.html"/>
      <url>/posts/663340e1.html</url>
      
        <content type="html"><![CDATA[<span class='p center logo large'>P2误差和梯度分析</span><div class="note warning no-icon flat"><h1 id="1-误差"><a href="#1-误差" class="headerlink" title="1.误差"></a>1.误差</h1></div><h1 id="Error的来源"><a href="#Error的来源" class="headerlink" title="Error的来源"></a>Error的来源</h1><p><img src="https://s1.328888.xyz/2022/09/16/oL1TS.png" alt="1"></p><p>从上节课测试集数据来看，AverageError 随着模型复杂增加呈指数上升趋势。更复杂的模型并不能给测试集带来更好的效果，而这些 Error 的主要有两个来源，分别是 bias（偏差）和 variance（方差）。</p><h1 id="估测"><a href="#估测" class="headerlink" title="估测"></a>估测</h1><p><img src="https://s1.328888.xyz/2022/09/16/oLaZF.png" alt="2"></p><p>假设\hat f为真实模型，我们通过数据训练出来的理想模型为f^<em>。这个过程就像打靶，\hat f 就是我们的靶心，f^</em> 就是我们投掷的结果。如上图所示，\hat f 与  f^* 之间蓝色部分的差距就是偏差和方差导致的。</p><h1 id="估测变量x的偏差和方差"><a href="#估测变量x的偏差和方差" class="headerlink" title="估测变量x的偏差和方差"></a>估测变量x的偏差和方差</h1><p><a href="https://segmentfault.com/a/1190000016447144">https://segmentfault.com/a/1190000016447144</a><br>误差的期望值 = 噪音的方差 + 模型预测值的方差 + 预测值相对真实值的偏差的平方<br><img src="https://s1.328888.xyz/2022/09/16/oL5s5.png" alt="3"></p><p><img src="https://s1.328888.xyz/2022/09/16/oL9qN.png" alt="4"></p><h2 id="考虑不同模型的方差"><a href="#考虑不同模型的方差" class="headerlink" title="考虑不同模型的方差"></a>考虑不同模型的方差</h2><p>用比较简单的模型，方差是比较小的（就像射击的时候每次的时候，每次射击的设置都集中在一个比较小的区域内）。如果用了复杂的模型，方差就很大，散布比较开。<br><img src="https://s1.328888.xyz/2022/09/16/oLcOy.png" alt="5"></p><h2 id="考虑不同模型的偏差"><a href="#考虑不同模型的偏差" class="headerlink" title="考虑不同模型的偏差"></a>考虑不同模型的偏差</h2><p><img src="https://s1.328888.xyz/2022/09/16/oLoKC.png" alt="6"></p><p>假设黑线为真正的模型，蓝线为平均值，可见更复杂的模型虽然离散程度很高，但其偏差比较小。直观的解释：简单的模型函数集的space比较小，所以可能space里面就没有包含靶心，肯定射不中。而复杂的模型函数集的space比较大，可能就包含的靶心，只是没有办法找到确切的靶心在哪，但足够多的，就可能得到真正的\hat f。</p><h1 id="偏差和方差"><a href="#偏差和方差" class="headerlink" title="偏差和方差"></a>偏差和方差</h1><h2 id="模型选择"><a href="#模型选择" class="headerlink" title="模型选择"></a>模型选择</h2><p>理想中，我们希望得到一个偏差和方差都很小的模型（下图左上），但实际上往往很困难。选择相对较好的模型的顺序：方差小，偏差小 &gt; 方差小，偏差大 &gt; 方差大，偏差小 &gt; 方差大，偏差大。 方差小，偏差大 之所以在实际中排位相对靠前，是因为它比较稳定。很多时候实际中无法获得非常全面的数据集，那么，如果一个模型在可获得的样本上有较小的方差，说明它对不同数据集的敏感度不高，可以期望它对新数据集的预测效果比较稳定。</p><h2 id="拟合"><a href="#拟合" class="headerlink" title="拟合"></a>拟合</h2><p><img src="https://s1.328888.xyz/2022/09/16/oL27d.png" alt="7"></p><p>简单模型（左边）是偏差比较大造成的误差，这种情况叫做欠拟合，而复杂模型（右边）是方差过大造成的误差，这种情况叫做过拟合。<br>如果模型没有很好的训练训练集，就是偏差过大，也就是欠拟合；如果模型很好的训练训练集，即再训练集上得到很小的错误，但在测试集上得到大的错误，这意味着模型可能是方差比较大，就是过拟合。</p><h3 id="偏差大-欠拟合"><a href="#偏差大-欠拟合" class="headerlink" title="偏差大-欠拟合"></a>偏差大-欠拟合</h3><p>此时应该重新设计模型。因为之前的函数集里面可能根本没有包含“靶心”。<br>可以：<br>将更多的函数加进去，比如考虑高度重量，或者HP值等等。或者考虑更多次幂、更复杂的模型。如果此时强行再收集更多的data去训练，这是没有什么帮助的，因为设计的函数集本身就不好，再找更多的训练集也不会更好。</p><h3 id="方差大-过拟合"><a href="#方差大-过拟合" class="headerlink" title="方差大-过拟合"></a>方差大-过拟合</h3><p>简单粗暴的方法：更多的数据<br>但是很多时候不一定能做到收集更多的data。可以针对对问题的理解对数据集做调整。比如识别手写数字的时候，偏转角度的数据集不够，那就将正常的数据集左转15度，右转15度，类似这样的处理。</p><h2 id="模型选择-1"><a href="#模型选择-1" class="headerlink" title="模型选择"></a>模型选择</h2><p class='p red'>#不要这样做！</p><p><img src="https://s1.328888.xyz/2022/09/16/oLVLU.png" alt="8"></p><p>用训练集训练不同的模型，然后在测试集上比较错误，模型3的错误比较小，就认为模型3好。但实际上这只是你手上的测试集，真正完整的测试集并没有。比如在已有的测试集上错误是0.5，但有条件收集到更多的测试集后通常得到的错误都是大于0.5的。</p><p class='p green'>#正确做法</p><p>1.交叉验证<br>交叉验证 就是将训练集再分为两部分，一部分作为训练集，一部分作为验证集。用训练集训练模型，然后再验证集上比较，确定出最好的模型之后（比如模型3），再用全部的训练集训练模型3，然后再用public的测试集进行测试，此时一般得到的错误都是大一些的。不过此时会比较想再回去调一下参数，调整模型，让在public的测试集上更好，但不太推荐这样。</p><p>2.N-折交叉验证<br>将训练集分成N份，比如分成3份。比如在三份中训练结果Average错误是模型1最好，再用全部训练集训练模型1。</p><div class="tag link"><a class="link-card" title="DataWhale李宏毅机器学习" href="https://github.com/datawhalechina/leeml-notes"><div class="left"><img src="https://linklearner.com/datawhale-homepage/#/favicon.ico"/></div><div class="right"><p class="text">DataWhale李宏毅机器学习</p><p class="url">https://github.com/datawhalechina/leeml-notes</p></div></a></div>]]></content>
      
      
      
        <tags>
            
            <tag> 笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>李宏毅机器学习-P1回归</title>
      <link href="/posts/c4d9e06e.html"/>
      <url>/posts/c4d9e06e.html</url>
      
        <content type="html"><![CDATA[<span class='p center logo large'>回归</span><div class="note warning flat"><h2 id="回归定义"><a href="#回归定义" class="headerlink" title="回归定义"></a>回归定义</h2><p>Regression 就是找到一个函数 function ，通过输入特征 x ，输出一个数值 Scalar 。</p></div><div class="note primary no-icon flat"><h2 id="应用举例"><a href="#应用举例" class="headerlink" title="应用举例"></a>应用举例</h2><pre><code>•    股市预测（Stock market forecast）◦    输入：过去10年股票的变动、新闻咨询、公司并购咨询等◦    输出：预测股市明天的平均值•    自动驾驶（Self-driving Car）◦    输入：无人车上的各个sensor的数据，例如路况、测出的车距等◦    输出：方向盘的角度•    商品推荐（Recommendation）◦    输入：商品A的特性，商品B的特性◦    输出：购买商品B的可能性•    Pokemon精灵攻击力预测（Combat Power of a pokemon）：◦    输入：进化前的CP值、物种（Bulbasaur）、血量（HP）、重量（Weight）、高度（Height）◦    输出：进化后的CP值</code></pre></div><div class="note info modern"><h2 id="模型步骤"><a href="#模型步骤" class="headerlink" title="模型步骤"></a>模型步骤</h2><pre><code>•    step1：模型假设，选择模型框架（线性模型）•    step2：模型评估，如何判断众多模型的好坏（损失函数）•    step3：模型优化，如何筛选最优的模型（梯度下降）</code></pre></div><div class="note modern"><h2 id="step1-模型假设-线性模型"><a href="#step1-模型假设-线性模型" class="headerlink" title="step1  模型假设 - 线性模型"></a>step1  模型假设 - 线性模型</h2><pre><code>一元线性模型（单个特征）：我们可以假设出多种可能的线性模型，但有些模型的参数会明显不合理，可以舍弃多元线性模型（多个特征）：需要根据各个特征的权重来假设模型</code></pre></div><div class="note modern"><h2 id="step2-模型评估-损失函数"><a href="#step2-模型评估-损失函数" class="headerlink" title="step2  模型评估 - 损失函数"></a>step2  模型评估 - 损失函数</h2><pre><code>如何判断众多模型的好坏：根据【实际结果】与【模型预测结果】的差，来判定模型的好坏，也    就是使用损失函数（Loss function） 来衡量模型的好坏。</code></pre></div><div class="note modern"><h2 id="Step-3-最佳模型-梯度下降"><a href="#Step-3-最佳模型-梯度下降" class="headerlink" title="Step 3  最佳模型 - 梯度下降"></a>Step 3  最佳模型 - 梯度下降</h2><p>筛选最优的模型使Loss最小<br>方法：    单个参数时：<br>        •作L随参数变化的图，然后随机选取一点，<br>        •计算微分，也就是当前的斜率，根据斜率来判定移动的方向，大于0向左移动（减少$w$）小于        0向右    移动（增加$w$）<br>        •根据学习率（步长）移动<br>    ⁃    •重复步骤2和步骤3，直到找到最低点</p><p><img src="https://s1.328888.xyz/2022/09/14/cuOMk.png" alt="1"></p><p><img src="https://s1.328888.xyz/2022/09/14/cujXE.png" alt="2"></p><p>解释完单个模型参数，引入2个模型参数 ， 其实过程是类似的，需要做的是偏微分，过程如下图所示。</p><p><img src="https://s1.328888.xyz/2022/09/14/cu3ah.png" alt="3"></p><p>如果想进一步提升准确值，则要更换更加合适模型。<br>模型越复杂，在训练集上的平均误差越小，但在实际应用中会出问题（Overfitting）。</p><p><img src="https://s1.328888.xyz/2022/09/14/cuBxn.png" alt="4"></p></div><h2 id="步骤优化"><a href="#步骤优化" class="headerlink" title="步骤优化"></a>步骤优化</h2><div class="tip bolt"><h2 id="1、考虑物种属性的影响"><a href="#1、考虑物种属性的影响" class="headerlink" title="1、考虑物种属性的影响"></a>1、考虑物种属性的影响</h2><p>输入更多Pokemons数据，相同的起始CP值，但进化后的CP差距竟然是2倍。如下图，其实将Pokemons种类通过颜色区分，就会发现Pokemons种类是隐藏得比较深得特征，不同Pokemons种类影响了进化后的CP值的结果。<br><img src="https://s1.328888.xyz/2022/09/14/cuG30.png" alt="5"></p></div><div class="tip bolt"><h1 id="通过对-Pokemons种类判断，将-4-个线性模型-合并到一个线性模型中："><a href="#通过对-Pokemons种类判断，将-4-个线性模型-合并到一个线性模型中：" class="headerlink" title="通过对 Pokemons种类判断，将 4 个线性模型 合并到一个线性模型中："></a>通过对 Pokemons种类判断，将 4 个线性模型 合并到一个线性模型中：</h1><p><img src="https://s1.328888.xyz/2022/09/14/cu1uF.png" alt="6"></p></div><div class="tip bolt"><h2 id="2、加入更多特征"><a href="#2、加入更多特征" class="headerlink" title="2、加入更多特征"></a>2、加入更多特征</h2><pre><code>如果希望模型更强大表现更好（更多参数，更多input）将更多的参数加到模型中，更多特征，更多input，数据量没有明显增加，仍旧导致overfitting</code></pre><p><img src="https://s1.328888.xyz/2022/09/14/cu9S5.png" alt="7"></p></div><div class="tip bolt"><h2 id="3、加入正则化"><a href="#3、加入正则化" class="headerlink" title="3、加入正则化"></a>3、加入正则化</h2><p><img src="https://s1.328888.xyz/2022/09/14/cu2JC.png" alt="8"></p><pre><code>•    w 越小，表示 function 较平滑的，function 输出值与输入值相差不大。•    在很多应用场景中，并不是 w 越小模型越平滑越好，但是经验值告诉我们 w 越小大部分情况下都是好的。•    b 的值接近于0 ，对曲线平滑是没有影响。</code></pre><p><img src="https://s1.328888.xyz/2022/09/14/cus3U.png" alt="9"></p></div><div class="tag link"><a class="link-card" title="DataWhale李宏毅机器学习" href="https://github.com/datawhalechina/leeml-notes"><div class="left"><img src="https://linklearner.com/datawhale-homepage/#/favicon.ico"/></div><div class="right"><p class="text">DataWhale李宏毅机器学习</p><p class="url">https://github.com/datawhalechina/leeml-notes</p></div></a></div>]]></content>
      
      
      
        <tags>
            
            <tag> 笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>叙利亚局势分析</title>
      <link href="/posts/990750a6.html"/>
      <url>/posts/990750a6.html</url>
      
        <content type="html"><![CDATA[<p>从世界政治格局发展的大背景来看，叙利亚的动荡是西亚北非变局的延续与发展。不可否认，中东地区这些国家受到金融危机之后的输入性通胀压力，在各自的发展道路上遭遇了巨大的障碍，加之国内各种政治派别（逊尼派、什叶派）教派纷争不断，导致了经济萧条、就业低迷，酿成了社会抗争与群体性暴力事件。但不管怎样，叙利亚局势仍属于国家发展问题，是内政，而非国际事务。然而，由于叙利亚紧邻伊拉克、约旦、以色列、黎巴嫩、土耳其等国，与伊朗遥相呼应，其所处的地理位置具有极为敏感的地缘政治色彩。所以，叙利亚以往在巴以和谈、伊拉克重建、中东反恐、伊朗核问题、库尔德民族问题等涉及西方敏感神经的区域事务中都在不经意间扮演着极为微妙而关键的角色。而且，令西方世界极为不安的是，统治这个地缘意义上“兵家必争之地”的阿萨德家族却延续着与西方社会格格不入的特立独行。面对叙利亚国内矛盾激化，西方国家自然积极介入，希望借此机会以压促变，进而通过叙利亚政权更迭，达到控制中东地区的政治局势与自然资源的目的。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 杂文 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
    
    
    <entry>
      <title></title>
      <link href="/manifest.json"/>
      <url>/manifest.json</url>
      
        <content type="html"><![CDATA[{"name":"yang-makabaka","short_name":"yang","theme_color":"#425aef","background_color":"#425aef","display":"standalone","scope":"/","start_url":"/","icons":[{"src":"/img/siteicon/16.png","sizes":"16x16","type":"image/png"},{"src":"/img/siteicon/32.png","sizes":"32x32","type":"image/png"},{"src":"/img/siteicon/48.png","sizes":"48x48","type":"image/png"},{"src":"/img/siteicon/64.png","sizes":"64x64","type":"image/png"},{"src":"/img/siteicon/128.png","sizes":"128x128","type":"image/png"},{"src":"/img/siteicon/144.png","sizes":"144x144","type":"image/png"},{"src":"/img/siteicon/512.png","sizes":"512x512","type":"image/png"}],"splash_pages":null}]]></content>
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/css/custom.css"/>
      <url>/css/custom.css</url>
      
        <content type="html"><![CDATA[/* @font-face {  font-family: Candyhome;  src: url(https://npm.elemecdn.com/anzhiyu-blog@1.1.6/fonts/Candyhome.ttf);  font-display: swap;  font-weight: lighter;} */@font-face {    font-family: ZhuZiAYuanJWD;    src: url(https://npm.elemecdn.com/anzhiyu-blog@1.1.6/fonts/ZhuZiAWan.woff2);    font-display: swap;    font-weight: lighter;  }    div#menus {    font-family: 'ZhuZiAYuanJWD';  }  h1#site-title {    font-family: ZhuZiAYuanJWD;    font-size: 3em !important;  }  a.article-title,  a.blog-slider__title,  a.categoryBar-list-link,  h1.post-title {    font-family: ZhuZiAYuanJWD;  }    .iconfont {    font-family: 'iconfont' !important;    font-size: 3em;    /* 可以定义图标大小 */    font-style: normal;    -webkit-font-smoothing: antialiased;    -moz-osx-font-smoothing: grayscale;  }    /* 时间轴生肖icon */  svg.icon {    /* 这里定义svg.icon，避免和Butterfly自带的note标签冲突 */    width: 1em;    height: 1em;    /* width和height定义图标的默认宽度和高度*/    vertical-align: -0.15em;    fill: currentColor;    overflow: hidden;  }    .icon-zhongbiao::before {    color: #f7c768;  }    /* bilibli番剧插件 */  .bangumi-active {    background: #dbecfe !important;    border-radius: 10px !important;  }  a.bangumi-tab:hover {    text-decoration: none !important;  }  .bangumi-button:hover {    background: #dbecfe !important;    border-radius: 10px !important;  }  a.bangumi-button.bangumi-nextpage:hover {    text-decoration: none !important;  }  .bangumi-button {    padding: 5px 10px !important;  }    a.bangumi-tab {    padding: 5px 10px !important;  }  svg.icon.faa-tada {    font-size: 1.1em;  }    /* 解决artitalk的图标问题 */  #uploadSource > svg {    width: 1.19em;    height: 1.5em;  }    /*top-img黑色透明玻璃效果移除，不建议加，除非你执着于完全一图流或者背景图对比色明显 */  #page-header:not(.not-top-img):before {    background-color: transparent !important;  }    /* 首页文章卡片 */  #recent-posts > .recent-post-item {    background: rgba(255, 255, 255, 0.9);  }    /* 首页侧栏卡片 */  #aside-content .card-widget {    background: rgba(255, 255, 255, 0.9);  }    /* 文章页面正文背景 */  div#post {    background: rgba(255, 255, 255, 0.9);  }    /* 分页页面 */  div#page {    background: rgba(255, 255, 255, 0.9);  }    /* 归档页面 */  div#archive {    background: rgba(255, 255, 255, 0.9);  }    /* 标签页面 */  div#tag {    background: rgba(255, 255, 255, 0.9);  }    /* 分类页面 */  div#category {    background: rgba(255, 255, 255, 0.9);  }    /*夜间模式伪类遮罩层透明*/  [data-theme='dark'] #recent-posts > .recent-post-item {    background: #121212;  }    [data-theme='dark'] .card-widget {    background: #121212 !important;  }    [data-theme='dark'] div#post {    background: #121212 !important;  }    [data-theme='dark'] div#tag {    background: #121212 !important;  }    [data-theme='dark'] div#archive {    background: #121212 !important;  }    [data-theme='dark'] div#page {    background: #121212 !important;  }    [data-theme='dark'] div#category {    background: #121212 !important;  }    [data-theme='dark'] div#category {    background: transparent !important;  }  /* 页脚透明 */  #footer {    background: transparent !important;  }    /* 头图透明 */  #page-header {    background: transparent !important;  }    #rightside > div > button {    border-radius: 5px;  }    /* 滚动条 */    ::-webkit-scrollbar {    width: 10px;    height: 10px;  }    ::-webkit-scrollbar-thumb {    background-color: #425aef;    border-radius: 2em;  }    ::-webkit-scrollbar-corner {    background-color: transparent;  }    ::-moz-selection {    color: #fff;    background-color: #425aef;  }    /* 音乐播放器 */    /* .aplayer .aplayer-lrc {    display: none !important;  } */    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {    left: -66px !important;    transition: all 0.3s;    /* 默认情况下缩进左侧66px，只留一点箭头部分 */  }    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {    left: 0 !important;    transition: all 0.3s;    /* 鼠标悬停是左侧缩进归零，完全显示按钮 */  }    .aplayer.aplayer-fixed {    z-index: 999999 !important;  }    /* 评论框  */  .vwrap {    box-shadow: 2px 2px 5px #bbb;    background: rgba(255, 255, 255, 0.3);    border-radius: 8px;    padding: 30px;    margin: 30px 0px 30px 0px;  }    /* 设置评论框 */    .vcard {    box-shadow: 2px 2px 5px #bbb;    background: rgba(255, 255, 255, 0.3);    border-radius: 8px;    padding: 30px;    margin: 30px 0px 0px 0px;  }    /* 鼠标图标 */  body {    cursor: url('/img/x1.cur'), auto;  }  a,  [type='button']:not(:disabled),  [type='reset']:not(:disabled),  [type='submit']:not(:disabled),  button:not(:disabled) {    cursor: url('/img/x2.cur'), auto !important;  }  /* md网站下划线 */  #article-container a:hover {    text-decoration: none !important;  }    #article-container #hpp_talk p img {    display: inline;  }    /* 404页面 */  #error-wrap {    position: absolute;    top: 40%;    right: 0;    left: 0;    margin: 0 auto;    padding: 0 1rem;    max-width: 1000px;    transform: translate(0, -50%);  }    #error-wrap .error-content {    display: flex;    flex-direction: row;    justify-content: center;    align-items: center;    margin: 0 1rem;    height: 18rem;    border-radius: 8px;    background: var(--card-bg);    box-shadow: var(--card-box-shadow);    transition: all 0.3s;  }    #error-wrap .error-content .error-img {    box-flex: 1;    flex: 1;    height: 100%;    border-top-left-radius: 8px;    border-bottom-left-radius: 8px;    background-color: #425aef;    background-position: center;    background-size: cover;  }    #error-wrap .error-content .error-info {    box-flex: 1;    flex: 1;    padding: 0.5rem;    text-align: center;    font-size: 14px;    font-family: Titillium Web, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft JhengHei', 'Microsoft YaHei', sans-serif;  }  #error-wrap .error-content .error-info .error_title {    margin-top: -4rem;    font-size: 9em;  }  #error-wrap .error-content .error-info .error_subtitle {    margin-top: -3.5rem;    word-break: break-word;    font-size: 1.6em;  }  #error-wrap .error-content .error-info a {    display: inline-block;    margin-top: 0.5rem;    padding: 0.3rem 1.5rem;    background: var(--btn-bg);    color: var(--btn-color);  }    #body-wrap.error .aside-list {    display: flex;    flex-direction: row;    flex-wrap: nowrap;    bottom: 0px;    position: absolute;    padding: 1rem;    width: 100%;    overflow: scroll;  }    #body-wrap.error .aside-list .aside-list-group {    display: flex;    flex-direction: row;    flex-wrap: nowrap;    max-width: 1200px;    margin: 0 auto;  }    #body-wrap.error .aside-list .aside-list-item {    padding: 0.5rem;  }    #body-wrap.error .aside-list .aside-list-item img {    width: 100%;    object-fit: cover;    border-radius: 12px;  }    #body-wrap.error .aside-list .aside-list-item .thumbnail {    overflow: hidden;    width: 230px;    height: 143px;    background: var(--heo-card-bg);    display: flex;  }    #body-wrap.error .aside-list .aside-list-item .content .title {    -webkit-line-clamp: 2;    overflow: hidden;    display: -webkit-box;    -webkit-box-orient: vertical;    line-height: 1.5;    justify-content: center;    align-items: flex-end;    align-content: center;    padding-top: 0.5rem;    color: white;  }    #body-wrap.error .aside-list .aside-list-item .content time {    display: none;  }    /* 代码框主题 */  #article-container figure.highlight {    border-radius: 10px;  }  ]]></content>
      
    </entry>
    
    
  
</search>
